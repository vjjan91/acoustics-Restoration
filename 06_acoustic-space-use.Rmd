---
editor_options: 
  chunk_output_type: console
---

Install necessary libraries
```{r}
library(seewave)
library(warbleR)
library(tuneR)
library(stringi)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
library(foreach)
library(doParallel)

# Source any custom/other internal functions necessary for analysis
source("code/01_internal-functions.R")
```

Acoustic Space Use (ASU)

We are interested in creating a three-dimensional matrix of acoustic activity (x=hour, y=acoustic frequencies, z=proportion of all recordings in each time/frequency bin)

Aide et al. 2017: We aggregated recordings at time scale of hour of day and used a frequency bin size of 86.13 Hz and an amplitude filtering threshold of 0.02. So if the sampling rate is 22000 Hz, that would mean - 22000/86.13 ~ 256 frequency bins to divide up the frequency space. In this paper, there would be 24hr*256 bins = 6144 time/frequency bins

Campos-Cerqueira et al. 2019: We aggregated recordings at the time scale of hour of day (24 h), used a frequency bin size of 172 Hz, and an amplitude filtering threshold of 0.003. So if the sampling rate is 22000 Hz, that would mean - 22000/172 ~ 128 frequency bins. This resulted in a three‐dimensional (x = hour, y = acoustic frequency, z = proportion of all recordings in each time/frequency bin with a frequency peak value > 0.003 amplitude) matrix of acoustic activity with a total of 3,072 time/frequency bins (24 h × 128 frequency bins).

Campos-Cerqueira and Aide 2017: To calculate the amplitude, we used the meanspec (f = 44,100, wl = 256, wn = “hanning”) and fpeaks (threshold = 0.1, freq = 172) function from the seewave package in R (Sueur et al., 2008a). The value of each peak was normalized using the maximum amplitude value within all recordings in the soundscape (i.e., site), and thus values ranged from 0 to 1. The number of frequency peaks was determined by counting the number of recordings with a peak within each of the 128 frequency bins that were equal or greater than the amplitude threshold. To control for the different number of recordings in each site and each time interval (i.e., hour), we divided the number of recordings with a peak in each time/frequency class by the total number of recordings collected during each hourly interval.

To calculate ASU: 

- A. Aggregate recordings for a multiple days across multiple sites (Keep only data that has been recorded continuously for a 24hr period)
```{r}
# List the path that contains all folders, which contain the audiomoth data
path <- "C:\\data\\"

# Listing the folders within which .WAV files are stored
folders <- dir(path, recursive=F,full.names=T)

# Let's first rename the files by name of each site (as prefix)
# Please note: The renaming needs to be done only a single time (else there will be errors)
for(i in 1:length(folders)){

setwd(folders[i])
  
# List the files within each folder and renaming the files with the prefix - SITE_ID
a <- list.files(paste0(path,basename(folders)[i],"\\"), full.names = T)
file.rename(from = a, to=paste0(basename(folders)[i],"_",basename(a)))
}

# Now get only those files for a full 24 hours across every unique site
files <- list()

for(i in 1:length(folders)){

a <- list.files(paste0(path,basename(folders)[i],"\\"), full.names = T)
site_date <- str_extract(basename(a),'\\w+_\\d+_')

# Choosing all 24 hours of data across every unique site (288 corresponds to 12 files every 1 hour)
  for(j in 1:length(unique(site_date))){
    dat <- a[str_detect(a,unique(site_date)[j])]
    if((length(dat)<288)==TRUE){
      next
    } else {
      files <- c(files, dat) 
    }
  }
}

files <- unlist(files)
```


- B. Aggregate recordings for any single day for every unique site and sort it in order (between 00:00:00 to 23:55:00 hrs) and then loop it across multiple days

ASU is being calculated for every one hour period and showcased for a 24 hour period of data. ASU here is defined as the overall area occupied by the frequency contour for a given amplitude threshold. 
```{r}
# Select all unique site_date combinations for each unique site
site_date <- str_extract(basename(files),'\\w+_\\d+_')
unique(site_date)

# Create a sequence of numbers to combine files by 1 hour
hour_seq <- seq(from=0,to=288, by=12)

# To name files with a suffix for each hour
time_of_day <- c("00:00-01:00","01:00-02:00","02:00-03:00","03:00-04:00",
                 "04:00-05:00","05:00-06:00","06:00-07:00","07:00-08:00",
                 "08:00-09:00","09:00-10:00","10:00-11:00","11:00-12:00",
                 "12:00-13:00","13:00-14:00","14:00-15:00","15:00-16:00",
                 "16:00-17:00","17:00-18:00","18:00-19:00","19:00-20:00",
                 "20:00-21:00","21:00-22:00","22:00-23:00","23:00-24:00")

# Loading parameters necessary for the Short-term fourier transform to be performed on
# hourly aggregates of data for each site_date combination
f <- 48000
wl <- 48 # Changing this to 256 to match Campos-Cerqueira et al. 2017 which results in 128 frequency bins
ovlp <- 0
wn <- "hanning"

# Store the 24 hour acoustic space use data in a list and name it by a unique site and date
site_date_asu <-  list()

# Add a progress bar for the loop
pb <- txtProgressBar(
  min = 0,
  max = length(unique(site_date)),
  style = 3
)

# Select only 24 hours of data (00:00:00 to 23:55:00) for every unique site-date
for (i in 1:length(unique(site_date))){ 
  
  # Store the each hour of data as a list here (raw audio files read by tuneR::readWave())
  hourlydata <- list()
  
  # Store the acoustic space use data in a data.frame for plotting and analysis
  space_use <- data.frame()

  # Extract the strings first by site 
  dat <- files[stringr::str_detect(files,unique(site_date)[1])]
  
  # Read the files using tuneR::readWave() function and save it in a list
  tmp_list <- list()
  for(k in 1:length(dat)){
    r <- tuneR::readWave(dat[k])
    s <- seewave::afilter(r,threshold = 1, plot = FALSE, output = "Wave")
    tmp_list<- c(tmp_list,s)
    }
    rm(r,s); gc()
    hourlydata <- do.call(c,tmp_list)
    rm(tmp_list); gc()
  
  for(t in 1:(length(hour_seq)-1)){ # Every 12 files correspond to one hour here
    if (t==1){
    e <- hourlydata[hour_seq[t]:hour_seq[t+1]]
    } else {
    e <- hourlydata[(hour_seq[t]+1):hour_seq[t+1]]
    }
  
  # Store every hour's ASU data here
  data_per_hour <- list()
    
  for(z in 1:length(e)){
      wave <- e[[z]] # this needs to be E!
      n <- length(wave)
        
      ## Short-term Fourier transform (using a seewave internal function)
      m <- sspectro(wave, f = f, wl = wl, ovlp = ovlp, wn = wn)

      # Frequency selection and frequency axis
      # Here, want only a sequence of numbers that correspond to the length of rows of the
      # short-time fourier transform and we divide it by 1000 to get values in kHz
      freq <- seq(0, (f/2) - (f/wl), length.out = nrow(m))/1000
        
      # Calculate acoustic space use per frequency bin 
      f.cont <- apply(m, MARGIN = 1, FUN = sum)
      #f.cont <- f.cont/sum(f.cont)
        
      # Store the space use data in a dataframe for plotting later
      a <- data.frame(freq, f.cont)
      data_per_hour <- rbind(a, data_per_hour)
    }
  rm(m, wave, a, e, freq, f.cont); gc()
  
  data_needed <- do.call(c, data_per_hour)
    
  data_per_hour  <- data_per_hour %>%
  group_by(freq) %>%
  summarise(f.cont=sum(f.cont)) 
  
  data_per_hour$f.cont <- (data_per_hour$f.cont)/12
  data_per_hour$time_of_day <- time_of_day[t]
  
  space_use <- rbind(data_per_hour, space_use)
}
  space_use <- as.data.frame(space_use)
  site_date_asu <- c(site_date_asu,list(space_use))
  names(site_date_asu)[i] <- unique(site_date)[i] 
  rm(hourlydata, space_use, data_needed); gc()
  setTxtProgressBar(pb, i)
}
close(pb)    
 
```

Other notes (to keep in mind):



To be done: 

Values of ASU needs to be extracted from each site per season: But, we shall consider 3-5 continuous days of data at each site for averaging of ASU for that season. 

Upon averaging, the values can be scaled between 0 and 1 for the sake of comparison across sites (and treatment types)

Since ASU values are being scaled between 0 and 1 for comparison between sites, does it make sense to carry out the averaging across multiple days and then carry out the scaling of data? - which will make it easier for sampling. 


Averaging across sites and days and visualization to be done.
```{r}

t1<-max(abs(wave))*(threshold/100)
  wave1 <- ifelse(abs(wave)<=t1,yes=0,no=1)
  wave2 <- wave[,1]*wave1[,1]
  wave2 <- outputw(wave=wave2, f=f, format=output)




f.cont <- range01(f.cont)


g1 <- ggplot(site_date_asu$HP37P4B_20200308_, aes(x=time_of_day, y=freq)) +  
  geom_tile(aes(fill = f.cont)) +
  scale_fill_gradientn(colours = brewer.pal(9,"Reds"))+
    # scale_fill_scico(palette = "vikO") +
    theme_bw() +
    labs(x="Time of Day (in hours)",
       y="Frequency (in kHz) ") +
    theme(axis.title = element_text(size = 16, face = "bold"), 
        axis.ticks.length.x = unit(.5, "cm"),
        axis.text = element_text(size = 14),
        axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5),
        legend.title = element_blank(),
        legend.key.size = unit(1,"cm"),
        legend.text = element_text(size = 12))

theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


ggsave(g1, filename = "figs/fig_asu_HP37P4B.svg", width=12, height=7,
       device = svg(), units="in", dpi = 300); dev.off()



g2 <- ggplot(plot_dat, aes(y=freq, fill= f.cont, x=time_of_day)) + 
  geom_density(alpha = 0.5) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme_minimal_hgrid(12)


plot(x = freq, y = f.cont, type = "l", xlab = "Frequency(kHz)", 
            ylab = "Amplitude")




```

