[["index.html", "Source code for Using acoustics to assess the impacts of forest restoration on bird communities in the Western Ghats Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing 1.4 Main Text Figure 1", " Source code for Using acoustics to assess the impacts of forest restoration on bird communities in the Western Ghats Vijay Ramesh VA Akshay Priyanka HariHaran Pooja Choksi Sarika Khanwilkar Don J Melnick VV Robin Ruth DeFries 2021-11-30 Section 1 Introduction This is the readable version that showcases analyses carried out to test the impacts of forest restoration on bird communities and other vocalizing biodiversity in the Anamalai hills of the Western Ghats biodiversity hotspot. 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University 1.2 Data access The data used in this work will be archived and made public upon publication. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. 1.4 Main Text Figure 1 This figure was prepared using ArcGIS Pro. Sites corresponding to acoustic recorder deployment locations across the Valparai plateau of the Anamalai hills. The above figure showcases a gradient of forest regeneration across the Valparai plateau. Sites shown in orange represent undisturbed benchmark rainforest sites, sites shown in blue represent actively restored forest sites and sites shown in white represent naturally regenerating forest sites. Ecological restoration is currently being carried out in cooperation with three plantation companies in the Valparai plateau. For more information with respect to the weeding and active restoration protocol, please see methods in Hariharan and Raman (2021) and Osuri et al. (2019). Over the last two decades, the ecological restoration efforts have resulted in restoration of over 100 ha of degraded forests. This map was prepared using 30m resolution SRTM data (Farr et al. 2007) and ESRI satellite imagery is used as a basemap. "],["splitting-large-wav-files-into-smaller-chunks.html", "Section 2 Splitting large .wav files into smaller chunks", " Section 2 Splitting large .wav files into smaller chunks Here, we will first split the raw data which was collected for 24 hours at a site, for 7 days at a stretch. Deployment schedule was set to record for 4-minutes and switched off for 1-min. For the sake of analysis, data was split into 10s chunks and annotated manually using Raven Pro. # Loading required libraries library(warbleR) library(seewave) library(dplyr) library(stringr) library(tools) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Now, we will use warbleR::split.wavs() to split a large file Steps: 1. Load a list of .wav files in a given folder (will have to be done site by site) 2. Select only files between 6am and 10am (to begin with) 3. Select only a random consecutive 16 min of data (to begin with) 3. Split file into chunks of a given duration. 4. Give unique names to each split file # List the path that contains all folders, which contain the audiomoth data path &lt;- &quot;D:\\\\2020-summer\\\\&quot; # Listing the folders within which .WAV files are stored folders &lt;- dir(path, recursive=F,full.names=T) # Now get only those files that begin at 6am and end at 10am files &lt;- list() for(i in 1:length(folders)){ setwd(folders[i]) # Below code needs to be run only if we have to rename files # List the files within each folder and renaming the files with the prefix - SITE_ID # a &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;), full.names = T) # file.rename(from = a, to=paste0(basename(folders)[i],&quot;_&quot;,basename(a))) # Extract the strings for .wav files between 6am and 10am time_str &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;),full.names = T) %&gt;% tools::file_path_sans_ext() %&gt;% str_extract(&#39;\\\\d+$&#39;) time_str &lt;- time_str[time_str&gt;=&quot;160000&quot; &amp; time_str &lt;=&quot;190000&quot;] for(j in 1:length(unique(time_str))){ b &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;),full.names = T, pattern = time_str[j]) files &lt;- c(files,b) } } # These are the list of files we need files &lt;- unlist(files) # Now we choose a random consecutive 16 min of data between 6am and 10am # Get a list of unique dates (since we will be generating a random 16min for every date across every site) site_date &lt;- str_extract(basename(files),&#39;\\\\w+_\\\\d+_&#39;) unique(site_date) # Give you unique date and sites for which we need to generate 16 min of data subset_files &lt;- list() for(i in 1:length(unique(site_date))){ a &lt;- files[str_detect(files,unique(site_date)[i])] if(length(a)&lt;4){ # essentially specifies that the min number you need next } else { subset_dat &lt;- extractRandWindow(a,4) subset_dat &lt;- na.exclude(subset_dat) # If there are less than 4 files subset_files &lt;- c(subset_files, subset_dat) } } final_subset &lt;- unlist(subset_files) # Subset those files and copy it to a separate folder dir.create(paste0(&quot;C:\\\\data\\\\&quot;,&quot;subset&quot;)) file.copy(from = final_subset, to=&quot;C:\\\\data\\\\subset\\\\&quot;) Split the files and provide unique names to each file # Note: the path you choose to store data is upto the user. subset_path &lt;- &quot;C:\\\\data\\\\subset\\\\&quot; # Split the files into n-second chunks split_wavs(path=subset_path, sgmt.dur = 10, parallel=4) # Get files that need to be renamed split_files &lt;- list.files(subset_path, full.names = T, pattern = &quot;-&quot;) # Note the number of chunks will vary as a function of segment duration # 240 seconds = 24 chunks each of 10s setwd(subset_path) chunks &lt;- c(&quot;01-10&quot;,&quot;10-20&quot;,&quot;20-30&quot;, &quot;30-40&quot;,&quot;40-50&quot;,&quot;50-60&quot;, &quot;60-70&quot;,&quot;70-80&quot;,&quot;80-90&quot;, &quot;90-100&quot;,&quot;100-110&quot;,&quot;110-120&quot;, &quot;120-130&quot;,&quot;130-140&quot;,&quot;140-150&quot;, &quot;150-160&quot;,&quot;160-170&quot;,&quot;170-180&quot;, &quot;180-190&quot;,&quot;190-200&quot;,&quot;200-210&quot;, &quot;210-220&quot;,&quot;220-230&quot;,&quot;230-240&quot;) for(i in 1:length(chunks)){ c &lt;- split_files[endsWith(split_files,paste0(&quot;-&quot;,i,&quot;.wav&quot;))] d &lt;- str_replace(c,paste0(&quot;-&quot;,i),paste0(&quot;_&quot;,chunks[i])) file.rename(from=c, to=d) } # Remove the original files orig_files &lt;- list.files(subset_path, full.names = T, pattern = &quot;.WAV$&quot;) file.remove(orig_files) Now, go ahead and begin the process of manual annotation! "],["species-richness.html", "Section 3 Species Richness", " Section 3 Species Richness In this script, we will calculate: Site-wise species richness to understand if species composition across treatment types are distinctly different. Repeat the above three calculations, but using species traits - If a species is a rainforest specialist or an open-country generalist. Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(data.table) library(extrafont) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) To start with, we will compute species richness from 3 non-consecutive visits for each site for each season. Across two seasons, a total of 6 non-consecutive visits are chosen for each site. We will use a excel sheet of annotations, which contains 10s-clips from each site (A random 16-min was chosen between 6am and 10am and divided into 10s chunks for the ease of annotations). # Attach the annotation data for summer and winter summer_data &lt;- read.csv(&quot;data/2020-summer-annotation-working-document.csv&quot;) winter_data &lt;- read.csv(&quot;data/2020-winter-annotation-working-document.csv&quot;) # combine the datasets for a single dataframe for the present analysis data &lt;- bind_rows(summer_data,winter_data) names(data) # reorder columns data &lt;- data %&gt;% relocate(c(&quot;BFO&quot;,&quot;SBEO&quot;, &quot;JN&quot;, &quot;AK&quot;, &quot;HSWP&quot;), .after = &quot;CR&quot;) # Site-wise sorting of the 16-min of data # Split the file names into 4 columns : Site, Date, Time and Splits data &lt;- separate(data, col = Filename, into = c(&quot;Site&quot;, &quot;Date&quot;, &quot;Time&quot;, &quot;Splits&quot;), sep = &quot;_&quot;) data # Load the species-trait-data trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) How many visits (counted as number of unique days) to a site has been annotated? Please note that some sites have more than 6 visits to a site, and hence we ensure that we choose only 6 non-consecutive visits to a given site # First we remove OLCAP5B - a site for which only 3 visits were made in summer and not sampled as a result of logistic reasons in winter data &lt;- data %&gt;% filter(!str_detect(Site, &#39;OLCAP5B&#39;)) # Number of visits to a particular site # Only INBS04U has 5 visits because another visit was not possible due to rain nSites_Days &lt;- data %&gt;% dplyr::select(Site, Date)%&gt;% distinct() %&gt;% arrange(Site) %&gt;% count(Site) # Unique date site combination to give you a sense of sampling uniqueSiteDate &lt;- data %&gt;% group_by(Site) %&gt;% distinct(Date) # Convert date column to YMD format using lubridate::ymd() uniqueSiteDate$Date &lt;- lubridate::ymd(uniqueSiteDate$Date) # The below lines of code were written following a query on stackOverflow to select six non-consecutive visits to any site # Link: https://stackoverflow.com/questions/67212152/select-non-consecutive-dates-for-every-grouped-element-in-r nonConVisits &lt;- uniqueSiteDate%&gt;% ungroup() %&gt;% group_split(Site) %&gt;% map_df(., ~ .x %&gt;% ungroup() %&gt;% arrange(Date) %&gt;% mutate(n = 1) %&gt;% complete(Date = seq.Date(first(Date), last(Date), by = &#39;days&#39;))%&gt;% group_by(n = cumsum(is.na(n))) %&gt;% filter(!is.na(Site)) %&gt;% filter(row_number() %% 2 == 1) %&gt;% ungroup() %&gt;% sample_n(min(n(), 6)) ) %&gt;% dplyr::select(-n) # Change the structure of the date column back to character for using one of the join functions nonConVisits$Date &lt;- str_remove_all(as.character(nonConVisits$Date),&quot;-&quot;) # Left-join with the original dataframe to subset the data for analysis datSubset &lt;- left_join(nonConVisits,data) names(datSubset) # renaming columns datSubset &lt;- rename(datSubset, Restoration.type = Restoration.Type..Benchmark.Active.Passive.) # Save this data as a .csv for future analysis (later scripts) write.csv(datSubset,&quot;data/datSubset.csv&quot;, row.names = F) Now group the data by site and restoration type and sum the number of detections across sites. At the moment, we will calculate the overall number of detections for each 10s clip, which will be used to estimate species richness below. # Calculate the overall number of detections for each site across 6 days of data (translates to ~ 96 min of data per site) nDetections_Site &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) Convert the detections to 1, since we are interested in calculating richness per site by converting values &gt;1 to 1 for multiple visits to a site. In other words, we want to establish overall species richness for a 16-min to 48-min window. richness &lt;- nDetections_Site %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, richness) # Test if there are significant differences in richness across treatment types anovaAll &lt;- aov(richness~Restoration.type, data = richness) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukeyAll &lt;- TukeyHSD(anovaAll) # The above result suggests that there are no differences in overall species richness across treatment types # Create a boxplot of species richness by Restoration Type # reordering factors for plotting richness$Restoration.type &lt;- factor(richness$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_richness &lt;- ggplot(richness, aes(x=Restoration.type, y=richness, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Species Richness\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_richness, filename = &quot;figs/fig_richness.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() # We observe that the overall species richness is relatively higher in passively restored forest patches, followed by benchmark (protected area) forest patches and actively restored forest patches (But these differences are not significant) Using species trait data to check if species richness varies by treatment type as a function of whether a species is a rainforest specialist vs. open-country specialist.To do so: Add an additional column of species-trait data and group data based on the same. # First, we pivot the species-codes and then match the codes with trait_data and reformat the data to keep all detections&gt;0 as 1 else they are 0 richness_trait &lt;- nDetections_Site %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1, count==0 ~ 0)) # Calculate overall richness for each site as a function of rainforest species and open-country species richness_trait &lt;- richness_trait %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, forRichness)%&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(richness = sum(forRichness)) %&gt;% drop_na() # Let&#39;s subset data for richness as a function of rainforest specialists and open-country generalists and test for significant differences (if any) richness_rainforest &lt;- richness_trait %&gt;% filter(Habitat==&quot;RF&quot;) richness_opencountry &lt;- richness_trait %&gt;% filter(Habitat==&quot;OC&quot;) # Test if there are significant differences in richness across treatment types as a function of species trait anova_rainforest &lt;- aov(richness~Restoration.type, data = richness_rainforest) anova_opencountry &lt;- aov(richness~Restoration.type, data = richness_opencountry) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types for rainforest birds, but a significant difference between benchmark-active and benchmark-passive sites for open country birds tukey_rainforest &lt;- TukeyHSD(anova_rainforest) tukey_opencountry &lt;- TukeyHSD(anova_opencountry) # reordering factors for plotting richness_trait$Restoration.type &lt;- factor(richness_trait$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Plot the above result fig_richness_trait &lt;- ggplot(richness_trait, aes(x=Restoration.type, y=richness, fill=Habitat)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;, labels=c(&quot;Open-country&quot;,&quot;Rainforest&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Species Richness\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) ggsave(fig_richness_trait, filename = &quot;figs/fig_richness_trait.png&quot;, width=12, height=7, device =png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["acoustic-detections.html", "Section 4 Acoustic Detections", " Section 4 Acoustic Detections In this script, we will calculate: Total Number of detections across sites (reported for varying time intervals 10s, 30s, 1-min, 2-min, 4-min). Repeat the above calculations, but using species traits - If a species is a rainforest specialist or an open-country generalist. Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(extrafont) We will use the data that was subset previously for further analysis. # Attach the annotated data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Load the species-trait-data trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) Given the data annotated so far, we will calculate the overall number of detections aHSWPoss different temporal periods, starting from 10s to 30-s, 1-min, 2-min and 4-min. We will first calculate the overall number of detections for the shortest possible temporal duration which could be annotated confidently - 10s. Other durations are chosen to confirm if the number of detections vary as a function of the temporal duration. Please note that: the data at the moment is imbalanced in terms of nVisits # Calculate the overall number of detections for each site where each temporal duration chosen is a 10s clip nDetections_10s &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 30s clip (In this case, every third row is chosen after grouping by Site, Date and Time) nDetections_30s &lt;- datSubset %&gt;% mutate(Splits = case_when((Splits == &quot;01-10&quot; | Splits==&quot;10-20&quot; | Splits ==&quot;20-30&quot;) ~ &quot;1&quot;,(Splits == &quot;30-40&quot; | Splits==&quot;40-50&quot; | Splits ==&quot;50-60&quot;) ~ &quot;2&quot;,(Splits == &quot;60-70&quot; | Splits==&quot;70-80&quot; | Splits ==&quot;80-90&quot;) ~ &quot;3&quot;, (Splits == &quot;90-100&quot; | Splits==&quot;100-110&quot; | Splits ==&quot;110-120&quot;) ~ &quot;4&quot;,(Splits == &quot;120-130&quot; | Splits==&quot;130-140&quot; | Splits ==&quot;140-150&quot;) ~ &quot;5&quot;,(Splits == &quot;150-160&quot; | Splits==&quot;160-170&quot; | Splits ==&quot;170-180&quot;) ~ &quot;6&quot;,(Splits == &quot;180-190&quot; | Splits==&quot;190-200&quot; | Splits ==&quot;200-210&quot;) ~ &quot;7&quot;, (Splits == &quot;210-220&quot; | Splits==&quot;220-230&quot; | Splits ==&quot;230-240&quot;) ~&quot;8&quot;)) %&gt;% group_by(Site, Date, Time, Splits, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections &gt;1 within a 30s period to 1 (since your temporal unit here is 30s) nDetections_30s &lt;- nDetections_30s %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 60s clip (In this case, every sixth row is chosen after grouping by Site, Date and Time) nDetections_1min &lt;- datSubset %&gt;% mutate(Splits = case_when((Splits == &quot;01-10&quot; | Splits==&quot;10-20&quot; | Splits ==&quot;20-30&quot; | Splits == &quot;30-40&quot; | Splits==&quot;40-50&quot; | Splits ==&quot;50-60&quot;) ~ &quot;1&quot;, (Splits == &quot;60-70&quot; | Splits==&quot;70-80&quot; | Splits ==&quot;80-90&quot; | Splits == &quot;90-100&quot; | Splits==&quot;100-110&quot; | Splits ==&quot;110-120&quot;) ~ &quot;2&quot;, (Splits == &quot;120-130&quot; | Splits==&quot;130-140&quot; | Splits ==&quot;140-150&quot; | Splits == &quot;150-160&quot; | Splits==&quot;160-170&quot; | Splits ==&quot;170-180&quot;) ~ &quot;3&quot;, (Splits == &quot;180-190&quot; | Splits==&quot;190-200&quot; | Splits ==&quot;200-210&quot; | Splits == &quot;210-220&quot; | Splits==&quot;220-230&quot; | Splits ==&quot;230-240&quot;) ~&quot;4&quot;)) %&gt;% group_by(Site, Date, Time, Splits, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections&gt;1 within a 1-min period to 1 (since your temporal unit here is 1-min) nDetections_1min &lt;- nDetections_1min %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 120s clip (In this case, every twelfth row is chosen after grouping by Site, Date and Time) nDetections_2min &lt;- datSubset %&gt;% mutate(Splits = case_when((Splits == &quot;01-10&quot; | Splits==&quot;10-20&quot; | Splits ==&quot;20-30&quot; | Splits == &quot;30-40&quot; | Splits==&quot;40-50&quot; | Splits ==&quot;50-60&quot; | Splits == &quot;60-70&quot; | Splits==&quot;70-80&quot; | Splits ==&quot;80-90&quot; | Splits == &quot;90-100&quot; | Splits==&quot;100-110&quot; | Splits ==&quot;110-120&quot;) ~ &quot;1&quot;, (Splits == &quot;120-130&quot; | Splits==&quot;130-140&quot; | Splits ==&quot;140-150&quot; | Splits == &quot;150-160&quot; | Splits==&quot;160-170&quot; | Splits ==&quot;170-180&quot; | Splits == &quot;180-190&quot; | Splits==&quot;190-200&quot; | Splits ==&quot;200-210&quot; | Splits == &quot;210-220&quot; | Splits==&quot;220-230&quot; | Splits ==&quot;230-240&quot;) ~&quot;2&quot;)) %&gt;% group_by(Site, Date, Time, Splits, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections&gt;1 within a 2-min period to 1 (since your temporal unit here is 2-min) nDetections_2min &lt;- nDetections_2min %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 240s clip (In this case, every twentyfourth row is chosen after grouping by Site, Date and Time) nDetections_4min &lt;- datSubset %&gt;% group_by(Site, Date, Time, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections&gt;1 within a 4-min period to 1 (since your temporal unit here is 4-min) nDetections_4min &lt;- nDetections_4min %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) How does the number of detections vary as a function of restoration type? (tested across different temporal durations)? # Testing if there is significant differences in overall number of detections across treatment types (this has been done only for the smallest temporal duration ~10s, while total number of detections have been estimated for different temporal durations) sum_Detections10s &lt;- nDetections_10s %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) # Test if there are significant differences in detections across treatment types anovaAllDetect &lt;- aov(sumDetections~Restoration.type, data = sum_Detections10s) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukeyAllDetect &lt;- TukeyHSD(anovaAllDetect) # Estimating total number of detections for different temporal durations sum_Detections30s &lt;- nDetections_30s %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) sum_Detections1min &lt;- nDetections_1min %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) sum_Detections2min &lt;- nDetections_2min %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) sum_Detections4min &lt;- nDetections_4min %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) # Plotting the above (multiple plots for each temporal duration) # Note: the cumulative number of detections aHSWPoss all species was obtained by summing every 16-min to 48-min set of detections aHSWPoss each site, including all species. # reordering factors for plotting sum_Detections10s$Restoration.type &lt;- factor(sum_Detections10s$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_sumDetections10s &lt;- ggplot(sum_Detections10s, aes(x=Restoration.type, y=sumDetections, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Cumulative number of detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_sumDetections10s, filename = &quot;figs/fig_sumDetections10s.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() How does the cumulative number of detections vary by treatment type, as a function of whether a species is a rainforest specialist or an open country generalist? (These calculations are repeated for different temporal durations to assess differences, if any) # First we merge the species trait dataset with the nDetections dataframe (across different temporal durations) detections_trait10s &lt;- nDetections_10s %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait30s &lt;- nDetections_30s %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait1min &lt;- nDetections_1min %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait2min &lt;- nDetections_2min %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait4min &lt;- nDetections_4min %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) # Calculate overall number of detections for each site as a function of rainforest species and open-country species and test for differences across treatment types (Calculated only for the smallest temporal duration - 10s) detections_trait10s &lt;- detections_trait10s %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) %&gt;% drop_na() # Split the above data into rainforest species and open country detections_10s_rainforest &lt;- detections_trait10s %&gt;% filter(Habitat==&quot;RF&quot;) detections_10s_openCountry &lt;- detections_trait10s %&gt;% filter(Habitat==&quot;OC&quot;) # Test if there are significant differences in detections across treatment types as a function of species trait anova_rainforestDet &lt;- aov(sumDetections~Restoration.type, data = detections_10s_rainforest) anova_opencountryDet &lt;- aov(sumDetections~Restoration.type, data = detections_10s_openCountry) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukey_rainforestDet &lt;- TukeyHSD(anova_rainforestDet) tukey_opencountryDet &lt;- TukeyHSD(anova_opencountryDet) # The above results for rainforest birds reveal a significant difference in rainforest bird detections between benchmark sites and active sites and benchmark sites and passive sites. # For open-country birds, there is a significant difference between every pair of treatment type. # Calculating overall number of detections for other temporal durations as a function of species trait detections_trait30s &lt;- detections_trait30s %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) detections_trait1min &lt;- detections_trait1min %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) detections_trait2min &lt;- detections_trait2min %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) detections_trait4min &lt;- detections_trait4min %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) # Plot the figures # reordering factors for plotting detections_trait10s$Restoration.type &lt;- factor(detections_trait10s$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_detections_trait10s &lt;- ggplot(detections_trait10s, aes(x=Restoration.type, y=sumDetections, fill=Habitat)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;, labels=c(&quot;Open-country&quot;,&quot;Rainforest&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Cumulative number of detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) # Save the above plot ggsave(fig_detections_trait10s, filename = &quot;figs/fig_detections_trait10s.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["jackknife-estimates.html", "Section 5 Jackknife estimates", " Section 5 Jackknife estimates In this script, we will extract jackknife scores, which essentially extrapolates species richness for a given species pool. This calculation is based on the number of sites and the number of visits to each site and the number of singletons/doubletons (detecting a species only once/site and twice/site respectively). Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Load the necessary data to calculate Jackknife scores # We load the subset data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # Site-summary (Number of detections across all sites) datSummary &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) Preparing dataframe for jacknifing and extracting jacknife scores # Calculate the overall number of detections for each site across 6 days of data (translates to ~96-min of data per site; each detection corresponding to a temporal unit of 10 seconds). Here, we include dates, since each visit can explain the extrapolation of species richness when jackknife estimates are extracted. nDetections_site_date &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Combine the nDetections and trait based data to obtain a dataframe for jackknife estimates nDetections_trait &lt;- nDetections_site_date %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) # Extract jackknife scores # To do the same, we first prepare the dataframe in a manner where we have a matrix of Site by Date by Species name jacknifeAll &lt;- nDetections_trait %&gt;% dplyr::select(Site, Date, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Site, Date, Restoration.type, Species_Code) %&gt;% summarise(totDetections = sum(nDetections)) %&gt;% pivot_wider(names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) # Prepare a dataframe of rainforest species for jacknifing jacknife_rainForest &lt;- nDetections_trait %&gt;% filter(Habitat==&quot;RF&quot;) %&gt;% dplyr::select(Site, Date, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Site, Date, Restoration.type, Species_Code) %&gt;% summarise(totDetections = sum(nDetections)) %&gt;% pivot_wider(names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) # Prepare a dataframe of open-country species for jacknifing jacknife_openCountry &lt;- nDetections_trait %&gt;% filter(Habitat==&quot;OC&quot;) %&gt;% dplyr::select(Site, Date, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Site, Date, Restoration.type, Species_Code) %&gt;% summarise(totDetections = sum(nDetections)) %&gt;% pivot_wider(names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) Extract jacknife scores jackAllScore &lt;- specpool(jacknifeAll[,4:ncol(jacknifeAll)], pool = jacknifeAll$Site) %&gt;% rownames_to_column(&quot;Site&quot;) %&gt;% add_column (Restoration.type = datSummary$Restoration.type) # write out results write.csv(jackAllScore, &quot;data/jackAll.csv&quot;, row.names=F) jack_rainForestScore &lt;- specpool(jacknife_rainForest[,4:ncol(jacknife_rainForest)], pool = jacknife_rainForest$Site) %&gt;% rownames_to_column(&quot;Site&quot;) %&gt;% add_column (Restoration.type = datSummary$Restoration.type) %&gt;% mutate(Habitat = &quot;RF&quot;) # write out results write.csv(jack_rainForestScore,&quot;data/jackRainforest.csv&quot;, row.names = F) jack_openCountryScore &lt;-specpool(jacknife_openCountry[,4:ncol(jacknife_openCountry)], pool = jacknife_openCountry$Site) %&gt;% rownames_to_column(&quot;Site&quot;) %&gt;% add_column (Restoration.type = datSummary$Restoration.type) %&gt;% mutate(Habitat = &quot;OC&quot;) # write out results write.csv(jack_openCountryScore, &quot;data/jackOpencountry.csv&quot;, row.names = F) Looking at correlations between jacknife scores and bootstrap estimates # This plot suggests an almost 1:1 correlation between jacknife estimates and bootstrap scores plot(jackAllScore$jack1, jackAllScore$boot) Plotting jacknife estimates and testing for any significant differences between treatment types # Test if there are significant differences in jacknife estimates across treatment types anovaJackAll &lt;- aov(jack1~Restoration.type, data = jackAllScore) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukeyJackAll &lt;- TukeyHSD(anovaJackAll) # The above result suggests that there is no significant different in jacknife scores between treatment types # Create a boxplot of jacknife estimates by group (Here: group refers to Restoration Type) # reordering factors for plotting jackAllScore$Restoration.type &lt;- factor(jackAllScore$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_jackAll &lt;- ggplot(jackAllScore, aes(x=Restoration.type, y=jack1, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Jackknife estimates\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_jackAll, filename = &quot;figs/fig_jackAll.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() Lets test for significant differences in jacknife estimates as a function of species trait anovaJack_rainForest &lt;- aov(jack1~Restoration.type, data = jack_rainForestScore) anovaJack_openCountry &lt;- aov(jack1~Restoration.type, data = jack_openCountryScore) # Tukey test to study each pair of treatment tukeyJack_rainForest &lt;- TukeyHSD(anovaJack_rainForest) tukeyJack_openCountry &lt;- TukeyHSD(anovaJack_openCountry) # For rainforest species - there is no significant difference in jacknife estimates between any treatment types, while for open-country birds; there is a significant difference in jacknife estimates across active-benchmark and passive-benchmark # Plot the above results jackTrait &lt;- bind_rows(jack_rainForestScore, jack_openCountryScore) # reordering factors for plotting jackTrait$Restoration.type &lt;- factor(jackTrait$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Rainforest species fig_jackTrait &lt;- ggplot(jackTrait, aes(x=Restoration.type, y=jack1, fill=Habitat)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;, labels=c(&quot;Open-country&quot;,&quot;Rainforest&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Jackknife estimates\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) ggsave(fig_jackTrait, filename = &quot;figs/fig_jackTrait.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() # Please note that this figure is required to create the Fig 2a in the next script. "],["non-metric-multidimensional-scaling-and-indicator-species-analysis.html", "Section 6 Non-metric multidimensional scaling and indicator-species analysis", " Section 6 Non-metric multidimensional scaling and indicator-species analysis Here, we are interested not only in comparing univariate descriptors of communities, like diversity, but also in how the constituent species  or the composition  changes from one community to the next. One tool to do this is non-metric multidimensional scaling, or NMDS. The goal of NMDS is to collapse information from multiple dimensions (e.g, from multiple communities, sites, etc.) into just a few, so that they can be visualized and interpreted. Unlike other ordination techniques that rely on (primarily Euclidean) distances, such as Principal Coordinates Analysis, NMDS uses rank orders, and thus is an extremely flexible technique that can accommodate a variety of different kinds of data (The text above was copied from the link below). NMDS does not use the absolute abundances of species in communities, but rather their rank orders. The use of ranks omits some of the issues associated with using absolute distance (e.g., sensitivity to transformation), and as a result is much more flexible technique that accepts a variety of types of data. (Its also where the non-metric part of the name comes from). A wonderful tutorial is presented in this link: https://jonlefcheck.net/2012/10/24/nmds-tutorial-in-r/ Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(ecodist) library(RColorBrewer) library(ggforce) library(ggalt) library(indicspecies) library(patchwork) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Load the necessary data for NMDS calculations # We load the subset data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # Site-summary (Number of detections across all sites) datSummary &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # richness across sites (converting detections to 1) richness &lt;- datSummary %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, richness) Preparing dataframe of traits and species to be used for NMDS later on # Calculate the overall number of detections for each site. Here, we include dates, since each visit can explain the extrapolation of species richness when jackknife estimates are extracted. nDetections_site_date &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Combine the nDetections and trait based data to obtain a dataframe for further analysis nDetections_trait &lt;- nDetections_site_date %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) Preparing a dataframe of detections to run ordinations # We will prepare a community matrix data that can be used to run dissimilarity indices nmdsDat &lt;- nDetections_trait %&gt;% dplyr::select(Site, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Species_Code, Site, Restoration.type) %&gt;% summarise (totDetections = sum(nDetections)) %&gt;% arrange(Restoration.type) %&gt;% pivot_wider (names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) # Convert to matrix form nmdsDatMatrix &lt;- as.matrix(nmdsDat[, 3:ncol(nmdsDat)]) Run a bray-curtis dissimilarity index and identify least stressed configuration for the ordinations. Bray-curtis is a statistic used to quantify the compositional dissimilarity between two different sites, based on counts at each site. Ecologists use the Bray-Curtis dissimilarity calculation, which has a number of ideal properties: 1. It is invariant to changes in units. 2. It is unaffected by additions/removals of species that are not present in two communities. 3. It is unaffected by the addition of a new community. 4. It can recognize differences in total abundances when relative abundances are the same. Please note that this link provides more information on NMDS: http://strata.uga.edu/8370/lecturenotes/multidimensionalScaling.html # Run a bray-curtis dissimilarity index and use metaMDS function from vegan to run ordinations disBrayCurtis &lt;- vegdist(nmdsDatMatrix, method = &quot;bray&quot;) nmdsBrayCurtis &lt;- vegdist (nmdsDatMatrix, method = &quot;bray&quot;) %&gt;% metaMDS (nmdsBrayCurtis, k=6) # extract nmds scores nmdsScores &lt;- as.tibble(scores(nmdsBrayCurtis)) # Write the scores to a separate .csv write.csv(nmdsScores, &quot;data/nmdsBrayCurtis-bird-detections.csv&quot;, row.names = F) # With the above analysis, we note the stress is 0.06107898. However, if stress is high, we should reposition the points in 2 dimensions in the direction of decreasing stress, and repeat until stress is below some threshold.**A good rule of thumb: stress &lt; 0.05 provides an excellent representation in reduced dimensions, &lt; 0.1 is great, &lt; 0.2 is good/ok, and stress &lt; 0.3 provides a poor representation.** To reiterate: high stress is bad, low stress is good! Plotting the NMDS scores using ggplot2 # First let&#39;s add the treatment type back to the nmds scores nmdsScores$Restoration.type &lt;- nmdsDat$Restoration.type # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting nmdsScores$Restoration.type &lt;- factor(nmdsScores$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_nmds &lt;- ggplot(data=nmdsScores) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=5) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScores$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScores$NMDS1), y = max(nmdsScores$NMDS2), label = &quot;Stress = 0.05&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) ggsave(fig_nmds, filename = &quot;figs/fig_nmds_birdDetections.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() Main text Figure 2 # reload previous figures to create part A and B for the main text Fig. 2 fig_jack_ordinations &lt;- wrap_plots(fig_jackTrait, fig_nmds, design = &quot;AABBBB&quot; ) + plot_annotation( tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot; ) # Expand the width to avoid compression ggsave(fig_jack_ordinations, filename = &quot;figs/fig2.png&quot;, width=20, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() Jackknife estimates of bird species richness and NMDS ordination results of bird species detections (a) We observed no significant difference in the first-order jackknife scores across the three treatment types when we considered rainforest bird species. The jackknife estimate of open-country bird species varied significantly between BM-AR sites and BM-NR sites (Tukey HSD test, P &lt; 0.05), with NR sites having the highest estimate (mean ± SD: 24 ± 5), followed by AR sites (mean ± SD: 20 ± 6) and BM sites (mean ± SD: 15 ± 4). (b) The ordination analysis of bird detections data (stress = 0.05) revealed distinct clusters of BM sites, but relatively loose clusters for AR and NR sites. However, AR sites occupied an intermediate position between BM and NR sites, indicating a direction of change in bird community composition toward BM sites. In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites and NR = Naturally regenerating forest sites. Testing multivariate homogeneity of group dispersions One measure of multivariate dispersion (variance) for a group of samples is to calculate the average distance of group members to the group centroid or spatial median (both referred to as centroid from now on unless stated otherwise) in multivariate space. To test if the dispersions (variances) of one or more groups are different, the distances of group members to the group centroid are subject to ANOVA. Betadisper tests whether two or more groups (for example, restored and unrestored sites) are homogeneously dispersed in relation to their species in studied samples. This test can be done to see if one group has more compositional variance than another. Moreover, homogeneity of dispersion among groups is very advisable to have if you want to test if two or more groups have different compositions, which is tested by adonis. nmdsVariance &lt;- betadisper(disBrayCurtis, group = nmdsDat$Restoration.type) nmdsVariance anova(nmdsVariance) permutest(nmdsVariance, pairwise = TRUE, permutations = 999) TukeyHSD(nmdsVariance) # These results suggest that there is no difference in variance between one group and another Visualizing the multivariate homogeneity of group dispersions The below lines of code have been adapted from: https://chrischizinski.github.io/rstats/adonis/ # extract the centroids and the site points in multivariate space. centroids &lt;-data.frame(grps=rownames(nmdsVariance$centroids), data.frame(nmdsVariance$centroids)) vectors &lt;- data.frame(group=nmdsVariance$group, data.frame(nmdsVariance$vectors)) # to create the lines from the centroids to each point we will put it in a format that ggplot can handle seg.data&lt;-cbind(vectors[,1:3],centroids[rep(1:nrow(centroids),as.data.frame(table(vectors$group))$Freq),2:3]) names(seg.data)&lt;-c(&quot;group&quot;,&quot;v.PCoA1&quot;,&quot;v.PCoA2&quot;,&quot;PCoA1&quot;,&quot;PCoA2&quot;) # create the convex hulls of the outermost points grp1.hull &lt;- seg.data[seg.data$group==&quot;Active&quot;,1:3][chull(seg.data[seg.data$group==&quot;Active&quot;,2:3]),] grp2.hull &lt;- seg.data[seg.data$group==&quot;Benchmark&quot;,1:3][chull(seg.data[seg.data$group==&quot;Benchmark&quot;,2:3]),] grp3.hull &lt;- seg.data[seg.data$group==&quot;Passive&quot;,1:3][chull(seg.data[seg.data$group==&quot;Passive&quot;,2:3]),] all.hull &lt;- rbind(grp1.hull,grp2.hull,grp3.hull) # plot the panel and convex hulls fig_hull &lt;- ggplot() + geom_polygon(data= all.hull,aes(x=v.PCoA1,y=v.PCoA2),colour=&quot;black&quot;,alpha=0,linetype=&quot;dashed&quot;) + geom_segment(data=seg.data,aes(x=v.PCoA1,xend=PCoA1,y=v.PCoA2,yend=PCoA2),alpha=0.30) + geom_point(data=centroids[,1:3], aes(x=PCoA1,y=PCoA2,shape=grps),size=4,colour=&quot;red&quot;) + geom_point(data=seg.data, aes(x=v.PCoA1,y=v.PCoA2,shape=group),size=2) + labs(title=&quot;All&quot;,x=&quot;&quot;,y=&quot;&quot;) + #coord_cartesian(xlim = c(-0.2,0.2), ylim = c(-0.25,0.2)) + theme_bw() + theme(legend.position=&quot;none&quot;) Testing compositional dissimilarity between groups We will do this by using the vegan::adonis() function which allows you to run permutational multivariate analysis of variance using distance matrices. In the above figure, the NMDS confidence ellipses suggest that there is a significant difference between benchmark and passive-active sites, but no difference between active and passive sites. Adonis works by first finding the centroids for each group and then calculates the squared deviations of each of site to that centroid. Then significance tests are performed using F-tests based on sequential sums of squares from permutations of the raw data. Please note that adonis analyzes and partitions sums of squares using distance matrices. It can be thought of as an ANOVA using distance matrices (analogous to MANOVA - multivariate analysis of variance). Therefore, it is used to test if two or more groups have similar compositions. # We will use the NMDS scores for axis 1 and axis 2 to test for compositional dissimilarity groups &lt;- nmdsScores$Restoration.type adonisNMDS &lt;- adonis(nmdsDatMatrix ~ groups, method=&quot;bray&quot;,perm=999) adonisNMDS # The results suggest that there are significant compositional differences between groups. Indicator species analysis This analysis aims to identify what species are indicators of groups of samples or treatment type. For instance:  A perfect indicator species will only occur in one group.  Generalist species will occur across multiple groups and therefore not a good indicator.  Rare species are not sampled often, also not a good indicator.  Indicator values range from 0 to 1 but may also be expressed as a percentage.  Each species is assigned an indicator value for each group and the significance of values can be assessed through permutations For more information, visit this link: https://ichthyology.usm.edu/courses/multivariate/apr_11.pdf Please note that when the aim is to determine which species can be used as indicators of certain site group, an approach commonly used in ecology is the Indicator Value [Dufrene and Legendre, 1997]. These authors defined an Indicator Value (IndVal) index to measure the association between a species and a site group. The method of Dufrene and Legendre [1997] calculates the IndVal index between the species and each site group and then looks for the group corresponding to the highest association value. Finally, the statistical significance of this relationship is tested using a permutation test. Diagnostic (or indicator) species are an important tool in vegetation science, because these species can be used to characterize and indicate specific plant community types. A statistic commonly used to determine the association (also known as fidelity, not to be confounded with the indicator value component) between species and vegetation types is Pearsons phi coefficient of association [Chytr´y et al., 2002]. This coefficient is a measure of the correlation between two binary vectors. The abundance-based counterpart of the phi coefficient is called the point biserial correlation coefficient (which is defined as r.g) See: https://cran.r-project.org/web/packages/indicspecies/vignettes/indicspeciesTutorial.pdf # Running the multi-pattern analysis across combinations of groups indicSpec &lt;- multipatt(nmdsDat[, 3:ncol(nmdsDat)], nmdsDat$Restoration.type, func = &quot;r.g&quot;, control = how(nperm=999)) # looking at associations with stratum/pairs of strata summary(indicSpec) Separate analysis: Instead of using bird detections, using richness to run nmds ordinations nmdsDatRichness &lt;- datSummary %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) # Convert to matrix form nmdsDatMatrixRichness &lt;- as.matrix(nmdsDatRichness[, 3:ncol(nmdsDatRichness)]) # Run a bray-curtis dissimilarity index and use metaMDS function from vegan to run ordinations disBrayCurtisRichness &lt;- vegdist(nmdsDatMatrixRichness, method = &quot;bray&quot;) nmdsBrayCurtisRichness &lt;- vegdist(nmdsDatMatrixRichness, method = &quot;bray&quot;) %&gt;% metaMDS (nmdsBrayCurtisRichness, k=6) # extract nmds scores nmdsScoresRichness &lt;- as.tibble(scores(nmdsBrayCurtisRichness)) # plot the data # First let&#39;s add the treatment type back to the nmds scores nmdsScoresRichness$Restoration.type &lt;- nmdsDatRichness$Restoration.type # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting nmdsScoresRichness$Restoration.type &lt;- factor(nmdsScoresRichness$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_nmds_richness &lt;- ggplot(data=nmdsScoresRichness) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=4) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScoresRichness$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScoresRichness$NMDS1), y = max(nmdsScoresRichness$NMDS2), label = &quot;Stress = 0.1&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) ggsave(fig_nmds_richness, filename = &quot;figs/fig_nmds_birdRichness.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["preparing-vegetation-data-for-glms.html", "Section 7 Preparing vegetation data for GLMs", " Section 7 Preparing vegetation data for GLMs Here, we prepare the vegetation data to run GLMMs in the next series of scripts. Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Load the vegetation data veg &lt;- read.csv(&quot;data/2020-vegetation-data.csv&quot;) veg$Site_ID &lt;- str_remove(veg$Site_ID,&quot;_&quot;) # We load the subset data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) sites_needed &lt;- data.frame(unique(datSubset$Site)) names(sites_needed) &lt;- &quot;Site_ID&quot; # Obtain a subset of the data which have the sites visited veg &lt;- merge(veg, sites_needed, by.x = &quot;Site_ID&quot;, by.y=&quot;Site_ID&quot;) # renaming restoration type veg$Site_type[veg$Site_type==&quot;Unrestored&quot;] &lt;- &quot;Passive&quot; veg$Site_type[veg$Site_type==&quot;Restored&quot;] &lt;- &quot;Active&quot; # Load species-trait data to essentially run GLMs trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) Carrying out exploratory analysis and preparing a dataframe for further steps # Counting number of tree species and unique species per plot treerich &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise (count = n(), richness = n_distinct(tree_species)) # Calculate average tree height across each unique site treeheight &lt;- veg %&gt;% drop_na(height) %&gt;% group_by(Site_ID) %&gt;% summarise(height = mean(height)) # Calculate basal area and left join with other data basal_area &lt;- veg %&gt;% mutate(basal_sum = rowSums(veg[,c(5:15)]^2)/(4*pi)) %&gt;% group_by(Site_ID, Site_type) %&gt;% summarise(basal_area = sum(basal_sum)) # Calculate average canopy height canopy_height &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(canopy_cover = mean(Canopy_cover)) # Calculate average leaf litter leaf_litter &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(leaf_litter = mean(Leaf_litter)) # Calculate average vertical stratification vert_strat &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(vert_strat = mean(Foliage_score)) # Year of planting plantingYear &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(plantingYear = unique(Year.of.planting)) # Creating a final dataframe for further analysis allVeg &lt;- basal_area %&gt;% left_join(treeheight) %&gt;% left_join(treerich) %&gt;% left_join(canopy_height) %&gt;% left_join(leaf_litter) %&gt;% left_join(vert_strat) %&gt;% left_join(plantingYear) write.csv(allVeg, &quot;data/summaryVeg.csv&quot;, row.names = F) Running a PCA of the vegetation data # Check for correlations among vegetation predictors pairs.panels(allVeg[,3:9]) # The above panel suggests that richness is highly correlated with a number of predictors including canopy cover, count and basal area. We will calculate a PCA and keep the top two explanatory axes vegPca &lt;- prcomp(allVeg[, 3:9], scale=TRUE, center = TRUE, retx=TRUE) summary(vegPca) # The proportion of variance explained by the first two axes account for ~73.42% # Extract PCA values PCAvalues &lt;- data.frame(&#39;Site_ID&#39;=allVeg$Site_ID, &#39;Site_type&#39; = allVeg$Site_type, vegPca$x[,1:2]) # the first two components are selected # save the data for use in a GLM later write.csv(PCAvalues,&quot;data/pcaVeg.csv&quot;, row.names = F) # Extract loadings of the variables PCAloadings &lt;- data.frame(variables = rownames(vegPca$rotation), vegPca$rotation) # figure below # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting PCAvalues$Site_type &lt;- factor(PCAvalues$Site_type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_pca &lt;- ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Site_type)) + geom_segment(data = PCAloadings, aes(x = 0, y = 0, xend = (PC1*5), yend = (PC2*5)), arrow = arrow(length = unit(1/2, &quot;picas&quot;)), color = &quot;black&quot;) + geom_point(aes(x=PC1, y=PC2, shape= Site_type, colour = Site_type),size=5) + annotate(&quot;text&quot;, x = (PCAloadings$PC1*3), y = (PCAloadings$PC2*3), label = PCAloadings$variables, family = &quot;Century Gothic&quot;, size=5) + theme_bw() + scale_x_continuous(name=&quot;PC1 (54.67%)&quot;) + scale_y_continuous(name=&quot;PC2 (18.74%)&quot;) + scale_color_manual(&quot;Treatment type&quot;,values = mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + scale_shape_manual(&quot;Treatment type&quot;, values= 1:length(unique(PCAvalues$Site_type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) ggsave(fig_pca, filename = &quot;figs/fig_pca.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["generalized-linear-mixed-modeling-species-richness-vegetation-data-and-planting-year.html", "Section 8 Generalized linear mixed modeling (species richness, vegetation data and planting year)", " Section 8 Generalized linear mixed modeling (species richness, vegetation data and planting year) In this script, we run generalized linear mixed models to test the association between first order jacknife scores and restoration type. In addition, we run generalized linear mixed models to test associations between species richness and habitat (vegetation structure) using site-pair name (actively restored and naturally regenerating were specified to be paired) and repeat visits as random effects. Lastly, we assess associations between year since restoration began and first order jackknife scores of birds. Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Load the necessary data for statistical modeling # We load the subset data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # Total number of detections totalDetections &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% summarise(totalDetections = sum(c_across(&quot;IP&quot;:&quot;HSWP&quot;))) # richness richness &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, richness) # richness by Visit # this data basically gives you richness per visit and adds a visit number for each consecutive visit to that site richnessPerVisit &lt;- datSubset %&gt;% group_by(Site, Date, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% dplyr::select(Site, Restoration.type, Date, richness, visit, siteCode) # We further estimate the number of detections per point count datSummary &lt;- totalDetections %&gt;% left_join(richness) %&gt;% mutate(pointDetections = totalDetections/3) # combine the Detections dataframe with the trait dataset nDetectionsTrait &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) # Load data from previous scripts for use in a GLM vegData &lt;- read.csv(&quot;data/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;data/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) jackAll &lt;- read.csv(&quot;data/jackAll.csv&quot;) jackRainforest &lt;- read.csv(&quot;data/jackRainforest.csv&quot;) jackOpencountry &lt;- read.csv(&quot;data/jackOpencountry.csv&quot;) Getting data ready in a format for generalized linear modeling # All birds modelDataAll &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% add_column(jacknife = jackAll$jack1, year = vegData$plantingYear, richness = richness$richness) %&gt;% mutate (&quot;roundjk&quot; = round(jacknife)) # rainforest birds modelData_rainForest &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% add_column(jacknife = jackRainforest$jack1, year = vegData$plantingYear) %&gt;% mutate (&quot;roundjk&quot; = round(jacknife)) # open country birds modelData_openCountry &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% add_column(jacknife = jackOpencountry$jack1, year = vegData$plantingYear) %&gt;% mutate (&quot;roundjk&quot; = round(jacknife)) Getting data ready for generalized linear mixed modeling with richness data as well as visits. # data for the GLMM (overall richness) glmmAll &lt;- richnessPerVisit[,-2] %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;)) # rainforest birds richness for glmm glmmRainforest &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (Habitat == &quot;RF&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelData_rainForest, by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # open-country birds glmmOpencountry &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (Habitat == &quot;OC&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelData_openCountry, by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # Let&#39;s look at species by foraging habit # canopy birds glmmCanopy &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (Habit == &quot;CAN&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # ground-feeding birds glmmGround &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (Habit == &quot;GRD&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # mid-storey birds glmmMidStorey &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (Habit == &quot;MID&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # understorey birds glmmUnderStory &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (Habit == &quot;UND&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) Running the generalized linear mixed models We now run generalized linear models (GLM) assuming Poisson errors and using log link functions to examine the effects of restoration type (benchmark, actively restored and passively restored) on the jackknife estimates of bird species richness (for all, rainforest, and open-country species), followed by TukeyHSD multiple comparisons tests of means. # all birds glmm_alljk &lt;- glmer(roundjk ~ Restoration.type +(1|siteCode), data = modelDataAll, family = poisson(link = log)) summary(glmm_alljk) tukey_glmmAllJack &lt;- summary(glht(glmm_alljk, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmAllJack) # The above result suggests that there is a significant difference in first order jacknife estimates for benchmark sites and passively restored site (but no difference between active-passive and active-benchmark). # rainforest birds glmm_rainForestJack &lt;- glmer(roundjk ~ Restoration.type + +(1|siteCode), data = modelData_rainForest, family = poisson(link = log)) summary(glmm_rainForestJack) tukey_glmmRainForestJack &lt;- summary(glht(glmm_rainForestJack, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmRainForestJack) # The above result suggests no significant difference in means between any treatment types # open country birds glmm_openCountryJack &lt;- glmer(roundjk ~ Restoration.type + +(1|siteCode), data = modelData_openCountry, family = poisson(link = log)) summary(glmm_openCountryJack) tukey_glmmOpenCountryJack &lt;- summary(glht(glmm_openCountryJack, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmOpenCountryJack) # For open country birds, there is a significant difference in first-order jacknife estimates between benchmark and passive sites and benchmark and active sites Running generalized linear mixed models Testing the role of habitat (vegetation structure) and foraging habit on species richness within a generalized linear modeling framework # Testing the role of habitat first # all bird species glmm_allBirds &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmAll, family = poisson(link = log)) summary(glmm_allBirds) # significant negative association with PC2 # rainforest birds glmm_Rainforest &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmRainforest, family = poisson(link = log)) summary(glmm_Rainforest) # Results above suggest PC2 is significantly negatively associated with richness of rainforest birds. A similar result is seen below for open-country bird species richness and PC1 glmm_openCountry &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmOpencountry, family = poisson(link = log)) summary(glmm_openCountry) # Testing the role of foraging habit # canopy birds (no significant association) glmm_Canopy &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmCanopy, family = poisson(link = log)) summary(glmm_Canopy) # ground-feeding birds (Marginal association between richness of ground-feeding birds and PC2) glmm_Ground &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmGround, family = poisson(link = log)) summary(glmm_Ground) # mid-storey birds (Marginal association between PC1 and richness of mid-storey birds) glmm_MidStorey &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmMidStorey, family = poisson(link = log)) summary(glmm_MidStorey) # understory birds (no significant association) glmm_Understory &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmUnderStory, family = poisson(link = log)) summary(glmm_Understory) Lastly, running a generalized linear model to test the effect of year since restoration on jacknife estimates of species richness # Let&#39;s look at overall species richness first # filter only data for restored sites allRestored &lt;- modelDataAll %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2021-year)) glmAll &lt;- glm(richness ~ yearSinceRestoration, data = allRestored, family = poisson(link = log)) summary(glmAll) # rainforest bird richness rainRestored &lt;- modelData_rainForest %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2021-year)) glmRain &lt;- glm(roundjk ~ yearSinceRestoration, data = rainRestored, family = poisson(link = log)) summary(glmRain) # open-country richness openRestored &lt;- modelData_openCountry %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2021-year)) glmOpen &lt;- glm(roundjk ~ yearSinceRestoration, data = openRestored, family = poisson(link = log)) summary(glmOpen) "],["multiple-regression-on-distance-matrices.html", "Section 9 Multiple regression on distance matrices", " Section 9 Multiple regression on distance matrices Load necessary libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(sf) library(sna) library(hier.part) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Load necessary data and create dissimilarity matrices to run MRM analyses # estimating geographic distance (euclidean) between sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% filter(!str_detect(Site.code, &#39;OLCAP5B&#39;)) sites &lt;- st_as_sf(sites, coords = c(&quot;Longitude&quot;,&quot;Latitude&quot;), crs=4326) sites &lt;- st_transform(sites, 32643) distanceData &lt;- dist(st_coordinates(sites), method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) distMatrix &lt;- as.matrix(distanceData) # getting vegetation PCA scores as a matrix vegPcaScores &lt;- read.csv(&quot;data/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- dist(vegPcaScores, method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) vegPcaScores &lt;- as.matrix(vegPcaScores) # load NMDS scores (run on bird detections) nmdsScores &lt;- read.csv(&quot;data/nmdsBrayCurtis-bird-detections.csv&quot;) birdMatrix &lt;- dist(nmdsScores[,1:2], method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) birdMatrix &lt;- as.matrix(birdMatrix) Extracting floristic data and running NMDS ordinations on the same # getting floristic data as a matrix, using NMDS scores # to do the above, we need to reload the vegetation data and process it veg &lt;- read.csv(&quot;data/2020-vegetation-data.csv&quot;) veg$Site_ID &lt;- str_remove(veg$Site_ID,&quot;_&quot;) # We load the subset data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) sites_needed &lt;- data.frame(unique(datSubset$Site)) names(sites_needed) &lt;- &quot;Site_ID&quot; # Obtain a subset of the data which have the sites visited veg &lt;- merge(veg, sites_needed, by.x = &quot;Site_ID&quot;, by.y=&quot;Site_ID&quot;) # renaming restoration type veg$Site_type[veg$Site_type==&quot;Unrestored&quot;] &lt;- &quot;Passive&quot; veg$Site_type[veg$Site_type==&quot;Restored&quot;] &lt;- &quot;Active&quot; # now extract floristics data (tree species abundance) floraData &lt;- veg %&gt;% dplyr::select(Site_ID, tree_species, Site_type) %&gt;% mutate(Number = 1) %&gt;% group_by(tree_species, Site_ID, Site_type) %&gt;% summarise (totalAbun = sum(Number)) %&gt;% pivot_wider (names_from = tree_species, values_from = totalAbun, values_fill = list(totalAbun=0)) %&gt;% arrange(.,Site_ID) # Convert to matrix form nmdsDatMatrix &lt;- as.matrix(floraData[, 3:ncol(floraData)]) # Run a bray-curtis dissimilarity index and use metaMDS function from vegan to run ordinations disBrayCurtis &lt;- vegdist(nmdsDatMatrix, method = &quot;bray&quot;) nmdsBrayCurtis &lt;- vegdist (nmdsDatMatrix, method = &quot;bray&quot;) %&gt;% metaMDS (nmdsBrayCurtis, k=6) # extract nmds scores nmdsScores &lt;- as.tibble(scores(nmdsBrayCurtis)) # stress is lowest at around 6 dimensions with value of 0.06152919. However, if stress is high, we should reposition the points in 2 dimensions in the direction of decreasing stress, and repeat until stress is below some threshold.**A good rule of thumb: stress &lt; 0.05 provides an excellent representation in reduced dimensions, &lt; 0.1 is great, &lt; 0.2 is good/ok, and stress &lt; 0.3 provides a poor representation.** To reiterate: high stress is bad, low stress is good! # create a distance matrix on the nmds scores of the flora data floraMatrix &lt;- dist(nmdsScores[,1:2], method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) floraMatrix &lt;- as.matrix(floraMatrix) Running multiple regression on distance matrices # First we get rid of redundant duplicate values, then make the diagonal zeros NA, unfold each matrix into a vector, and then omit rows with 0 in them birdVector &lt;- upper.tri.remove(birdMatrix) diag(birdVector) &lt;- NA birdVector &lt;- cbind(c(birdVector)) %&gt;% na.omit() floraVector &lt;- upper.tri.remove(floraMatrix) diag(floraVector) &lt;- NA floraVector &lt;- cbind(c(floraVector)) %&gt;% na.omit() pcaVector &lt;- upper.tri.remove(vegPcaScores) diag(pcaVector) &lt;- NA pcaVector &lt;- cbind(c(pcaVector)) %&gt;% na.omit() distanceVector &lt;- upper.tri.remove(distMatrix) diag(distanceVector) &lt;- NA distanceVector &lt;- cbind(c(distanceVector)) %&gt;% na.omit() %&gt;% scale #arranging data for MRM mrmData &lt;- bind_cols (birdVector, floraVector, pcaVector, distanceVector) %&gt;% rename (bird = ...1, flora = ...2, structure = ...3, distance = ...4) MRM(dist(bird) ~ dist(flora) + dist(structure) + dist(distance), data = mrmData, method = &quot;linear&quot;, nperm = 1000) # The above result suggests that bird compositional dissimilarity is not signficantly related to dissimilarity in habitat structure, geographic distance and florstics (Please note that the overall R2 values are very low) Hierarchical partitioning hier.part(mrmData$bird, mrmData[2:4], family = &quot;gaussian&quot;, gof = &quot;Rsqu&quot;, barplot = TRUE) "],["acoustic-space-use.html", "Section 10 Acoustic space use", " Section 10 Acoustic space use Install necessary libraries library(seewave) library(warbleR) library(tuneR) library(stringi) library(tidyverse) library(ggplot2) library(RColorBrewer) library(foreach) library(doSNOW) library(rlist) library(tictoc) library(patchwork) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) We are interested in creating a measure (3-dimensional matrix) of acoustic activity (x=hour, y=acoustic frequencies, z=proportion of all recordings in each time/frequency bin) Previous papers broadly calculated space use by ascertaining the proportion of recordings in a particular time and frequency bin that is above a certain amplitude. However, this proportion was estimated by counting the number of frequency peaks within a given recording (seewave::fpeaks()) and then scaling it to go from 0 to 1. In our analysis, we want to obtain an understanding of the overall acoustic activity for a given time and frequency bin. In other words, we want the area under the curve for a particular frequency contour. Note From Sound Analysis and Synthesis with R by Jerome Sueur The basic premise in calculating ASU is that we compute a Short-time discrete fourier transform for a given frequency bin size (obtained by dividing the sampling frequency/window length over which the fourier transform is computed, for example 48000Hz/256 = 187.5 is the bin size). However, the size of the frequency bin is inversely proportional to time (Uncertainty principle; Pg.312) which would mean a bin size of 187.5 corresponds to a time duration of 0.005s (1/187.5=0.005s). Ultimately what matters is a compromise between frequency resolution and temporal resolution (Fig 11.3; Pg. 313) Below, we have summarized approaches taken by previous studies: Aide et al. 2017: We aggregated recordings at time scale of hour of day and used a frequency bin size of 86.13 Hz and an amplitude filtering threshold of 0.02. So if the sampling rate is 22000 Hz, that would mean - 22000/86.13 ~ 256 frequency bins to divide up the frequency space. In this paper, there would be 24hr*256 bins = 6144 time/frequency bins Campos-Cerqueira et al. 2019: We aggregated recordings at the time scale of hour of day (24 h), used a frequency bin size of 172 Hz, and an amplitude filtering threshold of 0.003. So if the sampling rate is 22000 Hz, that would mean - 22000/172 ~ 128 frequency bins. This resulted in a threedimensional (x = hour, y = acoustic frequency, z = proportion of all recordings in each time/frequency bin with a frequency peak value &gt; 0.003 amplitude) matrix of acoustic activity with a total of 3,072 time/frequency bins (24 h × 128 frequency bins). Campos-Cerqueira and Aide 2017: To calculate the amplitude, we used the meanspec (f = 44,100, wl = 256, wn = hanning) and fpeaks (threshold = 0.1, freq = 172) function from the seewave package in R (Sueur et al., 2008a). The value of each peak was normalized using the maximum amplitude value within all recordings in the soundscape (i.e., site), and thus values ranged from 0 to 1. The number of frequency peaks was determined by counting the number of recordings with a peak within each of the 128 frequency bins that were equal or greater than the amplitude threshold. To control for the different number of recordings in each site and each time interval (i.e., hour), we divided the number of recordings with a peak in each time/frequency class by the total number of recordings collected during each hourly interval. To calculate ASU in our study: We aggregate recordings for a multiple days across multiple sites (Keep only data that has been recorded continuously for a 24hr period) # List the path that contains all folders, which contain the audiomoth data path &lt;- &quot;D:\\\\2020-summer\\\\&quot; # Listing the folders within which .WAV files are stored folders &lt;- dir(path, recursive=F,full.names=T) ###### Please note ###### # The below set of lines need to be run only once for a given set of sites and days. # Let&#39;s first rename the files by name of each site (as prefix) for(i in 1:length(folders)){ setwd(folders[i]) # List the files within each folder and renaming the files with the prefix - SITE_ID a &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;), full.names = T) file.rename(from = a, to=paste0(basename(folders)[i],&quot;_&quot;,basename(a))) } ###### Ending note here ###### # Now get only those files for a full 24 hours across every unique site files &lt;- list() for(i in 1:length(folders)){ a &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;), full.names = T) site_date &lt;- str_extract(basename(a),&#39;\\\\w+_\\\\d+_&#39;) # Choosing all 24 hours of data across every unique site (288 corresponds to 12 files every 1 hour) for(j in 1:length(unique(site_date))){ dat &lt;- a[str_detect(a,unique(site_date)[j])] if((length(dat)&lt;288)==TRUE){ next } else { files &lt;- c(files, dat) } } } files &lt;- unlist(files) Aggregate recordings for any single day for every unique site and sort it in order (between 00:00:00 to 23:55:00 hrs) and then loop it across multiple days ASU is being calculated for every one hour period and showcased for a 24 hour period of data. ASU here is defined as the overall area occupied by the frequency contour.In our case, the sampling rate of our signal is 48,000 Hz (in future iterations, this value can be changed depending on the range of frequency values for which we would want to estimate acoustic space use) # Select all unique site_date combinations for each unique site site_date &lt;- str_extract(basename(files),&#39;\\\\w+_\\\\d+_&#39;) unique(site_date) # The only step that involves sub-selection (Selecting 5 days across each site; Note: IPTO06R has 3 days sampled and OLV110R has only 2 days sampled) site_date &lt;- unique(site_date)[c(1:5,10:14,19:23,28:47,52:61,71:75, 86:90,97:101,107:111,118:122,130:134, 141:143,150:154,161:165,173:177,188:189, 193:197,202:206,211:215,219:223,225:229, 231:235,237:241,249:253,261:265,269:273, 281:285,289:293,300:304,309:313,319:323, 329:333,340:344,351:355,361:365,370:374, 378:382,387:391,396:400)] # Create a sequence of numbers to combine files by 1 hour hour_seq &lt;- seq(from=0,to=288, by=12) # To name files with a suffix for each hour time_of_day &lt;- c(&quot;00:00-01:00&quot;,&quot;01:00-02:00&quot;,&quot;02:00-03:00&quot;,&quot;03:00-04:00&quot;, &quot;04:00-05:00&quot;,&quot;05:00-06:00&quot;,&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;, &quot;08:00-09:00&quot;,&quot;09:00-10:00&quot;,&quot;10:00-11:00&quot;,&quot;11:00-12:00&quot;, &quot;12:00-13:00&quot;,&quot;13:00-14:00&quot;,&quot;14:00-15:00&quot;,&quot;15:00-16:00&quot;, &quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;,&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;, &quot;20:00-21:00&quot;,&quot;21:00-22:00&quot;,&quot;22:00-23:00&quot;,&quot;23:00-24:00&quot;) # Loading parameters necessary for the Short-term fourier transform to be performed on hourly aggregates of data for each site_date combination f &lt;- 48000 wl &lt;- 256 # This window length should be changed as a function of the frequency resolution (ie. bin size) and temporal resolution (ie. time) ovlp &lt;- 50 wn &lt;- &quot;hanning&quot; # Store the 24 hour acoustic space use data in a list and name it by a unique site and date site_date_asu &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar( min = 0, max = length(unique(site_date)), style = 3 ) # Select only 24 hours of data (00:00:00 to 23:55:00) for every unique site-date for(i in 1:length(unique(site_date))){ tic(&quot;Total time for a single site-day combination&quot;) # Store the acoustic space use data in a data.frame for plotting and analysis space_use &lt;- data.frame() # Extract the strings first by site dat &lt;- files[stringr::str_detect(files,unique(site_date)[i])] # Parallelize the runs cl &lt;- makeCluster(6, type = &quot;SOCK&quot;) registerDoSNOW(cl) # Store the each hour of data for an entire day as a list here (raw audio files read by tuneR::readWave()) tic(&quot;Reading in .WAV files&quot;) dailydata &lt;- foreach(k=1:length(dat), .combine = &#39;c&#39;, .inorder = T, .packages = &#39;tuneR&#39;) %dopar% { r &lt;- readWave(dat[k]) } toc() gc() stopCluster(cl) # renaming the files to ensure that data is read in hour by hour and stored in a separate object called hourlydata # Below we read the first 12 files in and then save it after computing the STDFT and then repeating it for the next 12 files and so on for(t in 1:(length(hour_seq)-1)) { # Every 12 files correspond to one hour tic(&quot;Computing Short-term fourier transforms for each hour&quot;) if (t == 1) { hourlydata &lt;- dailydata[hour_seq[t]:hour_seq[t+1]] } else { hourlydata &lt;- dailydata[(hour_seq[t]+1):hour_seq[t+1]] } gc() # Parallelize the runs cl &lt;- makeCluster(6, type = &quot;SOCK&quot;) registerDoSNOW(cl) # Store every hour&#39;s ASU data here data_per_hour &lt;- foreach(z = 1:length(hourlydata), .combine = &#39;rbind&#39;, .inorder=T, .packages = &#39;seewave&#39;) %dopar% { wave &lt;- hourlydata[[z]] n &lt;- length(wave) ## Short-term Fourier transform (using a seewave internal function) m &lt;- sspectro(wave, f = f, wl = wl, ovlp = ovlp, wn = wn) # Frequency selection and frequency axis # Here, want only a sequence of numbers that correspond to the length of rows # of the short-time fourier transform and we divide it by 1000 to get values # in kHz freq &lt;- seq(0, (f/2) - (f/wl), length.out = nrow(m))/1000 # Calculate acoustic space use per frequency bin f.cont &lt;- apply(m, MARGIN = 1, FUN = sum) # Store the space use data in a dataframe for plotting later a &lt;- data.frame(freq, f.cont) } rm(hourlydata); gc() data_per_hour &lt;- data_per_hour %&gt;% group_by(freq) %&gt;% summarise(f.cont=sum(f.cont)) data_per_hour$f.cont &lt;- (data_per_hour$f.cont)/12 data_per_hour$time_of_day &lt;- time_of_day[t] space_use &lt;- rbind(data_per_hour, space_use) stopCluster(cl); gc() toc() } space_use &lt;- as.data.frame(space_use) site_date_asu &lt;- c(site_date_asu,list(space_use)) names(site_date_asu)[i] &lt;- unique(site_date)[i] rm(space_use, dailydata, data_per_hour); gc() setTxtProgressBar(pb, i) toc() } close(pb) # store the list object for later list.save(site_date_asu, &quot;data/site_date_asu.rdata&quot;) # Save the list of dataframes for future analysis siteByDayAsu &lt;- bind_rows(site_date_asu, .id = &quot;Site_Day&quot;) # Write to .csv write.csv(siteByDayAsu, &quot;data/site-by-day-asu.csv&quot;, row.names = F) Now that we have calculated acoustic space use values for a given frequency bin size and time resolution for every unique site-date combination, we would like to obtain a single set of space use values for every unique site. In other words, we would like to average space use values across all frequency bins for seven days (a week approximately). This is done for comparison between sites and treatment types. # All unique sites site &lt;- str_extract(basename(files),&#39;^([[:alnum:]])+&#39;) unique(site) # Store the site-wise ASU values site_asu &lt;- list() # Loop through site-wise data and average data for each site for(i in 1:length(unique(site))) { # Extract data needed for every unique site dat &lt;- site_date_asu[stringr::str_detect(names(site_date_asu),unique(site)[i])] # Sum up values of acoustic space use across X no. of days for every unique site asu_sum &lt;- dat %&gt;% bind_rows(.id=&quot;data&quot;) %&gt;% group_by(freq, time_of_day) %&gt;% summarize(f.cont.sum=sum(f.cont)) # Averaging data as a function of number of days for a given site over which data was summed asu_sum$f.cont.sum &lt;- (asu_sum$f.cont.sum)/length(dat) # Scaling the data between 0 to 1 for the sake of comparison across sites asu_sum$f.cont.sum &lt;- range01(asu_sum$f.cont.sum) asu_sum &lt;- as.data.frame(asu_sum) site_asu &lt;- c(site_asu, list(asu_sum)) names(site_asu)[i] &lt;- unique(site)[i] rm(dat,asu_sum) } # store the list object for later list.save(site_asu, &quot;data/site_asu.rdata&quot;) # Save the list of dataframes for future analysis siteWiseAsu &lt;- bind_rows(site_asu, .id = &quot;Site&quot;) # Write to .csv write.csv(siteWiseAsu, &quot;data/site-wise-asu.csv&quot;, row.names = F) Visualization of data We will use ggplot2 to plot and save individual ASU plots for each unique site-date combination. This will then be repeated for each unique site (calculated in the previous chunk of code) # Saving the ggplots to a folder for every unique site-date combination for(i in 1:length(site_date_asu)){ # Get the data out from a list to a datframe (else ggplot won&#39;t accept it) dat &lt;- data.frame(site_date_asu[i]) names(dat) &lt;- c(&quot;freq&quot;,&quot;f.cont&quot;,&quot;time_of_day&quot;) # Plot the data g1 &lt;- ggplot(dat, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ # scale_fill_scico(palette = &quot;vikO&quot;) + theme_bw() + labs(x=&quot;Time of Day (in hours)&quot;, y=&quot;Frequency (in kHz) &quot;) + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # Save the data to a folder ggsave(filename=&quot;&quot;,width=12, height=7, units=&quot;in&quot;, dpi = 300, plot=g1, device=&quot;png&quot;, path = paste(&quot;figs/siteByDayAsu/&quot;, paste(names(site_date_asu)[i], &quot;.png&quot;, sep=&quot;&quot;), sep=&quot;&quot;)) } # Saving the ggplots to a folder for every unique site for(j in 1:length(site_asu)){ # Get the data out from a list to a .dataframe (else ggplot won&#39;t accept it) dat &lt;- data.frame(site_asu[j]) names(dat) &lt;- c(&quot;freq&quot;,&quot;time_of_day&quot;,&quot;f.cont.sum&quot;) # Plot the data g1 &lt;- ggplot(dat, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ # scale_fill_scico(palette = &quot;vikO&quot;) + theme_bw() + labs(x=&quot;Time of Day (in hours)&quot;, y=&quot;Frequency (in kHz) &quot;) + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # Save the data to a folder ggsave(filename=&quot;&quot;,width=12, height=7, units=&quot;in&quot;, dpi = 300, plot=g1, device=&quot;png&quot;, path = paste(&quot;figs/siteWiseAsu/&quot;, paste(names(site_asu)[j], &quot;.png&quot;, sep=&quot;&quot;), sep=&quot;&quot;)) } Main text Figure 3 # load .rdata to plot select figures for publication site_asu &lt;- load(&quot;data/site_asu.rdata&quot;) site_asu &lt;- bind_rows(site_asu, .id=&quot;Site&quot;) # Selecting three representative sites for Fig. 3 (PANMP1B, INBS04R, INBS04U) bm &lt;- site_asu %&gt;% filter(Site==&quot;PANMP1B&quot;) ar &lt;- site_asu %&gt;% filter(Site==&quot;INBS04R&quot;) nr &lt;- site_asu %&gt;% filter(Site==&quot;INBS04U&quot;) # create separate figures for benchmark, active and passive sites and then patchwork them # benchmark site fig_bm &lt;- ggplot(bm, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ theme_bw() + labs(fill = &quot;Acoustic Space Use\\n&quot;) + theme(axis.title = element_blank(), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(bm$time_of_day), y = 23, label = &quot;Benchmark site&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5, fontface=&quot;bold&quot;) # actively restored site fig_ar &lt;- ggplot(ar, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ theme_bw() + labs(x=&quot;\\nTime of Day (in hours)&quot;, fill = &quot;Acoustic Space Use\\n&quot;) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), axis.title.y = element_blank(), legend.position = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = max(ar$time_of_day), y = 23, label = &quot;Actively restored site&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5, fontface=&quot;bold&quot;) # naturally regenerating site fig_nr &lt;- ggplot(nr, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ theme_bw() + labs(y=&quot;Frequency (in kHz)\\n&quot;, fill = &quot;Acoustic Space Use\\n&quot;) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), axis.title.x = element_blank(), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.position = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = max(nr$time_of_day), y = 23, label = &quot;Naturally regenerating site&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5, fontface=&quot;bold&quot;) # patchwork the above three for the Fig. 3 fig_asu &lt;- fig_nr + fig_ar + fig_bm ggsave(fig_asu, filename = &quot;figs/fig3.png&quot;, width=52, height=17,device = png(), units=&quot;cm&quot;, dpi = 300); dev.off() Acoustic space use across a naturally regenerating, actively restored and benchmark site Acoustic space use (ASU) is the proportion of frequency space that is used for a given time period (where used refers to the frequency space that is occupied by vocalizations/sounds above a threshold amplitude). In this figure, we showcase ASU for a naturally regenerating site, an actively restored site and a benchmark site. We observed largely empty frequency bins between 12 kHz to 24 kHz for the majority of AR and NR sites. Our visual examination also highlighted well established diurnal and nocturnal activity patterns of birds and insects, with frequencies between 0 kHz to 12 kHz showing higher values of ASU during the day (6AM to 6PM), and frequencies between 12 kHz to 24 kHz showing higher values of ASU during the night (6PM to 6AM). "],["non-metric-multidimensional-scaling-of-acoustic-space-use-data.html", "Section 11 Non-metric multidimensional scaling of acoustic space use data", " Section 11 Non-metric multidimensional scaling of acoustic space use data Here, we will use ordinations as a method of analyzing how acoustic space use (defined in terms of frequency and time, across 24 hours in a day) to test if space use follows a similar pattern of direction of change that was observed for species richness with the direction of change observed as moving from naturally regenerating sites to actively restored to benchmark sites. Load necessary libraries for analysis library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(ecodist) library(RColorBrewer) library(ggforce) library(ggalt) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Load the necessary data for nmds calculations # load the site-wise space use data siteWiseAsu &lt;- read.csv(&quot;data/site-wise-asu.csv&quot;) # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% dplyr::select(&quot;Site.code&quot;,&quot;Restoration.type&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # Add restoration type column to the space use data siteWiseAsu &lt;- left_join(siteWiseAsu,sites, by=c(&quot;Site&quot;=&quot;Site.code&quot;)) # Separate analysis: space use during certain times of day alone # let&#39;s look only at dawn chorus to explore what the data looks like dawn &lt;- c(&quot;05:00-06:00&quot;,&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;,&quot;08:00-09:00&quot;, &quot;09:00-10:00&quot;) dawnChorusAsu &lt;- siteWiseAsu %&gt;% filter(time_of_day %in% dawn) # Separate analysis: space use during certain times of day alone # let&#39;s look only at dusk chorus to explore what the data looks like dusk &lt;- c(&quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;,&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;) duskChorusAsu &lt;- siteWiseAsu %&gt;% filter(time_of_day %in% dusk) Preparing a dataframe of space use to run ordinations # Please note that while creating this dataframe for ordinations, the overall acoustic space use (sum of f.cont.sum column) was calculated by frequency bin, irrespective of the time of day for a given site. In other words, for 24 hours of data, a single value of space use was obtained for a given frequency bin. The frequency bin was pivoted from long to wide and each column essentially corresponded to a frequency bin. Within that frequency bin, grouped for each site, an overall value of space use was computed. nmdsDat &lt;- siteWiseAsu %&gt;% # replace with dawnChorusAsu/duskChorusAsu dplyr::select(Site, freq, f.cont.sum, Restoration.type) %&gt;% group_by(Site, Restoration.type, freq) %&gt;% summarise(totSpaceuse = sum(f.cont.sum)) %&gt;% arrange(Restoration.type) %&gt;% pivot_wider (names_from = freq, values_from = totSpaceuse, values_fill = list(totSpaceuse=0)) # Convert to matrix form nmdsDatMatrix &lt;- as.matrix(nmdsDat[, 3:ncol(nmdsDat)]) Currently using a euclidean measure of estimating dissimilarity prior to running ordinations # Run a euclidean dissimilarity distance and use metaMDS function from vegan to run ordinations disEuclidean &lt;- vegdist(nmdsDatMatrix, method = &quot;euclidean&quot;) nmdsEuclidean &lt;- vegdist (nmdsDatMatrix, method = &quot;euclidean&quot;) %&gt;% metaMDS (nmdsEuclidean, k=6) # extract nmds scores nmdsScores &lt;- as_tibble(scores(nmdsEuclidean)) # Write the scores to a separate .csv write.csv(nmdsScores, &quot;data/nmds-acousticSpaceUse.csv&quot;, row.names = F) # With the above analysis, we note the stress is 0.008566905. However, if stress is high, we should reposition the points in 2 dimensions in the direction of decreasing stress, and repeat until stress is below some threshold.**A good rule of thumb: stress &lt; 0.05 provides an excellent representation in reduced dimensions, &lt; 0.1 is great, &lt; 0.2 is good/ok, and stress &lt; 0.3 provides a poor representation.** To reiterate: high stress is bad, low stress is good! Plotting the NMDS scores using ggplot2 # First let&#39;s add the treatment type back to the nmds scores nmdsScores$Restoration.type &lt;- nmdsDat$Restoration.type # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting nmdsScores$Restoration.type &lt;- factor(nmdsScores$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_nmds &lt;- ggplot(data=nmdsScores) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=5) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScores$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScores$NMDS1), y = max(nmdsScores$NMDS2), label = &quot;Stress = 0.01&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) ggsave(fig_nmds, filename = &quot;figs/fig_overallSpaceUse.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() NMDS ordinations of Acoustic Space Use Ordination analysis of average ASU data (stress = 0.1) revealed a distinct and large cluster of BM sites, while no clear clusters were observed for AR and NR sites. In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites and NR = Naturally regenerating forest sites. Testing multivariate homogeneity of group dispersions One measure of multivariate dispersion (variance) for a group of samples is to calculate the average distance of group members to the group centroid or spatial median (both referred to as centroid from now on unless stated otherwise) in multivariate space. To test if the dispersions (variances) of one or more groups are different, the distances of group members to the group centroid are subject to ANOVA. Betadisper tests whether two or more groups (for example, restored and unrestored sites) are homogeneously dispersed in relation to their species in studied samples. This test can be done to see if one group has more compositional variance than another. Moreover, homogeneity of dispersion among groups is very advisable to have if you want to test if two or more groups have different compositions, which is tested by adonis. nmdsVariance &lt;- betadisper(disEuclidean, group = nmdsDat$Restoration.type) nmdsVariance anova(nmdsVariance) permutest(nmdsVariance, pairwise = TRUE, permutations = 999) TukeyHSD(nmdsVariance) # For overall space use, there is a significant difference in within group variance between one group and another (For Benchmark-Active sites and Passive-Benchmark sites) # For dawn ASU, there is no significant difference in within group variance across the three treatment types # For dusk ASU, there is a significant difference in within group variance between benchmark and active sites and benchmark and passive sites. Visualizing the multivariate homogeneity of group dispersions The below lines of code have been adapted from: https://chrischizinski.github.io/rstats/adonis/ # extract the centroids and the site points in multivariate space. centroids &lt;-data.frame(grps=rownames(nmdsVariance$centroids), data.frame(nmdsVariance$centroids)) vectors &lt;- data.frame(group=nmdsVariance$group, data.frame(nmdsVariance$vectors)) # to create the lines from the centroids to each point we will put it in a format that ggplot can handle seg.data&lt;-cbind(vectors[,1:3],centroids[rep(1:nrow(centroids),as.data.frame(table(vectors$group))$Freq),2:3]) names(seg.data)&lt;-c(&quot;group&quot;,&quot;v.PCoA1&quot;,&quot;v.PCoA2&quot;,&quot;PCoA1&quot;,&quot;PCoA2&quot;) # create the convex hulls of the outermost points grp1.hull &lt;- seg.data[seg.data$group==&quot;Active&quot;,1:3][chull(seg.data[seg.data$group==&quot;Active&quot;,2:3]),] grp2.hull &lt;- seg.data[seg.data$group==&quot;Benchmark&quot;,1:3][chull(seg.data[seg.data$group==&quot;Benchmark&quot;,2:3]),] grp3.hull &lt;- seg.data[seg.data$group==&quot;Passive&quot;,1:3][chull(seg.data[seg.data$group==&quot;Passive&quot;,2:3]),] all.hull &lt;- rbind(grp1.hull,grp2.hull,grp3.hull) # plot the panel and convex hulls fig_hull &lt;- ggplot() + geom_polygon(data= all.hull,aes(x=v.PCoA1,y=v.PCoA2),colour=&quot;black&quot;,alpha=0,linetype=&quot;dashed&quot;) + geom_segment(data=seg.data,aes(x=v.PCoA1,xend=PCoA1,y=v.PCoA2,yend=PCoA2),alpha=0.30) + geom_point(data=centroids[,1:3], aes(x=PCoA1,y=PCoA2,shape=grps),size=4,colour=&quot;red&quot;) + geom_point(data=seg.data, aes(x=v.PCoA1,y=v.PCoA2,shape=group),size=2) + labs(title=&quot;All&quot;,x=&quot;&quot;,y=&quot;&quot;) + #coord_cartesian(xlim = c(-0.2,0.2), ylim = c(-0.25,0.2)) + theme_bw() + theme(legend.position=&quot;none&quot;) Testing compositional dissimilarity between groups We will do this by using the vegan::adonis() function which allows you to do permutational multivariate analysis of variance using distance matrices. In the above figure, the NMDS confidence ellipses suggest that there is a significant difference between benchmark and passive-active sites, but no difference between active and passive sites. Adonis works by first finding the centroids for each group and then calculates the squared deviations of each of site to that centroid. Then significance tests are performed using F-tests based on sequential sums of squares from permutations of the raw data. Please note that adonis analyzes and partitions sums of squares using distance matrices. It can be seen as an ANOVA using distance matrices (analogous to MANOVA - multivariate analysis of variance). Therefore, it is used to test if two or more groups have similar compositions. # We will use the NMDS scores for axis 1 and axis 2 to test for compositional dissimilarity groups &lt;- nmdsScores$Restoration.type adonisNMDS &lt;- adonis(nmdsDatMatrix ~ groups, method=&quot;bray&quot;,perm=999) adonisNMDS # For overall ASU, the results suggest that there are significant differences in ASU between groups. # For dawn ASU, there is a significant difference in ASU between treatment types # For dusk ASU, there is a significant difference in ASU between treatment types "],["generalized-linear-mixed-modeling-acoustic-space-use-and-vegetation-data.html", "Section 12 Generalized linear mixed modeling (acoustic space use and vegetation data)", " Section 12 Generalized linear mixed modeling (acoustic space use and vegetation data) In this script, we run generalized linear models to test the association between acoustic space use values and restoration type. In addition, we run generalized linear mixed models to test associations between acoustic space use and habitat (vegetation structure) using site-pair name (actively restored and naturally regenerating were specified to be paired) and repeat visits as random effects. Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(ggpubr) library(sjPlot) library(RColorBrewer) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Load necessary data for statistical modeling # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% dplyr::select(&quot;Site.code&quot;,&quot;Restoration.type&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # load the entire asu data across all sites and days computed sitebyDayAsu &lt;- read.csv(&quot;data/site-by-day-asu.csv&quot;) # separate by Site and Date sitebyDayAsu &lt;- separate(sitebyDayAsu, col = Site_Day, into = c(&quot;Site&quot;, &quot;Date&quot;), sep = &quot;_&quot;) # Add restoration type column to the space use data sitebyDayAsu &lt;- left_join(sitebyDayAsu, sites, by=c(&quot;Site&quot;=&quot;Site.code&quot;)) # scale values per site/date for comparison between sites and treatment types sitebyDayAsu &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% mutate(f.cont.scaled = range01(f.cont)) # computing total number of days for which space use has been computed for each site (some sites have very few days. Eg. OLV110R, else rest have 5 days of audio data each) nSites_Days &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, Date, Restoration.type) %&gt;% distinct() %&gt;% group_by(Site) %&gt;% count() # Let&#39;s look at data by restoration type # This suggests that we have more data for benchmark sites relative to the other two treatment types nDays_siteType &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, Date, Restoration.type) %&gt;% distinct() %&gt;% group_by(Restoration.type) %&gt;% count() # Separate analysis: space use during certain times of day alone # let&#39;s look only at dawn chorus to explore what the data looks like dawn &lt;- c(&quot;05:00-06:00&quot;,&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;,&quot;08:00-09:00&quot;, &quot;09:00-10:00&quot;) dawnChorusAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% dawn) # Separate analysis: space use during certain times of day alone # let&#39;s look only at dusk chorus to explore what the data looks like dusk &lt;- c(&quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;,&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;) duskChorusAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% dusk) # Prepare data for statistical modeling # Calculating total space use across all frequency bins and times of day: 128*24 for each site-day combination totSpaceUse &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # Calculating total space use for dawn chorus alone for each site-day combination totSpaceUseDawn &lt;- dawnChorusAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # Calculating total space use for dusk chorus alone for each site-date combination totSpaceUseDusk &lt;- duskChorusAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # Load data from previous scripts for use in a GLM vegData &lt;- read.csv(&quot;data/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;data/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) Getting data ready in a format for linear modeling # overall space use modelDataAll &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUse, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) # dawn chorus space use modelDataDawn &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUseDawn, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) # dusk chorus space use modelDataDusk &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUseDusk, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) Running the generalized linear mixed models We now run generalized linear mixed models (GLMM) assuming gaussian errors and using log link functions to examine the effects of restoration type (benchmark, actively restored and passively restored) on acoustic space use, followed by TukeyHSD multiple comparisons tests of means. # overall space use glmm_allRestoration &lt;- glmer(roundSpaceuse ~ Restoration.type + (1|siteCode) + (1|visit), data = modelDataAll, family = poisson(link=log)) summary(glmm_allRestoration) tukey_glmmAllRestoration &lt;- summary(glht(glmm_allRestoration, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmAllRestoration) # The above result suggests a significant difference in acoustic space use values across all three treatment types # dawn chorus glmm_dawnRestoration &lt;- glmer(roundSpaceuse ~ Restoration.type + (1|siteCode) + (1|visit), data = modelDataDawn, family = poisson(link=log)) summary(glmm_dawnRestoration) tukey_glmmdawnRestoration &lt;- summary(glht(glmm_dawnRestoration, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmdawnRestoration) # When we look at only the dawn chorus data, the above result suggests a significant difference in acoustic space use values for benchmark sites and passively restored sites and actively restored sites and passively restored sites (but no difference between active-benchmark) # dusk chorus glmm_duskRestoration &lt;- glmer(roundSpaceuse ~ Restoration.type + (1|siteCode) + (1|visit), data = modelDataDusk, family = poisson(link=log)) summary(glmm_duskRestoration) tukey_glmmduskRestoration &lt;- summary(glht(glmm_duskRestoration, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmduskRestoration) # For dusk chorus data, the above result suggests a significant difference in acoustic space use values for benchmark sites and passively restored sites and benchmark sites and actively restored sites (but no difference between active-passive) # Plotting the above results # reordering factors for plotting totSpaceUse$Restoration.type &lt;- factor(totSpaceUse$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_glmm_allRest &lt;- ggplot(totSpaceUse, aes(Restoration.type, totSpaceuse, group = Restoration.type, fill = Restoration.type)) + geom_boxplot(alpha = 0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Acoustic Space Use\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_glmm_allRest, filename = &quot;figs/fig_glmm_overallAcousticSpace.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() # reordering factors for plotting totSpaceUseDawn$Restoration.type &lt;- factor(totSpaceUseDawn$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_glmm_dawnRest &lt;- ggplot(totSpaceUseDawn, aes(Restoration.type, totSpaceuse, group = Restoration.type, fill = Restoration.type)) + geom_boxplot(alpha = 0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Acoustic Space Use\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_glmm_dawnRest, filename = &quot;figs/fig_glmm_dawnAcousticSpaceUse.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() # reordering factors for plotting totSpaceUseDusk$Restoration.type &lt;- factor(totSpaceUseDusk$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_glmm_duskRest &lt;- ggplot(totSpaceUseDusk, aes(Restoration.type, totSpaceuse, group = Restoration.type, fill = Restoration.type)) + geom_boxplot(alpha = 0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Acoustic Space Use\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_glmm_duskRest, filename = &quot;figs/fig_glmm_duskAcousticSpaceUse.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() Overall acoustic space use across treatment types. Analysis of ~4,128 hours of acoustic data across treatment types (for the 43 sites) revealed significant differences in ASU between treatment types (Tukey HSD P &lt; 0.05). ASU was significantly different across BM-AR, BM-NR and AR-NR sites, with the highest estimate in BM sites (mean ± SD: 413 ± 103), followed by AR sites (mean ± SD: 188 ± 78) and NR sites (mean ± SD: 182 ± 66). Mean and standard deviation were based on pooled values of ASU across 24 hours for every site-day combination. In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites and NR = Naturally regenerating forest sites. We now run generalized linear mixed models (GLMM) assuming gaussian errors and using log link functions to examine the effects of habitat (vegetation structure) on acoustic space use. # overall space use glmm_allVeg&lt;- glmer(roundSpaceuse ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelDataAll, family = poisson(link = log)) summary(glmm_allVeg) plot_model(glmm_allVeg, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_allVeg) # There is a significant negative effect of PC1 and PC2 on acoustic space use # dawn chorus glmm_dawnVeg&lt;- glmer(roundSpaceuse ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelDataDawn, family = poisson(link = log)) summary(glmm_dawnVeg) plot_model(glmm_dawnVeg, type=&quot;pred&quot;, terms=c(&quot;PC2&quot;,&quot;PC1&quot;)) report::report(glmm_dawnVeg) # There is a significant effect of PC1 and PC2 on dawn acoustic space use and a highly significant effect of PC2 on dawn acoustic space use # dawn chorus glmm_duskVeg&lt;- glmer(roundSpaceuse ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelDataDusk, family = poisson(link = log)) summary(glmm_duskVeg) plot_model(glmm_duskVeg, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_duskVeg) # There is a significant effect of PC1 on dusk acoustic space use "],["multiple-regression-on-distance-matrices-acoustic-space-use.html", "Section 13 Multiple regression on distance matrices (acoustic space use)", " Section 13 Multiple regression on distance matrices (acoustic space use) Load necessary libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(sf) library(sna) library(hier.part) library(ecodist) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Load necessary data and create dissimilarity matrices to run multiple regression on distance matrices # estimating geographic distance (euclidean) between sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% filter(!str_detect(Site.code, &#39;OLCAP5B&#39;)) sites &lt;- st_as_sf(sites, coords = c(&quot;Longitude&quot;,&quot;Latitude&quot;), crs=4326) sites &lt;- st_transform(sites, 32643) distanceData &lt;- dist(st_coordinates(sites), method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) distMatrix &lt;- as.matrix(distanceData) # getting vegetation PCA scores as a matrix vegPcaScores &lt;- read.csv(&quot;data/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- dist(vegPcaScores, method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) vegPcaScores &lt;- as.matrix(vegPcaScores) # load NMDS scores (run on bird detections) nmdsScoresBird &lt;- read.csv(&quot;data/nmdsBrayCurtis-bird-detections.csv&quot;) birdMatrix &lt;- dist(nmdsScoresBird[,1:2], method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) birdMatrix &lt;- as.matrix(birdMatrix) # load NMDS scores (run on acoustic space use) nmdsScoresAll &lt;- read.csv(&quot;data/nmds-acousticSpaceUse.csv&quot;) nmdsScoresDawn &lt;- read.csv(&quot;data/nmds-acousticSpaceUse-dawn.csv&quot;) nmdsScoresDusk &lt;- read.csv(&quot;data/nmds-acousticSpaceUse-dawn.csv&quot;) overallspaceUseMatrix &lt;- dist(nmdsScoresAll[,1:2], method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) overallspaceUseMatrix &lt;- as.matrix(overallspaceUseMatrix) dawnspaceUseMatrix &lt;- dist(nmdsScoresDawn[,1:2], method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) dawnspaceUseMatrix &lt;- as.matrix(dawnspaceUseMatrix) duskspaceUseMatrix &lt;- dist(nmdsScoresDusk[,1:2], method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) duskspaceUseMatrix &lt;- as.matrix(duskspaceUseMatrix) Extracting floristic data and running NMDS ordinations on the same # getting floristic data as a matrix, using NMDS scores # to do the above, we need to reload the vegetation data and process it veg &lt;- read.csv(&quot;data/2020-vegetation-data.csv&quot;) veg$Site_ID &lt;- str_remove(veg$Site_ID,&quot;_&quot;) # We load the subset data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) %&gt;% filter(!str_detect(Site, &#39;OLCAP5B&#39;)) sites_needed &lt;- data.frame(unique(datSubset$Site)) names(sites_needed) &lt;- &quot;Site_ID&quot; # Obtain a subset of the data which have the sites visited veg &lt;- merge(veg, sites_needed, by.x = &quot;Site_ID&quot;, by.y=&quot;Site_ID&quot;) # renaming restoration type veg$Site_type[veg$Site_type==&quot;Unrestored&quot;] &lt;- &quot;Passive&quot; veg$Site_type[veg$Site_type==&quot;Restored&quot;] &lt;- &quot;Active&quot; # now extract floristics data (tree species abundance) floraData &lt;- veg %&gt;% dplyr::select(Site_ID, tree_species, Site_type) %&gt;% mutate(Number = 1) %&gt;% group_by(tree_species, Site_ID, Site_type) %&gt;% summarise (totalAbun = sum(Number)) %&gt;% pivot_wider (names_from = tree_species, values_from = totalAbun, values_fill = list(totalAbun=0)) %&gt;% arrange(.,Site_ID) # Convert to matrix form nmdsDatMatrix &lt;- as.matrix(floraData[, 3:ncol(floraData)]) # Run a bray-curtis dissimilarity index and use metaMDS function from vegan to run ordinations disBrayCurtis &lt;- vegdist(nmdsDatMatrix, method = &quot;bray&quot;) nmdsBrayCurtis &lt;- vegdist (nmdsDatMatrix, method = &quot;bray&quot;) %&gt;% metaMDS (nmdsBrayCurtis, k=6) # extract nmds scores nmdsScores &lt;- as.tibble(scores(nmdsBrayCurtis)) # stress is lowest at around 6 dimensions with value of 0.06350522. However, if stress is high, we should reposition the points in 2 dimensions in the direction of decreasing stress, and repeat until stress is below some threshold.**A good rule of thumb: stress &lt; 0.05 provides an excellent representation in reduced dimensions, &lt; 0.1 is great, &lt; 0.2 is good/ok, and stress &lt; 0.3 provides a poor representation.** To reiterate: high stress is bad, low stress is good! # create a distance matrix on the nmds scores of the flora data floraMatrix &lt;- dist(nmdsScores[,1:2], method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE, p = 2) floraMatrix &lt;- as.matrix(floraMatrix) Running multiple regression on distance matrices # First we get rid of redundant duplicate values, then make the diagonal zeros NA, unfold each matrix into a vector, and then omit rows with 0 in them overallspaceUseVector &lt;- upper.tri.remove(overallspaceUseMatrix) diag(overallspaceUseVector) &lt;- NA overallspaceUseVector &lt;- cbind(c(overallspaceUseVector)) %&gt;% na.omit() dawnspaceUseVector &lt;- upper.tri.remove(dawnspaceUseMatrix) diag(dawnspaceUseVector) &lt;- NA dawnspaceUseVector &lt;- cbind(c(dawnspaceUseVector)) %&gt;% na.omit() duskspaceUseVector &lt;- upper.tri.remove(duskspaceUseMatrix) diag(duskspaceUseVector) &lt;- NA duskspaceUseVector &lt;- cbind(c(duskspaceUseVector)) %&gt;% na.omit() floraVector &lt;- upper.tri.remove(floraMatrix) diag(floraVector) &lt;- NA floraVector &lt;- cbind(c(floraVector)) %&gt;% na.omit() pcaVector &lt;- upper.tri.remove(vegPcaScores) diag(pcaVector) &lt;- NA pcaVector &lt;- cbind(c(pcaVector)) %&gt;% na.omit() distanceVector &lt;- upper.tri.remove(distMatrix) diag(distanceVector) &lt;- NA distanceVector &lt;- cbind(c(distanceVector)) %&gt;% na.omit() %&gt;% scale #arranging data for MRM mrmAllData &lt;- bind_cols (overallspaceUseVector, floraVector, pcaVector, distanceVector) %&gt;% rename (spaceUse = ...1, flora = ...2, structure = ...3, distance = ...4) mrmDawnData &lt;- bind_cols (dawnspaceUseVector, floraVector, pcaVector, distanceVector) %&gt;% rename (spaceUse = ...1, flora = ...2, structure = ...3, distance = ...4) mrmDuskData &lt;- bind_cols (duskspaceUseVector, floraVector, pcaVector, distanceVector) %&gt;% rename (spaceUse = ...1, flora = ...2, structure = ...3, distance = ...4) # run the analysis MRM(dist(spaceUse) ~ dist(flora) + dist(structure) + dist(distance), data = mrmAllData, method = &quot;linear&quot;, nperm = 1000) MRM(dist(spaceUse) ~ dist(flora) + dist(structure) + dist(distance), data = mrmDawnData, method = &quot;linear&quot;, nperm = 1000) MRM(dist(spaceUse) ~ dist(flora) + dist(structure) + dist(distance), data = mrmDuskData, method = &quot;linear&quot;, nperm = 1000) # Overall space use is not signficantly related to dissimilarity in habitat structure, geographic distance and floristics # dawn space use is positively and significantly related to dissimilarity in habitat structure (Please note that the overall R2 values are very low) # dusk space use is also positively and significantly related to dissimilarity in habitat structure(Please note that the overall R2 values are very low) Hierarchical partitioning hier.part(mrmAllData$spaceUse, mrmAllData[2:4], family = &quot;gaussian&quot;, gof = &quot;Rsqu&quot;, barplot = TRUE) hier.part(mrmDawnData$spaceUse, mrmDawnData[2:4], family = &quot;gaussian&quot;, gof = &quot;Rsqu&quot;, barplot = TRUE) hier.part(mrmDuskData$spaceUse, mrmDuskData[2:4], family = &quot;gaussian&quot;, gof = &quot;Rsqu&quot;, barplot = TRUE) "],["generalized-linear-modeling-acoustic-space-use-and-richness-and-time-since-restoration.html", "Section 14 Generalized linear modeling (acoustic space use and richness, and time since restoration)", " Section 14 Generalized linear modeling (acoustic space use and richness, and time since restoration) In this script, we run generalized linear models to test the association between acoustic space use values and species richness, vocal activity (total calling rate at each site, which is a proxy for the total number of vocal detections/unit of recording time) and time since restoration. Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(ggpubr) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Load necessary data for statistical modeling # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% dplyr::select(&quot;Site.code&quot;,&quot;Restoration.type&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # loading jacknife scores jackAll &lt;- read.csv(&quot;data/jackAll.csv&quot;) jack_rainForest &lt;- read.csv(&quot;data/jackRainforest.csv&quot;) jack_openCountry &lt;- read.csv(&quot;data/jackOpencountry.csv&quot;) # Load vegetation data from previous scripts vegData &lt;- read.csv(&quot;data/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;data/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) # Attach the annotation data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Calculate the overall number of detections for each site where each temporal duration chosen is a 10s clip nDetections &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # total number of detections at each site totDetections &lt;- nDetections %&gt;% rowwise() %&gt;% summarise(totDetections = sum(c_across(IP:HSWP))) # Load a dataframe of average space use by site siteWiseAsu &lt;- read.csv(&quot;data/site-wise-asu.csv&quot;) # add restoration type siteWiseAsu &lt;- left_join(siteWiseAsu, sites, by=c(&quot;Site&quot;=&quot;Site.code&quot;)) # Separate analysis: space use during certain times of day alone # let&#39;s look only at dawn chorus to explore what the data looks like dawn &lt;- c(&quot;05:00-06:00&quot;,&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;,&quot;08:00-09:00&quot;, &quot;09:00-10:00&quot;) dawnChorusAvgAsu &lt;- siteWiseAsu %&gt;% filter(time_of_day %in% dawn) # Separate analysis: space use during certain times of day alone # let&#39;s look only at dusk chorus to explore what the data looks like dusk &lt;- c(&quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;,&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;) duskChorusAvgAsu &lt;- siteWiseAsu %&gt;% filter(time_of_day %in% dusk) # Calculating total space use across all frequency bins and times of day: 128*24 for each site-day combination avgSpaceUse &lt;- siteWiseAsu %&gt;% group_by(Site, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.sum)) %&gt;% full_join(jackAll, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate (&quot;roundjk&quot; = round(jack1)) %&gt;% mutate(&quot;roundSpaceUse&quot; = round(totSpaceuse)) %&gt;% add_column(year = vegData$plantingYear) %&gt;% add_column(totDetections = totDetections$totDetections) # dawn dawnAvgSpaceUse &lt;- dawnChorusAvgAsu %&gt;% group_by(Site, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.sum)) %&gt;% full_join(jackAll, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate (&quot;roundjk&quot; = round(jack1)) %&gt;% mutate(&quot;roundSpaceUse&quot; = round(totSpaceuse)) %&gt;% add_column(year = vegData$plantingYear) %&gt;% add_column(totDetections = totDetections$totDetections) #dusk duskAvgSpaceUse &lt;- duskChorusAvgAsu %&gt;% group_by(Site, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.sum)) %&gt;% full_join(jackAll, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate (&quot;roundjk&quot; = round(jack1)) %&gt;% mutate(&quot;roundSpaceUse&quot; = round(totSpaceuse)) %&gt;% add_column(year = vegData$plantingYear) %&gt;% add_column(totDetections = totDetections$totDetections) Lets first explore richness (as measured through first order jacknife scores and test for associations with space use) # running generalized linear models glmRichnessSpace &lt;- glm(roundSpaceUse ~ roundjk, data = avgSpaceUse, family = poisson(link = log)) summary(glmRichnessSpace) plot_model(glmRichnessSpace, type=&quot;pred&quot;) report::report(glmRichnessSpace) # The results suggest a significant and negative association between overall space use and first-order jacknife scores # creating separate dataframes to test for associations with rainforest species and open country species avgSpaceUseRain &lt;- siteWiseAsu %&gt;% group_by(Site, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.sum)) %&gt;% full_join(jack_rainForest, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate (&quot;roundjk&quot; = round(jack1)) %&gt;% mutate(&quot;roundSpaceUse&quot; = round(totSpaceuse)) %&gt;% add_column(year = vegData$plantingYear) %&gt;% add_column(totDetections = totDetections$totDetections) avgSpaceUseOpen &lt;- siteWiseAsu %&gt;% group_by(Site, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.sum)) %&gt;% full_join(jack_openCountry, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate (&quot;roundjk&quot; = round(jack1)) %&gt;% mutate(&quot;roundSpaceUse&quot; = round(totSpaceuse)) %&gt;% add_column(year = vegData$plantingYear) %&gt;% add_column(totDetections = totDetections$totDetections) # Rainforest species glmRichnessRain &lt;- glm(roundSpaceUse ~ roundjk, data = avgSpaceUseRain, family = poisson(link = log)) summary(glmRichnessRain) plot_model(glmRichnessRain, type=&quot;pred&quot;) report::report(glmRichnessRain) # for rainforest species, The effect of roundjk is statistically significant and positive (beta = 0.01, 95% CI [0.01, 0.02], p &lt; .001; Std. beta = 0.09, 95% CI [0.08, 0.11]) # open country species glmRichnessOpen &lt;- glm(roundSpaceUse ~ roundjk, data = avgSpaceUseOpen, family = poisson(link = log)) summary(glmRichnessOpen) plot_model(glmRichnessOpen, type=&quot;pred&quot;) report::report(glmRichnessOpen) # For open country birds, The effect of roundjk is statistically significant and negative (beta = -0.04, 95% CI [-0.04, -0.04], p &lt; .001; Std. beta = -0.24, 95% CI [-0.26, -0.22]) Lets look at year since restoration and ask if space use has a significant association with time allRestored &lt;- avgSpaceUse %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-year)) dawnRestored &lt;- dawnAvgSpaceUse %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-year)) duskRestored &lt;- duskAvgSpaceUse %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-year)) glmAll &lt;- glm(roundSpaceUse ~ yearSinceRestoration, data = allRestored, family = poisson(link = log)) summary(glmAll) plot_model(glmAll, type=&quot;pred&quot;) report::report(glmAll) glmDawn &lt;- glm(roundSpaceUse ~ yearSinceRestoration, data = dawnRestored, family = poisson(link = log)) summary(glmDawn) plot_model(glmDawn, type=&quot;pred&quot;) report::report(glmDawn) glmDusk &lt;- glm(roundSpaceUse ~ yearSinceRestoration, data = duskRestored, family = poisson(link = log)) summary(glmDusk) plot_model(glmDusk, type=&quot;pred&quot;) report::report(glmDusk) # all three analysis report a statistically significant and negative association with year since restoration and spaceuse values Lets look at associations between acoustic space use and sum total of detections of species at each site glmDetections&lt;- glm(roundSpaceUse ~ totDetections, data = avgSpaceUse, family = poisson(link = log)) summary(glmDetections) plot_model(glmDetections, type=&quot;pred&quot;) report::report(glmDetections) # The effect of totDetections is statistically significant and positive (beta = 7.93e-05, 95% CI [5.19e-05, 1.07e-04], p &lt; .001; Std. beta = 0.05, 95% CI [0.03, 0.07]) # # What if we look at it by habitat affiliation # rainforest species and open country species glmRainDet &lt;- glm(roundSpaceUse ~ totDetections, data = avgSpaceUseRain, family = poisson(link = log)) summary(glmRainDet) plot_model(glmRainDet, type=&quot;pred&quot;) report::report(glmRainDet) # For rainforest birds, The effect of totDetections is statistically significant and positive (beta = 7.93e-05, 95% CI [5.19e-05, 1.07e-04], p &lt; .001; Std. beta = 0.05, 95% CI [0.03, 0.07]) glmOpenDet &lt;- glm(roundSpaceUse ~ totDetections, data = avgSpaceUseOpen, family = poisson(link = log)) summary(glmOpenDet) plot_model(glmOpenDet, type=&quot;pred&quot;) report::report(glmOpenDet) # For open country birds, The effect of totDetections is statistically significant and positive (beta = 7.93e-05, 95% CI [5.19e-05, 1.07e-04], p &lt; .001; Std. beta = 0.05, 95% CI [0.03, 0.07]) "]]
