[["index.html", "Source code and supporting information for Using passive acoustic monitoring to examine the impacts of ecological restoration on faunal biodiversity in the Western Ghats Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing 1.4 Main Text Figure 1 1.5 Main Text Figure 2", " Source code and supporting information for Using passive acoustic monitoring to examine the impacts of ecological restoration on faunal biodiversity in the Western Ghats Vijay Ramesh Priyanka Hariharan VA Akshay Pooja Choksi Sarika Khanwilkar VV Robin Ruth DeFries 2022-08-19 Section 1 Introduction This is the readable version that showcases analyses carried out to test the impacts of rainforest restoration on vocalizing biodiversity in the Anamalai hills of the Western Ghats biodiversity hotspot. 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University 1.2 Data access The data used in this work is archived on Zenodo. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. 1.4 Main Text Figure 1 Sites corresponding to acoustic recorder deployment locations across the Valparai plateau of the Anamalai hills. The above figure showcases a gradient of forest regeneration across the Valparai plateau. Sites shown in green represent undisturbed benchmark rainforest sites, sites shown in orange represent actively restored forest sites and sites shown in purple represent naturally regenerating forest sites. Ecological restoration is currently being carried out in cooperation with three plantation companies in the Valparai plateau. For more information with respect to the weeding and active restoration protocol, please see methods in Hariharan and Raman (2021) and Osuri et al. (2019). Over the last two decades, the ecological restoration efforts have resulted in restoration of over 100 ha of degraded forests. This map was prepared using 30m resolution SRTM data (Farr et al. 2007) and ESRI satellite imagery is used as a basemap. 1.5 Main Text Figure 2 Ecological restoration across the Valparai plateau of the Anamalai hills. (a) Images of an actively restored forest fragment (Selaliparai) were taken in 2007 and in 2021. Habitat and vegetation structure has responded positively to active restoration (Osuri et al. 2019) (Photo credit: T R Shankar Raman). (b) AudioMoth audio recorders were deployed across naturally regenerating, actively restored, and benchmark (not shown in picture) sites across the Valparai plateau (Photo credit: Vijay Ramesh). "],["site-selection.html", "Section 2 Site selection 2.1 Install required libraries 2.2 Load list of sites 2.3 Extract elevation 2.4 Distance to nearest road 2.5 Distance between AR-NR site pairs", " Section 2 Site selection In this script, we plot the elevation, distance to the nearest road, and distance between sites to show that the sites selected are comparable (we also followed site selection criteria outlined by Osuri et al., (2019) and Hariharan and Raman (2021) to ensure that sites are similar in terms of physiognomy and climate). 2.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(data.table) library(extrafont) library(sf) library(raster) # for plotting library(scales) library(ggplot2) library(ggspatial) library(colorspace) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 2.2 Load list of sites # read from local file sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # removed this site as it was sampled in only one season and not across multiple seasons # load a shapefile of the Western Ghats (here, I will use the boundary of the Nilgiris, Anamalais and Palanis) # this file can be used for plotting (if necessary) # hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) # hills &lt;- st_transform(hills, 32643) 2.3 Extract elevation Extract elevation at each site and examine differences in elevation across treatment types # convert to sf object and transform sites &lt;- st_as_sf(sites, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) # add elevation raster alt &lt;- raster(&quot;data/spatial/elevation/alt&quot;) # this layer is not added to github as a result of its large size and can be downloaded from SRTM (Farr et al. (2007)) # extract values from that raster (note: transformation of coordinate system) elev &lt;- extract(alt, sites) sites &lt;- cbind(sites, elev) # Test if there are significant differences in elevation across treatment types anovaElevAll &lt;- aov(elev~Restoration.type, data = sites) # Tukey test to study each pair of treatment - reveals no significant difference across treatment types tukeyElevAll &lt;- TukeyHSD(anovaElevAll) # Create a boxplot of elevation estimates by group (Here: group refers to Restoration Type) # reordering factors for plotting sites$Restoration.type &lt;- factor(sites$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) fig_elevAll &lt;- ggplot(sites, aes(x=Restoration.type, y=elev, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Elevation (in meters)\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) # plot and save elevation ggsave(fig_elevAll, filename = &quot;figs/fig_elev.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() We did not observe any significant difference in elevation across treatment types (Tukey HSD P &gt; 0.05) 2.4 Distance to nearest road Calculate distance to nearest road for each site and examine differences across treatment types. # add roads data # Note: This shapefile was downloaded from Open Street Map at http://download.geofabrik.de/asia/india/southern-zone.html # This file is not uploaded to GitHub as it is extremely large and can be obtained from the link above roads &lt;- st_read(&quot;data/spatial/roadsShapefiles/osm_roads_southIndia.shp&quot;) %&gt;% st_transform(32643) # get index of the nearest road to a particular site index &lt;- st_nearest_feature(sites, roads) # calculate distance between each site and the nearest road distToRoad &lt;- as.numeric(st_distance(sites, roads[index,], by_element=TRUE)) sites &lt;- cbind(sites, distToRoad) # Test if there are significant differences in distance to roads across treatment types anovaDistAll &lt;- aov(distToRoad~Restoration.type, data = sites) # Tukey test to study each pair of treatment - no significant difference between AR-NR sites but a significant difference in distance to road between AR-BM and NR-BM sites tukeyDistAll &lt;- TukeyHSD(anovaDistAll) # Create a boxplot of distance to road estimates by group (Here: group refers to Restoration Type) # reordering factors for plotting sites$Restoration.type &lt;- factor(sites$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_distRoad &lt;- ggplot(sites, aes(x=Restoration.type, y=distToRoad, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Distance to nearest road (in meters)\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) # plot and save above figure ggsave(fig_distRoad, filename = &quot;figs/fig_distRoad.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() We did not observe any significant difference in distance to nearest road between AR and NR sites. However, the distance to nearest road for BM sites were significantly different from both AR and NR sites (Tukey HSD P &lt;0.05) 2.5 Distance between AR-NR site pairs Calculate distances between actively restored and naturally regenerating sites that are paired. # Please note that there are only 12 site pairs as two AR sites and one NR site did not have a site-pair because they were too close to another AR/NR site in terms of their acoustic radius. sitePairs &lt;- sites %&gt;% rename(Site_ID = Site.code) %&gt;% mutate(Site.code = str_extract(Site_ID, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% filter(!Restoration.type == &quot;Benchmark&quot;) %&gt;% filter(!Site.code == &quot;THEN10&quot;) %&gt;% filter(!Site.code ==&quot;VAR210&quot;) %&gt;% filter(!Site.code == &quot;INOA03&quot;) AR &lt;- sitePairs[sitePairs$Restoration.type==&quot;Active&quot;,] NR &lt;- sitePairs[sitePairs$Restoration.type==&quot;Passive&quot;,] sitePairDist &lt;- st_distance(AR$geometry, NR$geometry, by_element = T) # The 12 site-pairs were at a minimum distance of 162 m to a maximum distance of 1.1 km. "],["processing-vegetation-data.html", "Section 3 Processing vegetation data 3.1 Install required libraries 3.2 Load the vegetation data 3.3 Process habitat structure variables 3.4 Principal component analysis of vegetation data", " Section 3 Processing vegetation data In this script, we process vegetation data and examine differences in vegetation structure and composition across treatment types. Note: this analysis is provided as supporting information to showcase differences between AR, NR, and BM sites. In addition, we save results of principal component analyses for future statistical models. 3.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 3.2 Load the vegetation data # load the .csv containing different vegetation variables (data collected by the Nature Conservation Foundation) veg &lt;- read.csv(&quot;data/2020-vegetation-data.csv&quot;) veg$Site_ID &lt;- str_remove(veg$Site_ID,&quot;_&quot;) # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # join dataframes to obtain vegetation data for only the required list of sites veg &lt;- right_join(veg,sites, by=c(&quot;Site_ID&quot;=&quot;Site.code&quot;)) # renaming restoration type column veg$Site_type[veg$Site_type==&quot;Unrestored&quot;] &lt;- &quot;Passive&quot; veg$Site_type[veg$Site_type==&quot;Restored&quot;] &lt;- &quot;Active&quot; 3.3 Process habitat structure variables Carrying out exploratory analysis and preparing a dataframe for further steps # Counting number of tree species and unique species per plot treerich &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise (count = n(), richness = n_distinct(tree_species)) # Calculate average tree height across each unique site treeheight &lt;- veg %&gt;% drop_na(height) %&gt;% group_by(Site_ID) %&gt;% summarise(height = mean(height)) # Calculate basal area and left join with other data basal_area &lt;- veg %&gt;% mutate(basal_sum = rowSums(veg[,c(5:15)]^2)/(4*pi)) %&gt;% group_by(Site_ID, Site_type) %&gt;% summarise(basal_area = sum(basal_sum)) # Calculate average canopy height canopy_height &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(canopy_cover = mean(Canopy_cover)) # Calculate average leaf litter leaf_litter &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(leaf_litter = mean(Leaf_litter)) # Calculate average vertical stratification vert_strat &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(vert_strat = mean(Foliage_score)) # Year of planting plantingYear &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(plantingYear = unique(Year.of.planting)) # Creating a final dataframe for further analysis allVeg &lt;- basal_area %&gt;% left_join(treeheight) %&gt;% left_join(treerich) %&gt;% left_join(canopy_height) %&gt;% left_join(leaf_litter) %&gt;% left_join(vert_strat) %&gt;% left_join(plantingYear) write.csv(allVeg, &quot;data/summaryVeg.csv&quot;, row.names = F) 3.4 Principal component analysis of vegetation data # Check for correlations among vegetation predictors pairs.panels(allVeg[,3:9]) # The above panel suggests that richness is highly correlated with a number of predictors including canopy cover, count and basal area. We will calculate a PCA and keep the top two explanatory axes vegPca &lt;- prcomp(allVeg[, 3:9], scale=TRUE, center = TRUE, retx=TRUE) summary(vegPca) # The proportion of variance explained by the first two axes account for ~73.42% # Extract PCA values PCAvalues &lt;- data.frame(&#39;Site_ID&#39;=allVeg$Site_ID, &#39;Site_type&#39; = allVeg$Site_type, vegPca$x[,1:2]) # the first two components are selected # save the data for use in a GLM later write.csv(PCAvalues,&quot;data/pcaVeg.csv&quot;, row.names = F) # Extract loadings of the variables PCAloadings &lt;- data.frame(variables = rownames(vegPca$rotation), vegPca$rotation) # figure below # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting PCAvalues$Site_type &lt;- factor(PCAvalues$Site_type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_pca &lt;- ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Site_type)) + geom_segment(data = PCAloadings, aes(x = 0, y = 0, xend = (PC1*5), yend = (PC2*5)), arrow = arrow(length = unit(1/2, &quot;picas&quot;)), color = &quot;black&quot;) + geom_point(aes(x=PC1, y=PC2, shape= Site_type, colour = Site_type),size=5) + annotate(&quot;text&quot;, x = (PCAloadings$PC1*3), y = (PCAloadings$PC2*3), label = PCAloadings$variables, family = &quot;Century Gothic&quot;, size=5) + theme_bw() + scale_x_continuous(name=&quot;PC1 (54.67%)&quot;) + scale_y_continuous(name=&quot;PC2 (18.74%)&quot;) + scale_color_manual(&quot;Treatment type&quot;,values = mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + scale_shape_manual(&quot;Treatment type&quot;, values= 1:length(unique(PCAvalues$Site_type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) ggsave(fig_pca, filename = &quot;figs/fig_pca.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() Principal component analysis of vegetation data shows that AR sites were situated between BM and NR sites. Please see Hariharan and Raman 2021 and Osuri et al. 2019 for more analysis using vegetation data. "],["acoustic-space-use.html", "Section 4 Acoustic space use 4.1 Install necessary libraries 4.2 Load recordings across multiple days and sites 4.3 Short-time discrete fourier transform (STDFT) 4.4 Average acoustic space use 4.5 Visual examination of ASU 4.6 Representative figure for publication", " Section 4 Acoustic space use In this study, acoustic space use reflects the amount and pattern of vocalizations within each frequency bin for a given time period. Previous papers calculated acoustic space use by ascertaining the proportion of recordings in a particular time and frequency bin that is above a certain amplitude. However, this proportion was estimated by counting the number of frequency peaks within a given recording (see seewave::fpeaks()) and then scaling it to go from 0 to 1. In our analysis, we want to obtain an understanding of the overall acoustic activity for a given time and frequency bin. In other words, we want the area under the curve for a particular frequency contour. Note From Sound Analysis and Synthesis with R by Jerome Sueur “The basic premise in calculating ASU is that we compute a Short-time discrete fourier transform for a given frequency bin size (obtained by dividing the sampling frequency/window length over which the fourier transform is computed, for example 48000Hz/256 = 187.5 is the bin size). However, the size of the frequency bin is inversely proportional to time (Uncertainty principle; Pg.312) which would mean a bin size of 187.5 corresponds to a time duration of 0.005s (1/187.5=0.005s). Ultimately what matters is a compromise between frequency resolution and temporal resolution (Fig 11.3; Pg. 313).” 4.1 Install necessary libraries library(seewave) library(warbleR) library(tuneR) library(stringi) library(tidyverse) library(ggplot2) library(RColorBrewer) library(foreach) library(doSNOW) library(rlist) library(tictoc) library(patchwork) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Below, we have summarized approaches taken by previous studies: Aide et al. 2017 aggregated recordings at time scale of hour of day and used a frequency bin size of 86.13 Hz and an amplitude filtering threshold of 0.02. So if the sampling rate is 22000 Hz, that would mean - 22000/86.13 ~ 256 frequency bins to divide up the frequency space. In this paper, there would be 24hr*256 bins = 6144 time/frequency bins. Campos-Cerqueira et al. 2019 aggregated recordings at the time scale of hour of day (24 h), used a frequency bin size of 172 Hz, and an amplitude filtering threshold of 0.003. So if the sampling rate is 22000 Hz, that would mean - 22000/172 ~ 128 frequency bins. This resulted in a three‐dimensional (x = hour, y = acoustic frequency, z = proportion of all recordings in each time/frequency bin with a frequency peak value &gt; 0.003 amplitude) matrix of acoustic activity with a total of 3,072 time/frequency bins (24 h × 128 frequency bins). Campos-Cerqueira and Aide 2017 used the meanspec (f = 44,100, wl = 256, wn = “hanning”) and fpeaks (threshold = 0.1, freq = 172) function from the seewave package in R (Sueur et al., 2008a). The value of each peak was normalized using the maximum amplitude value within all recordings in the soundscape (i.e., site), and thus values ranged from 0 to 1. The number of frequency peaks was determined by counting the number of recordings with a peak within each of the 128 frequency bins that were equal or greater than the amplitude threshold. To control for the different number of recordings in each site and each time interval (i.e., hour), we divided the number of recordings with a peak in each time/frequency class by the total number of recordings collected during each hourly interval. 4.2 Load recordings across multiple days and sites First, we examined all unique sites and dates for which we had a continuous 24 hours of recordings. # List the path that contains all folders, which contain the audiomoth data path &lt;- &quot;D:\\\\2020-summer\\\\&quot; # change the path depending on where the raw data is # Listing the folders within which .WAV files are stored folders &lt;- dir(path, recursive=F,full.names=T) ###### Please note ###### # The below set of lines need to be run only once for a given set of sites and days. # Let&#39;s first rename the files by name of each site (as prefix) for(i in 1:length(folders)){ setwd(folders[i]) # List the files within each folder and renaming the files with the prefix - SITE_ID a &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;), full.names = T) file.rename(from = a, to=paste0(basename(folders)[i],&quot;_&quot;,basename(a))) } ###### Ending note here ###### # Now get only those files for a full 24 hours across every unique site files &lt;- list() for(i in 1:length(folders)){ a &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;), full.names = T) site_date &lt;- str_extract(basename(a),&#39;\\\\w+_\\\\d+_&#39;) # Choosing all 24 hours of data across every unique site (288 corresponds to 12 files every 1 hour) for(j in 1:length(unique(site_date))){ dat &lt;- a[str_detect(a,unique(site_date)[j])] if((length(dat)&lt;288)==TRUE){ next } else { files &lt;- c(files, dat) } } } files &lt;- unlist(files) 4.3 Short-time discrete fourier transform (STDFT) To compute an STDFT, we shall begin with an audio recording that has a sampling rate of s = 48,000 Hz and a window length (for the STDFT) wl = 256 samples, which results in a frequency bin size of z = 187.5 Hz (where z = s/wl). Given the sampling rate s, our Nyquist frequency f = 24,000 Hz (where f = s/2), and therefore, the total number of frequency bins across which an STDFT was run is n = 128 frequency bins (n = f/z). The final output of an STDFT corresponds to a matrix of N * m fourier coefficients (N = length of the audio recording and m = number of frequency bins). In this study, we computed STDFT across 24 hours of audio recordings, which translates to a matrix of 3072 (24 hours * 128 bins) fourier coefficients (Campos-Cerqueira et al. 2019). This matrix of coefficients essentially corresponds to ASU (or a measure of space ’used, for every hour of a day across the 128 bins in this study). Prior to computing STDFTs and ASU values, we set a threshold amplitude of 0.003 dB for audio recordings (following Campos-Cerqueira et al. 2019 and examination of our data). To calculate ASU, we selected five consecutive days of acoustic data (24 continuous hours of data) across each site. Five days was the minimum number of days of consecutive sampling without disruptions across sites (eg. memory card switching/battery replacements/failure of equipment etc.). # Select all unique site_date combinations for each unique site site_date &lt;- str_extract(basename(files),&#39;\\\\w+_\\\\d+_&#39;) unique(site_date) # The only step that involves sub-selection (Selecting 5 days across each site; Note: IPTO06R has 3 days sampled and OLV110R has only 2 days sampled) site_date &lt;- unique(site_date)[c(1:5,10:14,19:23,28:47,52:61,71:75, 86:90,97:101,107:111,118:122,130:134, 141:143,150:154,161:165,173:177,188:189, 193:197,202:206,211:215,219:223,225:229, 231:235,237:241,249:253,261:265,269:273, 281:285,289:293,300:304,309:313,319:323, 329:333,340:344,351:355,361:365,370:374, 378:382,387:391,396:400)] # Create a sequence of numbers to combine files by 1 hour hour_seq &lt;- seq(from=0,to=288, by=12) # To name files with a suffix for each hour time_of_day &lt;- c(&quot;00:00-01:00&quot;,&quot;01:00-02:00&quot;,&quot;02:00-03:00&quot;,&quot;03:00-04:00&quot;, &quot;04:00-05:00&quot;,&quot;05:00-06:00&quot;,&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;, &quot;08:00-09:00&quot;,&quot;09:00-10:00&quot;,&quot;10:00-11:00&quot;,&quot;11:00-12:00&quot;, &quot;12:00-13:00&quot;,&quot;13:00-14:00&quot;,&quot;14:00-15:00&quot;,&quot;15:00-16:00&quot;, &quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;,&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;, &quot;20:00-21:00&quot;,&quot;21:00-22:00&quot;,&quot;22:00-23:00&quot;,&quot;23:00-24:00&quot;) # Loading parameters necessary for the Short-term fourier transform to be performed on hourly aggregates of data for each site_date combination f &lt;- 48000 wl &lt;- 256 # This window length should be changed as a function of the frequency resolution (ie. bin size) and temporal resolution (ie. time) ovlp &lt;- 50 wn &lt;- &quot;hanning&quot; # Store the 24 hour acoustic space use data in a list and name it by a unique site and date site_date_asu &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar( min = 0, max = length(unique(site_date)), style = 3 ) # Select only 24 hours of data (00:00:00 to 23:55:00) for every unique site-date for(i in 1:length(unique(site_date))){ tic(&quot;Total time for a single site-day combination&quot;) # Store the acoustic space use data in a data.frame for plotting and analysis space_use &lt;- data.frame() # Extract the strings first by site dat &lt;- files[stringr::str_detect(files,unique(site_date)[i])] # Parallelize the runs cl &lt;- makeCluster(6, type = &quot;SOCK&quot;) registerDoSNOW(cl) # Store the each hour of data for an entire day as a list here (raw audio files read by tuneR::readWave()) tic(&quot;Reading in .WAV files&quot;) dailydata &lt;- foreach(k=1:length(dat), .combine = &#39;c&#39;, .inorder = T, .packages = &#39;tuneR&#39;) %dopar% { r &lt;- readWave(dat[k]) } toc() gc() stopCluster(cl) # renaming the files to ensure that data is read in hour by hour and stored in a separate object called hourlydata # Below we read the first 12 files in and then save it after computing the STDFT and then repeating it for the next 12 files and so on for(t in 1:(length(hour_seq)-1)) { # Every 12 files correspond to one hour tic(&quot;Computing Short-term fourier transforms for each hour&quot;) if (t == 1) { hourlydata &lt;- dailydata[hour_seq[t]:hour_seq[t+1]] } else { hourlydata &lt;- dailydata[(hour_seq[t]+1):hour_seq[t+1]] } gc() # Parallelize the runs cl &lt;- makeCluster(6, type = &quot;SOCK&quot;) registerDoSNOW(cl) # Store every hour&#39;s ASU data here data_per_hour &lt;- foreach(z = 1:length(hourlydata), .combine = &#39;rbind&#39;, .inorder=T, .packages = &#39;seewave&#39;) %dopar% { wave &lt;- hourlydata[[z]] n &lt;- length(wave) ## Short-term Fourier transform (using a seewave internal function) m &lt;- sspectro(wave, f = f, wl = wl, ovlp = ovlp, wn = wn) # Frequency selection and frequency axis # Here, want only a sequence of numbers that correspond to the length of rows # of the short-time fourier transform and we divide it by 1000 to get values # in kHz freq &lt;- seq(0, (f/2) - (f/wl), length.out = nrow(m))/1000 # Calculate acoustic space use per frequency bin f.cont &lt;- apply(m, MARGIN = 1, FUN = sum) # Store the space use data in a dataframe for plotting later a &lt;- data.frame(freq, f.cont) } rm(hourlydata); gc() data_per_hour &lt;- data_per_hour %&gt;% group_by(freq) %&gt;% summarise(f.cont=sum(f.cont)) data_per_hour$f.cont &lt;- (data_per_hour$f.cont)/12 data_per_hour$time_of_day &lt;- time_of_day[t] space_use &lt;- rbind(data_per_hour, space_use) stopCluster(cl); gc() toc() } space_use &lt;- as.data.frame(space_use) site_date_asu &lt;- c(site_date_asu,list(space_use)) names(site_date_asu)[i] &lt;- unique(site_date)[i] rm(space_use, dailydata, data_per_hour); gc() setTxtProgressBar(pb, i) toc() } close(pb) # store the list object for later list.save(site_date_asu, &quot;data/site_date_asu.rdata&quot;) # Save the list of dataframes for future analysis siteByDayAsu &lt;- bind_rows(site_date_asu, .id = &quot;Site_Day&quot;) # Write to .csv write.csv(siteByDayAsu, &quot;data/site-by-day-asu.csv&quot;, row.names = F) 4.4 Average acoustic space use Now that we have calculated acoustic space use values for a given frequency bin size and time resolution for every unique site-date combination, we would like to obtain a single set of space use values for every unique site. In other words, we would like to average space use values across all frequency bins for five days. # All unique sites site &lt;- str_extract(basename(files),&#39;^([[:alnum:]])+&#39;) unique(site) # Store the site-wise ASU values site_asu &lt;- list() # Loop through site-wise data and average data for each site for(i in 1:length(unique(site))) { # Extract data needed for every unique site dat &lt;- site_date_asu[stringr::str_detect(names(site_date_asu),unique(site)[i])] # Sum up values of acoustic space use across X no. of days for every unique site asu_sum &lt;- dat %&gt;% bind_rows(.id=&quot;data&quot;) %&gt;% group_by(freq, time_of_day) %&gt;% summarize(f.cont.sum=sum(f.cont)) # Averaging data as a function of number of days for a given site over which data was summed asu_sum$f.cont.sum &lt;- (asu_sum$f.cont.sum)/length(dat) # Scaling the data between 0 to 1 for the sake of comparison across sites asu_sum$f.cont.sum &lt;- range01(asu_sum$f.cont.sum) asu_sum &lt;- as.data.frame(asu_sum) site_asu &lt;- c(site_asu, list(asu_sum)) names(site_asu)[i] &lt;- unique(site)[i] rm(dat,asu_sum) } # store the list object for later list.save(site_asu, &quot;data/site_asu.rdata&quot;) # Save the list of dataframes for future analysis siteWiseAsu &lt;- bind_rows(site_asu, .id = &quot;Site&quot;) # Write to .csv write.csv(siteWiseAsu, &quot;data/site-wise-asu.csv&quot;, row.names = F) 4.5 Visual examination of ASU We will use ggplot2 to plot and save individual ASU plots for each unique site-date combination. This will then be repeated for each unique site (calculated in the previous chunk of code) # Saving the ggplots to a folder for every unique site-date combination for(i in 1:length(site_date_asu)){ # Get the data out from a list to a dataframe (else ggplot won&#39;t accept it) dat &lt;- data.frame(site_date_asu[i]) names(dat) &lt;- c(&quot;freq&quot;,&quot;f.cont&quot;,&quot;time_of_day&quot;) # Plot the data g1 &lt;- ggplot(dat, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ # scale_fill_scico(palette = &quot;vikO&quot;) + theme_bw() + labs(x=&quot;Time of Day (in hours)&quot;, y=&quot;Frequency (in kHz) &quot;) + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # Save the data to a folder ggsave(filename=&quot;&quot;,width=12, height=7, units=&quot;in&quot;, dpi = 300, plot=g1, device=&quot;png&quot;, path = paste(&quot;figs/siteByDayAsu/&quot;, paste(names(site_date_asu)[i], &quot;.png&quot;, sep=&quot;&quot;), sep=&quot;&quot;)) } # Saving the ggplots to a folder for every unique site for(j in 1:length(site_asu)){ # Get the data out from a list to a .dataframe (else ggplot won&#39;t accept it) dat &lt;- data.frame(site_asu[j]) names(dat) &lt;- c(&quot;freq&quot;,&quot;time_of_day&quot;,&quot;f.cont.sum&quot;) # Plot the data g1 &lt;- ggplot(dat, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ # scale_fill_scico(palette = &quot;vikO&quot;) + theme_bw() + labs(x=&quot;Time of Day (in hours)&quot;, y=&quot;Frequency (in kHz) &quot;) + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # Save the data to a folder ggsave(filename=&quot;&quot;,width=12, height=7, units=&quot;in&quot;, dpi = 300, plot=g1, device=&quot;png&quot;, path = paste(&quot;figs/siteWiseAsu/&quot;, paste(names(site_asu)[j], &quot;.png&quot;, sep=&quot;&quot;), sep=&quot;&quot;)) } 4.6 Representative figure for publication # load .rdata to plot select figures for publication site_asu &lt;- load(&quot;data/site_asu.rdata&quot;) site_asu &lt;- bind_rows(site_asu, .id=&quot;Site&quot;) # Selecting three representative sites (PANMP1B, INBS04R, INBS04U) bm &lt;- site_asu %&gt;% filter(Site==&quot;PANMP1B&quot;) ar &lt;- site_asu %&gt;% filter(Site==&quot;INBS04R&quot;) nr &lt;- site_asu %&gt;% filter(Site==&quot;INBS04U&quot;) # create separate figures for benchmark, active and passive sites and then patchwork them # benchmark site fig_bm &lt;- ggplot(bm, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ theme_bw() + labs(fill = &quot;Acoustic Space Use\\n&quot;) + theme(axis.title = element_blank(), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(bm$time_of_day), y = 23, label = &quot;Benchmark site&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5, fontface=&quot;bold&quot;) # actively restored site fig_ar &lt;- ggplot(ar, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ theme_bw() + labs(x=&quot;\\nTime of Day (in hours)&quot;, fill = &quot;Acoustic Space Use\\n&quot;) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), axis.title.y = element_blank(), legend.position = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = max(ar$time_of_day), y = 23, label = &quot;Actively restored site&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5, fontface=&quot;bold&quot;) # naturally regenerating site fig_nr &lt;- ggplot(nr, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ theme_bw() + labs(y=&quot;Frequency (in kHz)\\n&quot;, fill = &quot;Acoustic Space Use\\n&quot;) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), axis.title.x = element_blank(), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.position = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = max(nr$time_of_day), y = 23, label = &quot;Naturally regenerating site&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5, fontface=&quot;bold&quot;) # patchwork the above three for the Fig. 3 fig_asu &lt;- fig_nr + fig_ar + fig_bm ggsave(fig_asu, filename = &quot;figs/fig3.png&quot;, width=52, height=17,device = png(), units=&quot;cm&quot;, dpi = 300); dev.off() Visual examination of acoustic space use across a naturally regenerating, actively restored and benchmark site In this figure, we visually examined ASU for each of the 128 frequency bins and 24 hours across each site and treatment type. Shown here are representative figures for an NR, AR, and BM site. In this visualization, we estimated the proportion of frequency space (values between 0 to 1) that was occupied by vocalizations/sounds above 0.003 dB for every single hour of recording across 24 hours in a day. We observed largely empty frequency bins between 12 kHz to 24 kHz for the majority of AR and NR sites. For the sake of this representative figure, we show the average ASU calculated across five days for each site. However, the patterns described here are broadly consistent across days and sites (Supporting Information). In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites, and NR = Naturally regenerating forest sites. "],["non-metric-multidimensional-scaling-of-acoustic-space-use-data.html", "Section 5 Non-metric multidimensional scaling of acoustic space use data 5.1 Load necessary libraries for analysis 5.2 Load the necessary data for nmds calculations 5.3 Preparing a dataframe of space use to run ordinations 5.4 Plotting the NMDS scores 5.5 Testing multivariate homogeneity of group dispersions 5.6 Visualizing the multivariate homogeneity of group dispersions", " Section 5 Non-metric multidimensional scaling of acoustic space use data Here, we will use ordinations as a method of analyzing how acoustic space use (defined in terms of frequency and time, across 24 hours in a day) varies between treatment types. 5.1 Load necessary libraries for analysis library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(ecodist) library(RColorBrewer) library(ggforce) library(ggalt) library(extrafont) # loadfonts(device = &quot;win&quot;) # run this prior to creating publication figures # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 5.2 Load the necessary data for nmds calculations # load the site-wise space use data sitebyDayAsu &lt;- read.csv(&quot;results/site-by-day-asu.csv&quot;) # separate the columns sitebyDayAsu &lt;- separate(sitebyDayAsu, col = Site_Day, into = c(&quot;Site&quot;,&quot;Day&quot;), sep = &quot;_&quot;) # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% dplyr::select(&quot;Site.code&quot;,&quot;Restoration.type&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # Add restoration type column to the space use data sitebyDayAsu &lt;- left_join(sitebyDayAsu,sites, by=c(&quot;Site&quot;=&quot;Site.code&quot;)) # Supplementary analysis: space use during diurnal and nocturnal periods diurnal &lt;- c(&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;,&quot;08:00-09:00&quot;, &quot;09:00-10:00&quot;,&quot;10:00-11:00&quot;,&quot;11:00-12:00&quot;, &quot;12:00-13:00&quot;,&quot;13:00-14:00&quot;,&quot;14:00-15:00&quot;, &quot;15:00-16:00&quot;,&quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;) diurnalAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% diurnal) nocturnal &lt;- c(&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;,&quot;20:00-21:00&quot;,&quot;21:00-22:00&quot;, &quot;22:00-23:00&quot;,&quot;23:00-00:00&quot;,&quot;00:00-01:00&quot;,&quot;01:00-02:00&quot;, &quot;02:00-03:00&quot;,&quot;03:00-04:00&quot;,&quot;04:00-05:00&quot;,&quot;05:00-06:00&quot;) nocturnalAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% nocturnal) 5.3 Preparing a dataframe of space use to run ordinations # Please note that while creating this dataframe for ordinations, the overall acoustic space use (sum of f.cont.sum column) was calculated by frequency bin, irrespective of the time of day for a given site. In other words, for 24 hours of data, a single value of space use was obtained for a given frequency bin. The frequency bin was pivoted from long to wide and each column essentially corresponded to a frequency bin. Within that frequency bin, grouped for each site, an overall value of space use was computed. nmdsDat &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, freq, f.cont, Restoration.type) %&gt;% group_by(Site, Restoration.type, freq) %&gt;% summarise(totSpaceuse = sum(f.cont)) %&gt;% arrange(Restoration.type) %&gt;% pivot_wider (names_from = freq, values_from = totSpaceuse, values_fill = list(totSpaceuse=0)) nmdsDiurnal &lt;- diurnalAsu %&gt;% dplyr::select(Site, freq, f.cont, Restoration.type) %&gt;% group_by(Site, Restoration.type, freq) %&gt;% summarise(totSpaceuse = sum(f.cont)) %&gt;% arrange(Restoration.type) %&gt;% pivot_wider (names_from = freq, values_from = totSpaceuse, values_fill = list(totSpaceuse=0)) nmdsNocturnal &lt;- nocturnalAsu %&gt;% dplyr::select(Site, freq, f.cont, Restoration.type) %&gt;% group_by(Site, Restoration.type, freq) %&gt;% summarise(totSpaceuse = sum(f.cont)) %&gt;% arrange(Restoration.type) %&gt;% pivot_wider (names_from = freq, values_from = totSpaceuse, values_fill = list(totSpaceuse=0)) # Convert to matrix form nmdsDatMatrix &lt;- as.matrix(nmdsDat[, 3:ncol(nmdsDat)]) nmdsDiurnalMatrix &lt;- as.matrix(nmdsDiurnal[, 3:ncol(nmdsDiurnal)]) nmdsNocturnalMatrix &lt;- as.matrix(nmdsNocturnal[, 3:ncol(nmdsNocturnal)]) # Run a euclidean dissimilarity distance and use metaMDS function from vegan to run ordinations disEuclidean &lt;- vegdist(nmdsDatMatrix, method = &quot;euclidean&quot;) disEuclideanDiurnal &lt;- vegdist(nmdsDiurnalMatrix, method = &quot;euclidean&quot;) disEuclideanNocturnal &lt;- vegdist(nmdsNocturnalMatrix, method = &quot;euclidean&quot;) nmdsEuclidean &lt;- vegdist (nmdsDatMatrix, method = &quot;euclidean&quot;) %&gt;% metaMDS (nmdsEuclidean, k=6) # stress = 0.007478165 nmdsEuclideanDiurnal &lt;- vegdist (nmdsDiurnalMatrix, method = &quot;euclidean&quot;) %&gt;% metaMDS (nmdsEuclideanDiurnal, k=6) # stress = 0.003387608 nmdsEuclideanNocturnal &lt;- vegdist (nmdsNocturnalMatrix, method = &quot;euclidean&quot;) %&gt;% metaMDS (nmdsEuclideanNocturnal, k=6) # stress = 0.01572579 # extract nmds scores nmdsScores &lt;- as_tibble(scores(nmdsEuclidean, display = &quot;site&quot;)) nmdsScoresDiurnal &lt;- as_tibble(scores(nmdsEuclideanDiurnal, display = &quot;site&quot;)) nmdsScoresNocturnal &lt;- as_tibble(scores(nmdsEuclideanNocturnal, display = &quot;site&quot;)) # Write the scores to a separate .csv write.csv(nmdsScores, &quot;results/nmds-acousticSpaceUse.csv&quot;, row.names = F) # With the above analysis, we note the stress is 0.008131476. However, if stress is high, we should reposition the points in 2 dimensions in the direction of decreasing stress, and repeat until stress is below some threshold.**A good rule of thumb: stress &lt; 0.05 provides an excellent representation in reduced dimensions, &lt; 0.1 is great, &lt; 0.2 is good/ok, and stress &lt; 0.3 provides a poor representation.** To reiterate: high stress is bad, low stress is good! # write scores for diurnal and nocturnal data write.csv(nmdsScoresDiurnal, &quot;results/nmds-acousticSpaceUse-diurnal.csv&quot;, row.names = F) write.csv(nmdsScoresNocturnal, &quot;results/nmds-acousticSpaceUse-nocturnal.csv&quot;, row.names = F) 5.4 Plotting the NMDS scores # First let&#39;s add the treatment type back to the nmds scores nmdsScores$Restoration.type &lt;- nmdsDat$Restoration.type nmdsScoresDiurnal$Restoration.type &lt;- nmdsDiurnal$Restoration.type nmdsScoresNocturnal$Restoration.type &lt;- nmdsNocturnal$Restoration.type # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting nmdsScores$Restoration.type &lt;- factor(nmdsScores$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) nmdsScoresDiurnal$Restoration.type &lt;- factor(nmdsScoresDiurnal$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) nmdsScoresNocturnal$Restoration.type &lt;- factor(nmdsScoresNocturnal$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_nmds &lt;- ggplot(data=nmdsScores) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=5) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScores$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScores$NMDS1), y = max(nmdsScores$NMDS2), label = &quot;Stress = 0.008&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) fig_nmds_diurnal &lt;- ggplot(data=nmdsScoresDiurnal) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=5) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScoresDiurnal$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScoresDiurnal$NMDS1), y = max(nmdsScoresDiurnal$NMDS2), label = &quot;Stress = 0.003&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) fig_nmds_nocturnal &lt;- ggplot(data=nmdsScoresNocturnal) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=5) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScoresNocturnal$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScoresNocturnal$NMDS1), y = max(nmdsScoresNocturnal$NMDS2), label = &quot;Stress = 0.01&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) ggsave(fig_nmds, filename = &quot;figs/fig_nmds_overallSpaceUse.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() ggsave(fig_nmds_diurnal, filename = &quot;figs/fig_nmds_diurnalSpaceUse.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() ggsave(fig_nmds_nocturnal, filename = &quot;figs/fig_nmds_nocturnalSpaceUse.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() NMDS ordination of overall acoustic space use values revealed a distinct cluster for BM sites but overlapping clusters for AR and NR sites 5.5 Testing multivariate homogeneity of group dispersions One measure of multivariate dispersion (variance) for a group of samples is to calculate the average distance of group members to the group centroid or spatial median in multivariate space. To test if the dispersions (variances) of one or more groups are different, the distances of group members to the group centroid are subject to ANOVA. Betadisper tests whether two or more groups (for example, restored and unrestored sites) are homogeneously dispersed in relation to their species in studied samples. This test can be done to see if one group has more compositional variance than another. Moreover, homogeneity of dispersion among groups is very advisable to have if you want to test if two or more groups have different compositions, which is tested by adonis. nmdsVariance &lt;- betadisper(disEuclidean, group = nmdsDat$Restoration.type) nmdsVariance anova(nmdsVariance) permutest(nmdsVariance, pairwise = TRUE, permutations = 999) TukeyHSD(nmdsVariance) # For overall space use, there is a significant difference in within group variance between one group and another (For Benchmark-Active sites and Passive-Benchmark sites, but not between Active-Passive) nmdsVarianceDiurnal &lt;- betadisper(disEuclideanDiurnal, group = nmdsDiurnal$Restoration.type) nmdsVarianceDiurnal anova(nmdsVarianceDiurnal) permutest(nmdsVarianceDiurnal, pairwise = TRUE, permutations = 999) TukeyHSD(nmdsVarianceDiurnal) # For diurnal space use, there is a significant difference in within group variance between one group and another (For Benchmark-Active sites and Passive-Benchmark sites, but not between Active-Passive) nmdsVarianceNocturnal &lt;- betadisper(disEuclideanNocturnal, group = nmdsNocturnal$Restoration.type) nmdsVarianceNocturnal anova(nmdsVarianceNocturnal) permutest(nmdsVarianceNocturnal, pairwise = TRUE, permutations = 999) TukeyHSD(nmdsVarianceNocturnal) # For nocturnal space use, there is a significant difference in within group variance between one group and another (For Benchmark-Active sites and Passive-Benchmark sites, but not between Active-Passive) # The above results suggests that there is heterogeneous variation and does not meet assumptions to run adonis 5.6 Visualizing the multivariate homogeneity of group dispersions The below lines of code have been adapted from: https://chrischizinski.github.io/rstats/adonis/ # extract the centroids and the site points in multivariate space. centroids &lt;-data.frame(grps=rownames(nmdsVariance$centroids), data.frame(nmdsVariance$centroids)) vectors &lt;- data.frame(group=nmdsVariance$group, data.frame(nmdsVariance$vectors)) # to create the lines from the centroids to each point we will put it in a format that ggplot can handle seg.data&lt;-cbind(vectors[,1:3],centroids[rep(1:nrow(centroids),as.data.frame(table(vectors$group))$Freq),2:3]) names(seg.data)&lt;-c(&quot;group&quot;,&quot;v.PCoA1&quot;,&quot;v.PCoA2&quot;,&quot;PCoA1&quot;,&quot;PCoA2&quot;) # create the convex hulls of the outermost points grp1.hull &lt;- seg.data[seg.data$group==&quot;Active&quot;,1:3][chull(seg.data[seg.data$group==&quot;Active&quot;,2:3]),] grp2.hull &lt;- seg.data[seg.data$group==&quot;Benchmark&quot;,1:3][chull(seg.data[seg.data$group==&quot;Benchmark&quot;,2:3]),] grp3.hull &lt;- seg.data[seg.data$group==&quot;Passive&quot;,1:3][chull(seg.data[seg.data$group==&quot;Passive&quot;,2:3]),] all.hull &lt;- rbind(grp1.hull,grp2.hull,grp3.hull) # plot the panel and convex hulls fig_hull &lt;- ggplot() + geom_polygon(data= all.hull,aes(x=v.PCoA1,y=v.PCoA2),colour=&quot;black&quot;,alpha=0,linetype=&quot;dashed&quot;) + geom_segment(data=seg.data,aes(x=v.PCoA1,xend=PCoA1,y=v.PCoA2,yend=PCoA2),alpha=0.30) + geom_point(data=centroids[,1:3], aes(x=PCoA1,y=PCoA2,shape=grps),size=4,colour=&quot;red&quot;) + geom_point(data=seg.data, aes(x=v.PCoA1,y=v.PCoA2,shape=group),size=2) + labs(title=&quot;All&quot;,x=&quot;&quot;,y=&quot;&quot;) + #coord_cartesian(xlim = c(-0.2,0.2), ylim = c(-0.25,0.2)) + theme_bw() + theme(legend.position=&quot;none&quot;) "],["generalized-linear-mixed-modeling-acoustic-space-use-and-vegetation-data.html", "Section 6 Generalized linear mixed modeling (acoustic space use and vegetation data) 6.1 Install required libraries 6.2 Load necessary data for statistical modeling 6.3 Getting data ready in a format for linear modeling 6.4 Running the generalized linear mixed models 6.5 We now run generalized linear mixed models (GLMM) assuming gaussian errors and using log link functions to examine the effects of habitat (vegetation structure) on acoustic space use. 6.6 Main Text Figure 4", " Section 6 Generalized linear mixed modeling (acoustic space use and vegetation data) In this script, we run generalized linear models to test the association between acoustic space use values and restoration type. In addition, we run generalized linear mixed models to test associations between acoustic space use and habitat (vegetation structure) using site-pair name (actively restored and naturally regenerating were specified to be paired) and repeat visits as random effects. 6.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(ggpubr) library(sjPlot) library(RColorBrewer) library(extrafont) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 6.2 Load necessary data for statistical modeling # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% dplyr::select(&quot;Site.code&quot;,&quot;Restoration.type&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # load the entire asu data across all sites and days computed sitebyDayAsu &lt;- read.csv(&quot;results/site-by-day-asu.csv&quot;) # separate by Site and Date sitebyDayAsu &lt;- separate(sitebyDayAsu, col = Site_Day, into = c(&quot;Site&quot;, &quot;Date&quot;), sep = &quot;_&quot;) # Add restoration type column to the space use data sitebyDayAsu &lt;- left_join(sitebyDayAsu, sites, by=c(&quot;Site&quot;=&quot;Site.code&quot;)) # scale values per site/date for comparison between sites and treatment types sitebyDayAsu &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% mutate(f.cont.scaled = range01(f.cont)) # computing total number of days for which space use has been computed for each site (some sites have very few days. Eg. OLV110R, else rest have 5 days of audio data each) nSites_Days &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, Date, Restoration.type) %&gt;% distinct() %&gt;% group_by(Site) %&gt;% count() # Let&#39;s look at data by restoration type # This suggests that we have more data for benchmark sites relative to the other two treatment types nDays_siteType &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, Date, Restoration.type) %&gt;% distinct() %&gt;% group_by(Restoration.type) %&gt;% count() # Separate analysis: space use during diurnal and nocturnal periods diurnal &lt;- c(&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;,&quot;08:00-09:00&quot;, &quot;09:00-10:00&quot;,&quot;10:00-11:00&quot;,&quot;11:00-12:00&quot;, &quot;12:00-13:00&quot;,&quot;13:00-14:00&quot;,&quot;14:00-15:00&quot;, &quot;15:00-16:00&quot;,&quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;) diurnalAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% diurnal) nocturnal &lt;- c(&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;,&quot;20:00-21:00&quot;,&quot;21:00-22:00&quot;, &quot;22:00-23:00&quot;,&quot;23:00-00:00&quot;,&quot;00:00-01:00&quot;,&quot;01:00-02:00&quot;, &quot;02:00-03:00&quot;,&quot;03:00-04:00&quot;,&quot;04:00-05:00&quot;,&quot;05:00-06:00&quot;) nocturnalAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% nocturnal) # Prepare data for statistical modeling # Calculating total space use across all frequency bins and times of day: 128*24 for each site-day combination totSpaceUse &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # Calculating total space use for diurnal times for each site-day combination totSpaceUseDiurnal &lt;- diurnalAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # Calculating total space use for nocturnal times for each site-date combination totSpaceUseNocturnal &lt;- nocturnalAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # Load data from previous scripts for use in a GLM vegData &lt;- read.csv(&quot;results/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;results/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) 6.3 Getting data ready in a format for linear modeling # overall space use modelDataAll &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUse, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) # diurnal space use modelDataDiurnal &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUseDiurnal, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) # nocturnal space use modelDataNocturnal &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUseNocturnal, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) 6.4 Running the generalized linear mixed models We now run generalized linear mixed models (GLMM) assuming gaussian errors to examine the effects of restoration type (benchmark, actively restored and passively restored) on acoustic space use, followed by TukeyHSD multiple comparisons tests of means. # overall space use glmm_allRestoration &lt;- glmer(roundSpaceuse ~ Restoration.type + (1|siteCode) + (1|visit), data = modelDataAll, family = gaussian(link=&quot;identity&quot;)) summary(glmm_allRestoration) tukey_glmmAllRestoration &lt;- summary(glht(glmm_allRestoration, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmAllRestoration) # The above result suggests a significant difference in acoustic space use between BM-AR and BM-NR sites, but no difference between AR and NR sites. # diurnal glmm_diurnalRestoration &lt;- glmer(roundSpaceuse ~ Restoration.type + (1|siteCode) + (1|visit), data = modelDataDiurnal, family = gaussian(link=&quot;identity&quot;)) summary(glmm_diurnalRestoration) tukey_glmmDiurnalRestoration &lt;- summary(glht(glmm_diurnalRestoration, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmDiurnalRestoration) # When we look at only the diurnal data, there is a significant difference in acoustic space use between BM-AR and BM-NR sites, but no difference between AR and NR sites. # nocturnal data glmm_nocturnalRestoration &lt;- glmer(roundSpaceuse ~ Restoration.type + (1|siteCode) + (1|visit), data = modelDataNocturnal, family=gaussian(link=&quot;identity&quot;)) summary(glmm_nocturnalRestoration) tukey_glmmNocturnalRestoration &lt;- summary(glht(glmm_nocturnalRestoration, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmNocturnalRestoration) # When we look at only the diurnal data, there is a significant difference in acoustic space use between BM-AR and BM-NR sites, but no difference between AR and NR sites. # Plotting the above results # reordering factors for plotting modelDataAll$Restoration.type &lt;- factor(modelDataAll$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) fig_glmm_allRest &lt;- ggplot(modelDataAll, aes(Restoration.type, totSpaceuse, fill = Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Acoustic Space Use\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_glmm_allRest, filename = &quot;figs/fig_glmm_overallAcousticSpace.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() # reordering factors for plotting modelDataDiurnal$Restoration.type &lt;- factor(modelDataDiurnal$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_glmm_diurnalRest &lt;- ggplot(modelDataDiurnal, aes(Restoration.type, totSpaceuse, group = Restoration.type, fill = Restoration.type)) + geom_boxplot(alpha = 0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Acoustic Space Use\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_glmm_diurnalRest, filename = &quot;figs/fig_glmm_diurnalAcousticSpaceUse.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() # reordering factors for plotting modelDataNocturnal$Restoration.type &lt;- factor(modelDataNocturnal$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_glmm_nocturnalRest &lt;- ggplot(modelDataNocturnal, aes(Restoration.type, totSpaceuse, group = Restoration.type, fill = Restoration.type)) + geom_boxplot(alpha = 0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Acoustic Space Use\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_glmm_nocturnalRest, filename = &quot;figs/fig_glmm_nocturnalAcousticSpaceUse.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() Overall acoustic space use across treatment types. Analysis of ~4,128 hours of acoustic data across treatment types (for the 43 sites) revealed significant differences in ASU across BM-AR and BM-NR sites, with the highest estimate in BM sites (mean ± SD: 413 ± 103), followed by AR sites (mean ± SD: 188 ± 78) and NR sites (mean ± SD: 182 ± 66). In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites and NR = Naturally regenerating forest sites. 6.5 We now run generalized linear mixed models (GLMM) assuming gaussian errors and using log link functions to examine the effects of habitat (vegetation structure) on acoustic space use. # overall space use glmm_allVeg&lt;- glmer(roundSpaceuse ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelDataAll, family = gaussian(link=&quot;identity&quot;)) summary(glmm_allVeg) plot_model(glmm_allVeg, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_allVeg) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict roundSpaceuse with PC1 and PC2 (formula: roundSpaceuse ~ PC1 + PC2). The model included siteCode and visit as random effects (formula: list(~1 | siteCode, ~1 | visit)). The model&#39;s total explanatory power is substantial (conditional R2 = 0.77) and the part related to the fixed effects alone (marginal R2) is of 0.10. The model&#39;s intercept, corresponding to PC1 = 0 and PC2 = 0, is at 294.12 (95% CI [254.39, 333.84], t(204) = 14.60, p &lt; .001). Within this model: # - The effect of PC1 is statistically significant and negative (beta = -20.20, 95% CI [-29.35, -11.04], t(204) = -4.35, p &lt; .001; Std. beta = -0.28, 95% CI [-0.41, -0.15]) # - The effect of PC2 is statistically non-significant and negative (beta = -12.66, 95% CI [-26.83, 1.51], t(204) = -1.76, p = 0.080; Std. beta = -0.10, 95% CI [-0.22, 0.01]) # Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation # diurnal glmm_diurnalVeg&lt;- glmer(roundSpaceuse ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelDataDiurnal, family = gaussian(link=&quot;identity&quot;)) summary(glmm_diurnalVeg) plot_model(glmm_diurnalVeg, type=&quot;pred&quot;, terms=c(&quot;PC2&quot;,&quot;PC1&quot;)) report::report(glmm_diurnalVeg) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict roundSpaceuse with PC1 and PC2 (formula: roundSpaceuse ~ PC1 + PC2). The model included siteCode and visit as random effects (formula: list(~1 | siteCode, ~1 | visit)). The model&#39;s explanatory power related to the fixed effects alone (marginal R2) is 0.26. The model&#39;s intercept, corresponding to PC1 = 0 and PC2 = 0, is at 145.60 (95% CI [122.30, 168.89], t(204) = 12.32, p &lt; .001). Within this model: # - The effect of PC1 is statistically significant and negative (beta = -12.91, 95% CI [-18.99, -6.82], t(204) = -4.18, p &lt; .001; Std. beta = -0.29, 95% CI [-0.43, -0.15]) # - The effect of PC2 is statistically non-significant and negative (beta = -1.10, 95% CI [-10.62, 8.43], t(204) = -0.23, p = 0.821; Std. beta = -0.01, 95% CI [-0.14, 0.11]) # Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation. # nocturnal glmm_nocturnalVeg&lt;- glmer(roundSpaceuse ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelDataNocturnal, family = gaussian(link=&quot;identity&quot;)) summary(glmm_nocturnalVeg) plot_model(glmm_nocturnalVeg, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_nocturnalVeg) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict roundSpaceuse with PC1 and PC2 (formula: roundSpaceuse ~ PC1 + PC2). The model included siteCode and visit as random effects (formula: list(~1 | siteCode, ~1 | visit)). The model&#39;s total explanatory power is substantial (conditional R2 = 0.80) and the part related to the fixed effects alone (marginal R2) is of 0.05. The model&#39;s intercept, corresponding to PC1 = 0 and PC2 = 0, is at 134.62 (95% CI [110.39, 158.85], t(204) = 10.96, p &lt; .001). Within this model: # - The effect of PC1 is statistically significant and negative (beta = -6.95, 95% CI [-12.04, -1.87], t(204) = -2.70, p = 0.008; Std. beta = -0.19, 95% CI [-0.32, -0.05]) # - The effect of PC2 is statistically significant and negative (beta = -10.09, 95% CI [-17.89, -2.28], t(204) = -2.55, p = 0.012; Std. beta = -0.16, 95% CI [-0.28, -0.04]) # Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation. 6.6 Main Text Figure 4 # Patchworking nmds and glmm for acoustic space use to create part a and b for Fig. 4 library(patchwork) fig_glmm_ordinations &lt;- wrap_plots(fig_glmm_allRest, fig_nmds, design = &quot;AABBBB&quot; ) + plot_annotation( tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot; ) # Expand the width to avoid compression ggsave(fig_glmm_ordinations, filename = &quot;figs/fig04.png&quot;, width=20, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() Acoustic space use across treatment types. (a) Analysis of ~4,128 hours of acoustic data across treatment types (for the 43 sites) revealed significant differences in ASU between BM-AR and BM-NR sites (Tukey HSD P &lt; 0.05). ASU was the highest in BM sites (mean ± SD: 413 ± 103), followed by AR sites (mean ± SD: 188 ± 78) and NR sites (mean ± SD: 182 ± 66). (b) NMDS ordination of ASU (stress = 0.008) revealed a loose cluster of benchmark sites, while actively restored and naturally regenerating sites showed tight overlapping clusters. In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites, and NR = Naturally regenerating forest sites. "],["splitting-large-.wav-files.html", "Section 7 Splitting large .wav files 7.1 Load required libraries 7.2 Selecting dawn acoustic data 7.3 Split the files", " Section 7 Splitting large .wav files Here, we will first split the raw data which was collected for 24 hours at a site, for 7 days at a stretch. This is being done for the sake of manual annotation of bird species. The deployment schedule of the AudioMoths was set to record for 4-minutes and was switched off for 1-min. For the sake of analysis, data was split into 10s chunks and annotated manually using Raven Pro. 7.1 Load required libraries library(warbleR) library(seewave) library(dplyr) library(stringr) library(tools) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 7.2 Selecting dawn acoustic data We will use warbleR::split.wavs() to split a large file. To do so, we will first load a list of .wav files from folders (will have to be done site by site). Next, we we select only files between 6 am and 10 am (this can be varied depending on the exercise or the question at hand). For each day selected, we randomly extracted a continuous 16-min of recording. # List the path that contains all folders, which contain the audiomoth data path &lt;- &quot;C:\\\\data\\\\2020-winter\\\\&quot; # Listing the folders within which .WAV files are stored folders &lt;- dir(path, recursive=F,full.names=T) # Now get only those files that begin at 6am and end at 10am files &lt;- list() for(i in 1:length(folders)){ setwd(folders[i]) # Below code needs to be run only if we have to rename files # List the files within each folder and renaming the files with the prefix - SITE_ID a &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;), full.names = T) file.rename(from = a, to=paste0(basename(folders)[i],&quot;_&quot;,basename(a))) # Extract the strings for .wav files between 6am and 10am time_str &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;),full.names = T) %&gt;% tools::file_path_sans_ext() %&gt;% str_extract(&#39;\\\\d+$&#39;) time_str &lt;- time_str[time_str&gt;=&quot;060000&quot; &amp; time_str &lt;=&quot;100000&quot;] # vary times here depending on the question at hand for(j in 1:length(unique(time_str))){ b &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;),full.names = T, pattern = time_str[j]) files &lt;- c(files,b) } } # These are the list of files we need files &lt;- unlist(files) # Now we choose a random consecutive 16 min of data between 6am and 10am # Get a list of unique dates (since we will be generating a random 16min for every date across every site) site_date &lt;- str_extract(basename(files),&#39;\\\\w+_\\\\d+_&#39;) unique(site_date) # Give you unique date and sites for which we need to generate 16 min of data subset_files &lt;- list() for(i in 1:length(unique(site_date))){ a &lt;- files[str_detect(files,unique(site_date)[i])] if(length(a)&lt;4){ # essentially specifies that the min number you need next } else { subset_dat &lt;- extractRandWindow(a,4) subset_dat &lt;- na.exclude(subset_dat) # If there are less than 4 files subset_files &lt;- c(subset_files, subset_dat) } } final_subset &lt;- unlist(subset_files) # Subset those files and copy it to a separate folder # Please note that these folders &amp; files are locally stored (they are extremely large and cannot be added to GitHub) dir.create(paste0(&quot;C:\\\\data\\\\&quot;,&quot;subset&quot;)) file.copy(from = final_subset, to=&quot;C:\\\\data\\\\subset\\\\&quot;) 7.3 Split the files Split the files and provide unique names to each file # Note: the path you choose to store data is upto the user. subset_path &lt;- &quot;C:\\\\data\\\\subset\\\\&quot; # Split the files into n-second chunks split_wavs(path=subset_path, sgmt.dur = 10, parallel=4) # Get files that need to be renamed split_files &lt;- list.files(subset_path, full.names = T, pattern = &quot;-&quot;) # Note the number of chunks will vary as a function of segment duration # 240 seconds = 24 chunks each of 10s setwd(subset_path) chunks &lt;- c(&quot;01-10&quot;,&quot;10-20&quot;,&quot;20-30&quot;, &quot;30-40&quot;,&quot;40-50&quot;,&quot;50-60&quot;, &quot;60-70&quot;,&quot;70-80&quot;,&quot;80-90&quot;, &quot;90-100&quot;,&quot;100-110&quot;,&quot;110-120&quot;, &quot;120-130&quot;,&quot;130-140&quot;,&quot;140-150&quot;, &quot;150-160&quot;,&quot;160-170&quot;,&quot;170-180&quot;, &quot;180-190&quot;,&quot;190-200&quot;,&quot;200-210&quot;, &quot;210-220&quot;,&quot;220-230&quot;,&quot;230-240&quot;) for(i in 1:length(chunks)){ c &lt;- split_files[endsWith(split_files,paste0(&quot;-&quot;,i,&quot;.wav&quot;))] d &lt;- str_replace(c,paste0(&quot;-&quot;,i),paste0(&quot;_&quot;,chunks[i])) file.rename(from=c, to=d) } # Remove the original files orig_files &lt;- list.files(subset_path, full.names = T, pattern = &quot;.WAV$&quot;) file.remove(orig_files) Now, go ahead and begin the process of manual annotation! "],["bird-species-richness.html", "Section 8 Bird Species Richness 8.1 Install required libraries 8.2 Load manual annotations data 8.3 Subset data 8.4 Total number of detections 8.5 Calculate species richness", " Section 8 Bird Species Richness In this script, we will calculate: Site-wise species richness to understand if species composition across treatment types are distinctly different. Repeat the above calculations, but using species traits - If a species is a rainforest specialist or an open-country generalist. 8.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(data.table) library(extrafont) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 8.2 Load manual annotations data To start with, we will compute species richness from 3 non-consecutive visits for each site for each season. Across two seasons, a total of 6 non-consecutive visits are chosen for each site. We will use an excel sheet of manual annotations, which contains 10s-clips from each site (A random 16-min was chosen between 6am and 10am and divided into 10s chunks for the ease of annotations). # Attach the annotation data for summer and winter summer_data &lt;- read.csv(&quot;data/2020-summer-annotation-working-document.csv&quot;) winter_data &lt;- read.csv(&quot;data/2020-winter-annotation-working-document.csv&quot;) ## Please write to the lead author if you would like to access this data (Vijay Ramesh; vr2352@columbia.edu) # combine the datasets for a single dataframe for the present analysis data &lt;- bind_rows(summer_data,winter_data) names(data) # reorder columns data &lt;- data %&gt;% relocate(c(&quot;BFO&quot;,&quot;SBEO&quot;, &quot;JN&quot;, &quot;AK&quot;, &quot;HSWP&quot;), .after = &quot;CR&quot;) # certain species codes are mentioned here and these can be found in the supplementary data. # Site-wise sorting of the 16-min of data # Split the file names into 4 columns : Site, Date, Time and Splits data &lt;- separate(data, col = Filename, into = c(&quot;Site&quot;, &quot;Date&quot;, &quot;Time&quot;, &quot;Splits&quot;), sep = &quot;_&quot;) data # Load the species-trait-data trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) 8.3 Subset data How many visits (counted as number of unique days) to a site has been annotated? Please note that some sites have more than 6 visits to a site, and hence we ensure that we choose only 6 non-consecutive visits to a given site. NOTE: You need to run this only once and if you have already run it, move to the next chunk of code and load datSubset. # First we remove OLCAP5B - a site for which only 3 visits were made in summer and not sampled as a result of logistic reasons in winter data &lt;- data %&gt;% filter(!str_detect(Site, &#39;OLCAP5B&#39;)) # Number of visits to a particular site # Only INBS04U has 5 visits because another visit was not possible due to rain nSites_Days &lt;- data %&gt;% dplyr::select(Site, Date)%&gt;% distinct() %&gt;% arrange(Site) %&gt;% count(Site) # Unique date site combination to give you a sense of sampling uniqueSiteDate &lt;- data %&gt;% group_by(Site) %&gt;% distinct(Date) # Convert date column to YMD format using lubridate::ymd() uniqueSiteDate$Date &lt;- lubridate::ymd(uniqueSiteDate$Date) # The below lines of code were written following a query on stackOverflow to select six non-consecutive visits to any site # Link: https://stackoverflow.com/questions/67212152/select-non-consecutive-dates-for-every-grouped-element-in-r nonConVisits &lt;- uniqueSiteDate%&gt;% ungroup() %&gt;% group_split(Site) %&gt;% map_df(., ~ .x %&gt;% ungroup() %&gt;% arrange(Date) %&gt;% mutate(n = 1) %&gt;% complete(Date = seq.Date(first(Date), last(Date), by = &#39;days&#39;))%&gt;% group_by(n = cumsum(is.na(n))) %&gt;% filter(!is.na(Site)) %&gt;% filter(row_number() %% 2 == 1) %&gt;% ungroup() %&gt;% sample_n(min(n(), 6)) ) %&gt;% dplyr::select(-n) # Change the structure of the date column back to character for using one of the join functions nonConVisits$Date &lt;- str_remove_all(as.character(nonConVisits$Date),&quot;-&quot;) # Left-join with the original dataframe to subset the data for analysis datSubset &lt;- left_join(nonConVisits,data) names(datSubset) # renaming columns datSubset &lt;- rename(datSubset, Restoration.type = Restoration.Type..Benchmark.Active.Passive.) # Save this data as a .csv for future analysis (later scripts) write.csv(datSubset,&quot;results/datSubset.csv&quot;, row.names = F) 8.4 Total number of detections Now group the data by site and restoration type and sum the number of detections across sites. We will calculate the overall number of detections for each 10s clip, which will be used to estimate species richness below. # If you have already processed subsetting of data, read it in directly datSubset &lt;- read.csv(&quot;results/datSubset.csv&quot;) # Calculate the overall number of detections for each site across 6 days of data (translates to ~ 96 min of data per site) nDetections_Site &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) 8.5 Calculate species richness Convert the detections to 1, since we are interested in calculating richness per site by converting values &gt;1 to 1 for multiple visits to a site. In other words, we want to establish overall species richness for a 16-min to 48-min window. richness &lt;- nDetections_Site %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, richness) # Test if there are significant differences in richness across treatment types anovaAll &lt;- aov(richness~Restoration.type, data = richness) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukeyAll &lt;- TukeyHSD(anovaAll) # The above result suggests that there are no differences in overall species richness across treatment types # Create a boxplot of species richness by Restoration Type # reordering factors for plotting richness$Restoration.type &lt;- factor(richness$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) fig_richness &lt;- ggplot(richness, aes(x=Restoration.type, y=richness, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Species Richness\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_richness, filename = &quot;figs/fig_richness.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() # We observe that the overall species richness is relatively higher in passively restored forest patches, followed by benchmark (protected area) forest patches and actively restored forest patches (But these differences are not significant) ## Species richness by trait Using species trait data to check if species richness varies by treatment type as a function of whether a species is a rainforest specialist vs. open-country specialist. To do so: Add an additional column of species-trait data and group data based on the same. # First, we pivot the species-codes and then match the codes with trait_data and reformat the data to keep all detections&gt;0 as 1 else they are 0 richness_trait &lt;- nDetections_Site %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1, count==0 ~ 0)) # Calculate overall richness for each site as a function of rainforest species and open-country species richness_trait &lt;- richness_trait %&gt;% dplyr::select(Site, Restoration.type, Species_Code, habitat, forRichness)%&gt;% group_by(Site, Restoration.type, habitat) %&gt;% summarise(richness = sum(forRichness)) %&gt;% drop_na() # Let&#39;s subset data for richness as a function of rainforest specialists and open-country generalists and test for significant differences (if any) richness_rainforest &lt;- richness_trait %&gt;% filter(habitat==&quot;RF&quot;) richness_opencountry &lt;- richness_trait %&gt;% filter(habitat==&quot;OC&quot;) # Test if there are significant differences in richness across treatment types as a function of species trait anova_rainforest &lt;- aov(richness~Restoration.type, data = richness_rainforest) anova_opencountry &lt;- aov(richness~Restoration.type, data = richness_opencountry) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types for rainforest birds, but a significant difference between benchmark-active and benchmark-passive sites for open country birds tukey_rainforest &lt;- TukeyHSD(anova_rainforest) tukey_opencountry &lt;- TukeyHSD(anova_opencountry) # reordering factors for plotting richness_trait$Restoration.type &lt;- factor(richness_trait$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Plot the above result fig_richness_trait &lt;- ggplot(richness_trait, aes(x=Restoration.type, y=richness, fill=habitat)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;, labels=c(&quot;Open country&quot;,&quot;Rainforest&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Species Richness\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) ggsave(fig_richness_trait, filename = &quot;figs/fig_richness_trait.png&quot;, width=12, height=7, device =png(), units=&quot;in&quot;, dpi = 300); dev.off() No significant differences were observed for rainforest species across treatment types, but we noticed a significant difference between BM-NR and BM-AR for open-country birds. "],["acoustic-detections.html", "Section 9 Acoustic Detections 9.1 Install required libraries 9.2 Load data 9.3 Overall number of detections 9.4 Detections by treatment type 9.5 Detections by species traits", " Section 9 Acoustic Detections In this script, we will calculate: Total Number of detections across sites (reported for varying time intervals 10s, 30s, 1-min, 2-min, 4-min). Repeat the above calculations, but using species traits - If a species is a rainforest specialist or an open-country generalist. 9.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(extrafont) 9.2 Load data We will use the data that was subset previously for further analysis. # Attach the annotated data datSubset &lt;- read.csv(&quot;results/datSubset.csv&quot;) # Load the species-trait-data trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) 9.3 Overall number of detections Given the data annotated so far, we will calculate the overall number of detections across different temporal periods, starting from 10s to 30-s, 1-min, 2-min and 4-min. We will first calculate the overall number of detections for the shortest possible temporal duration which could be annotated confidently - 10s. Other durations are chosen to confirm if the number of detections vary as a function of the temporal duration. # Calculate the overall number of detections for each site where each temporal duration chosen is a 10s clip nDetections_10s &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 30s clip (In this case, every third row is chosen after grouping by Site, Date and Time) nDetections_30s &lt;- datSubset %&gt;% mutate(Splits = case_when((Splits == &quot;01-10&quot; | Splits==&quot;10-20&quot; | Splits ==&quot;20-30&quot;) ~ &quot;1&quot;,(Splits == &quot;30-40&quot; | Splits==&quot;40-50&quot; | Splits ==&quot;50-60&quot;) ~ &quot;2&quot;,(Splits == &quot;60-70&quot; | Splits==&quot;70-80&quot; | Splits ==&quot;80-90&quot;) ~ &quot;3&quot;, (Splits == &quot;90-100&quot; | Splits==&quot;100-110&quot; | Splits ==&quot;110-120&quot;) ~ &quot;4&quot;,(Splits == &quot;120-130&quot; | Splits==&quot;130-140&quot; | Splits ==&quot;140-150&quot;) ~ &quot;5&quot;,(Splits == &quot;150-160&quot; | Splits==&quot;160-170&quot; | Splits ==&quot;170-180&quot;) ~ &quot;6&quot;,(Splits == &quot;180-190&quot; | Splits==&quot;190-200&quot; | Splits ==&quot;200-210&quot;) ~ &quot;7&quot;, (Splits == &quot;210-220&quot; | Splits==&quot;220-230&quot; | Splits ==&quot;230-240&quot;) ~&quot;8&quot;)) %&gt;% group_by(Site, Date, Time, Splits, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections &gt;1 within a 30s period to 1 (since your temporal unit here is 30s) nDetections_30s &lt;- nDetections_30s %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 60s clip (In this case, every sixth row is chosen after grouping by Site, Date and Time) nDetections_1min &lt;- datSubset %&gt;% mutate(Splits = case_when((Splits == &quot;01-10&quot; | Splits==&quot;10-20&quot; | Splits ==&quot;20-30&quot; | Splits == &quot;30-40&quot; | Splits==&quot;40-50&quot; | Splits ==&quot;50-60&quot;) ~ &quot;1&quot;, (Splits == &quot;60-70&quot; | Splits==&quot;70-80&quot; | Splits ==&quot;80-90&quot; | Splits == &quot;90-100&quot; | Splits==&quot;100-110&quot; | Splits ==&quot;110-120&quot;) ~ &quot;2&quot;, (Splits == &quot;120-130&quot; | Splits==&quot;130-140&quot; | Splits ==&quot;140-150&quot; | Splits == &quot;150-160&quot; | Splits==&quot;160-170&quot; | Splits ==&quot;170-180&quot;) ~ &quot;3&quot;, (Splits == &quot;180-190&quot; | Splits==&quot;190-200&quot; | Splits ==&quot;200-210&quot; | Splits == &quot;210-220&quot; | Splits==&quot;220-230&quot; | Splits ==&quot;230-240&quot;) ~&quot;4&quot;)) %&gt;% group_by(Site, Date, Time, Splits, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections&gt;1 within a 1-min period to 1 (since your temporal unit here is 1-min) nDetections_1min &lt;- nDetections_1min %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 120s clip (In this case, every twelfth row is chosen after grouping by Site, Date and Time) nDetections_2min &lt;- datSubset %&gt;% mutate(Splits = case_when((Splits == &quot;01-10&quot; | Splits==&quot;10-20&quot; | Splits ==&quot;20-30&quot; | Splits == &quot;30-40&quot; | Splits==&quot;40-50&quot; | Splits ==&quot;50-60&quot; | Splits == &quot;60-70&quot; | Splits==&quot;70-80&quot; | Splits ==&quot;80-90&quot; | Splits == &quot;90-100&quot; | Splits==&quot;100-110&quot; | Splits ==&quot;110-120&quot;) ~ &quot;1&quot;, (Splits == &quot;120-130&quot; | Splits==&quot;130-140&quot; | Splits ==&quot;140-150&quot; | Splits == &quot;150-160&quot; | Splits==&quot;160-170&quot; | Splits ==&quot;170-180&quot; | Splits == &quot;180-190&quot; | Splits==&quot;190-200&quot; | Splits ==&quot;200-210&quot; | Splits == &quot;210-220&quot; | Splits==&quot;220-230&quot; | Splits ==&quot;230-240&quot;) ~&quot;2&quot;)) %&gt;% group_by(Site, Date, Time, Splits, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections&gt;1 within a 2-min period to 1 (since your temporal unit here is 2-min) nDetections_2min &lt;- nDetections_2min %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 240s clip (In this case, every twentyfourth row is chosen after grouping by Site, Date and Time) nDetections_4min &lt;- datSubset %&gt;% group_by(Site, Date, Time, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections&gt;1 within a 4-min period to 1 (since your temporal unit here is 4-min) nDetections_4min &lt;- nDetections_4min %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) 9.4 Detections by treatment type How does the number of detections vary as a function of restoration type? (tested across different temporal durations)? # Testing if there is significant differences in overall number of detections across treatment types (this has been done only for the smallest temporal duration ~10s, while total number of detections have been estimated for different temporal durations) sum_Detections10s &lt;- nDetections_10s %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) # Test if there are significant differences in detections across treatment types anovaAllDetect &lt;- aov(sumDetections~Restoration.type, data = sum_Detections10s) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukeyAllDetect &lt;- TukeyHSD(anovaAllDetect) # Estimating total number of detections for different temporal durations sum_Detections30s &lt;- nDetections_30s %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) sum_Detections1min &lt;- nDetections_1min %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) sum_Detections2min &lt;- nDetections_2min %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) sum_Detections4min &lt;- nDetections_4min %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) # Plotting the above (multiple plots for each temporal duration) # Note: the cumulative number of detections across all species was obtained by summing every 16-min to 48-min set of detections across each site, including all species. # reordering factors for plotting sum_Detections10s$Restoration.type &lt;- factor(sum_Detections10s$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) fig_sumDetections10s &lt;- ggplot(sum_Detections10s, aes(x=Restoration.type, y=sumDetections, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Cumulative number of detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_sumDetections10s, filename = &quot;figs/fig_sumDetections10s.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() No significant difference in overall number of detections across treatment types. 9.5 Detections by species traits How does the cumulative number of detections vary by treatment type, as a function of whether a species is a rainforest specialist or an open country generalist? (These calculations are repeated for different temporal durations to assess differences, if any) # First we merge the species trait dataset with the nDetections dataframe (across different temporal durations) detections_trait10s &lt;- nDetections_10s %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait30s &lt;- nDetections_30s %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait1min &lt;- nDetections_1min %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait2min &lt;- nDetections_2min %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait4min &lt;- nDetections_4min %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) # Calculate overall number of detections for each site as a function of rainforest species and open-country species and test for differences across treatment types (Calculated only for the smallest temporal duration - 10s) detections_trait10s &lt;- detections_trait10s %&gt;% dplyr::select(Site, Restoration.type, Species_Code, habitat, count) %&gt;% group_by(Site, Restoration.type, habitat) %&gt;% summarise(sumDetections = sum(count)) %&gt;% drop_na() # Split the above data into rainforest species and open country detections_10s_rainforest &lt;- detections_trait10s %&gt;% filter(habitat==&quot;RF&quot;) detections_10s_openCountry &lt;- detections_trait10s %&gt;% filter(habitat==&quot;OC&quot;) # Test if there are significant differences in detections across treatment types as a function of species trait anova_rainforestDet &lt;- aov(sumDetections~Restoration.type, data = detections_10s_rainforest) anova_opencountryDet &lt;- aov(sumDetections~Restoration.type, data = detections_10s_openCountry) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukey_rainforestDet &lt;- TukeyHSD(anova_rainforestDet) tukey_opencountryDet &lt;- TukeyHSD(anova_opencountryDet) # The above results for rainforest birds reveal a significant difference in rainforest bird detections between benchmark sites and active sites and benchmark sites and passive sites. # For open-country birds, there is a significant difference between every pair of treatment type. # Calculating overall number of detections for other temporal durations as a function of species trait detections_trait30s &lt;- detections_trait30s %&gt;% dplyr::select(Site, Restoration.type, Species_Code, habitat, count) %&gt;% group_by(Site, Restoration.type, habitat) %&gt;% summarise(sumDetections = sum(count)) detections_trait1min &lt;- detections_trait1min %&gt;% dplyr::select(Site, Restoration.type, Species_Code, habitat, count) %&gt;% group_by(Site, Restoration.type, habitat) %&gt;% summarise(sumDetections = sum(count)) detections_trait2min &lt;- detections_trait2min %&gt;% dplyr::select(Site, Restoration.type, Species_Code, habitat, count) %&gt;% group_by(Site, Restoration.type, habitat) %&gt;% summarise(sumDetections = sum(count)) detections_trait4min &lt;- detections_trait4min %&gt;% dplyr::select(Site, Restoration.type, Species_Code, habitat, count) %&gt;% group_by(Site, Restoration.type, habitat) %&gt;% summarise(sumDetections = sum(count)) # Plot the figures # reordering factors for plotting detections_trait10s$Restoration.type &lt;- factor(detections_trait10s$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_detections_trait10s &lt;- ggplot(detections_trait10s, aes(x=Restoration.type, y=sumDetections, fill=habitat)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;, labels=c(&quot;Open country&quot;,&quot;Rainforest&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Cumulative number of detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) # Save the above plot ggsave(fig_detections_trait10s, filename = &quot;figs/fig_detections_trait10s.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() Overall number of rainforest bird species detections differed between BM and AR and BM and NR sites. For open-country bird species detections, we observed significant differences across every pair of treatment types. "],["species-proportions.html", "Section 10 Species proportions 10.1 Install required libraries 10.2 Load the necessary data to calculate proportions 10.3 Proportion of acoustic detections across all 10-s chunks for every site-date combination 10.4 Testing for differences in species acoustic detections across treatment types 10.5 Proportion of detections across acoustic point counts 10.6 Testing for differences in rainforest and open-country species detections across acoustic point counts", " Section 10 Species proportions In this script, we will calculate the proportion of rainforest and open-country bird species that was detected across the three treatment types for a given period of time. Species richness may only give us a broader understanding of the overall levels of diversity, but getting at a measure of abundance (where we calculate the proportion of rainforest and open-country birds that were vocal in a given habitat type for a given period of time) may provide insights on the use of a particular habitat - an important question that needs to be answered, given that we would like to understand if fauna are using actively restored habitats. 10.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(data.table) library(extrafont) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 10.2 Load the necessary data to calculate proportions # We load the subset data datSubset &lt;- read.csv(&quot;results/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # Site-summary (Number of detections across all sites) datSummary &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) 10.3 Proportion of acoustic detections across all 10-s chunks for every site-date combination # Calculate the overall number of detections for each site across 6 days of data (translates to ~96-min of data per site; each detection corresponding to a temporal unit of 10 seconds). Here, we include dates, since each visit can explain the detections/proportion of detections for future modeling. nDetections_site_date &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Combine the nDetections and trait based data nDetections_trait &lt;- nDetections_site_date %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forProp = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) # Extract proportion of acoustic detections totRainDetections &lt;- nDetections_trait %&gt;% filter(habitat==&quot;RF&quot;) %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totRFDetections = sum(nDetections)) totOpenDetections &lt;- nDetections_trait %&gt;% filter(habitat==&quot;OC&quot;) %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totOCDetections = sum(nDetections)) # proportion of acoustic detections (note: sampling unit = 10-s) prop &lt;- left_join(totRainDetections, totOpenDetections, by=c(&quot;Site&quot;,&quot;Date&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(propRF = (totRFDetections)/(totRFDetections + totOCDetections)) %&gt;% mutate(propOC = (1 - propRF)) # write the above results write.csv(prop, &quot;results/acoustic-detections.csv&quot;, row.names = F) 10.4 Testing for differences in species acoustic detections across treatment types Plotting proportion of rainforest and open -country species detections and testing for any significant differences between treatment types # Test if there are significant differences in the proportion of rainforest species across treatment types anovaPropRF &lt;- aov(propRF~Restoration.type, data = prop) tukeyPropRF &lt;- TukeyHSD(anovaPropRF) # Test if there are significant differences in the proportion of open country species across treatment types anovaPropOC &lt;- aov(propOC~Restoration.type, data = prop) tukeyPropOC &lt;- TukeyHSD(anovaPropOC) # The above result suggests that there is a significant difference in the proportion of detections of rainforest and open-country species across treatment types # reordering factors for plotting prop$Restoration.type &lt;- factor(prop$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) fig_propRF &lt;- ggplot(prop, aes(x=Restoration.type, y=propRF, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Proportion of rainforest species detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_propRF, filename = &quot;figs/fig_acousticDetections_propRF.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() fig_propOC &lt;- ggplot(prop, aes(x=Restoration.type, y=propOC, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Proportion of open-country species detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_propOC, filename = &quot;figs/fig_acousticDetections_propOC.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() 10.5 Proportion of detections across acoustic point counts Here, we repeat the above analysis with a different sampling unit, where the total number of detections of a species was estimated for each point count rather than across 10-s chunks. In other words, if a species was detected thrice vocally, it was marked as a single detection in that point count. # Rather than summing the total number of detections across all 10-s clips for every 16-min, here we calculate the proportion of times a species was detected in six visits. # Extract proportion of rainforest species detections across six visits totRainVisits &lt;- nDetections_trait %&gt;% filter(habitat==&quot;RF&quot;) %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totRF = sum(forProp)) totOpenVisits &lt;- nDetections_trait %&gt;% filter(habitat==&quot;OC&quot;) %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totOC = sum(forProp)) # proportion of acoustic detections (note: sampling unit = 1 point count) propVisit &lt;- left_join(totRainVisits, totOpenVisits, by=c(&quot;Site&quot;,&quot;Date&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(propRF = (totRF)/(totRF + totOC)) %&gt;% mutate(propOC = (1 - propRF)) # write the above results to a file write.csv(propVisit, &quot;results/acoustic-detections-across-visits.csv&quot;, row.names = F) 10.6 Testing for differences in rainforest and open-country species detections across acoustic point counts # Test if there are significant differences in the proportion of rainforest species across treatment types anovaRF &lt;- aov(propRF~Restoration.type, data = propVisit) tukeyRF &lt;- TukeyHSD(anovaRF) # Test if there are significant differences in the proportion of rainforest species across treatment types anovaOC &lt;- aov(propOC~Restoration.type, data = propVisit) tukeyOC &lt;- TukeyHSD(anovaOC) # The above result suggests that there is a significant difference in the proportion of detections of rainforest and open-country species across all treatment types at the level of an acoustic point count. # Create a boxplot of proportion estimates by group (Here: group refers to Restoration Type) # reordering factors for plotting propVisit$Restoration.type &lt;- factor(propVisit$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_RF &lt;- ggplot(propVisit, aes(x=Restoration.type, y=propRF, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Proportion of rainforest species detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_RF, filename = &quot;figs/fig_acousticPointCount_propRF.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() fig_OC &lt;- ggplot(propVisit, aes(x=Restoration.type, y=propOC, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Proportion of open-country species detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_OC, filename = &quot;figs/fig_acousticPointCount_propOC.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["non-metric-multidimensional-scaling-of-bird-species-detections.html", "Section 11 Non-metric multidimensional scaling of bird species detections 11.1 Install required libraries 11.2 Load the necessary data for NMDS calculations 11.3 Preparing dataframe of traits and species to be used for NMDS later on 11.4 Preparing a dataframe of detections to run ordinations 11.5 Bray-Curtis dissimilarity index 11.6 Plotting the NMDS scores 11.7 Main text Figure 5 11.8 Testing multivariate homogeneity of group dispersions 11.9 Visualizing the multivariate homogeneity of group dispersions 11.10 Testing compositional dissimilarity between groups 11.11 Separate analysis: Instead of using bird detections, using richness to run nmds ordinations", " Section 11 Non-metric multidimensional scaling of bird species detections Here, we are interested not only in comparing univariate descriptors of communities, like diversity, but also in how the constituent species — or the composition — changes from one community to the next. One tool to do this is non-metric multidimensional scaling, or NMDS. The goal of NMDS is to collapse information from multiple dimensions (e.g, from multiple communities, sites, etc.) into just a few, so that they can be visualized and interpreted. Unlike other ordination techniques that rely on (primarily Euclidean) distances, such as Principal Component Analysis, NMDS uses rank orders, and thus is an extremely flexible technique that can accommodate a variety of different kinds of data (The text above was copied from the link below). NMDS does not use the absolute abundances of species in communities, but rather their rank orders. The use of ranks omits some of the issues associated with using absolute distance (e.g., sensitivity to transformation), and as a result is much more flexible technique that accepts a variety of types of data. (It’s also where the “non-metric” part of the name comes from). A wonderful tutorial is presented in this link: https://jonlefcheck.net/2012/10/24/nmds-tutorial-in-r/ 11.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(ecodist) library(RColorBrewer) library(ggforce) library(ggalt) library(patchwork) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 11.2 Load the necessary data for NMDS calculations # We load the subset data datSubset &lt;- read.csv(&quot;results/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # Site-summary (Number of detections across all sites) datSummary &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # richness across sites (converting detections to 1) richness &lt;- datSummary %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, richness) 11.3 Preparing dataframe of traits and species to be used for NMDS later on # Calculate the overall number of detections for each site. Here, we include dates, since each visit can explain the extrapolation of species richness when jackknife estimates are extracted. nDetections_site_date &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Combine the nDetections and trait based data to obtain a dataframe for further analysis nDetections_trait &lt;- nDetections_site_date %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) 11.4 Preparing a dataframe of detections to run ordinations # We will prepare a community matrix data that can be used to run dissimilarity indices nmdsDat &lt;- nDetections_trait %&gt;% dplyr::select(Site, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Species_Code, Site, Restoration.type) %&gt;% summarise (totDetections = sum(nDetections)) %&gt;% arrange(Restoration.type) %&gt;% pivot_wider (names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) # Convert to matrix form nmdsDatMatrix &lt;- as.matrix(nmdsDat[, 3:ncol(nmdsDat)]) 11.5 Bray-Curtis dissimilarity index Run a bray-curtis dissimilarity index and identify least stressed configuration for the ordinations. Bray-curtis is a statistic used to quantify the compositional dissimilarity between two different sites, based on counts at each site. Ecologists use the Bray-Curtis dissimilarity calculation, which has a number of ideal properties: 1. It is invariant to changes in units. 2. It is unaffected by additions/removals of species that are not present in two communities. 3. It is unaffected by the addition of a new community. 4. It can recognize differences in total abundances when relative abundances are the same. Please note that this link provides more information on NMDS: http://strata.uga.edu/8370/lecturenotes/multidimensionalScaling.html # Run a bray-curtis dissimilarity index and use metaMDS function from vegan to run ordinations disBrayCurtis &lt;- vegdist(nmdsDatMatrix, method = &quot;bray&quot;) nmdsBrayCurtis &lt;- vegdist (nmdsDatMatrix, method = &quot;bray&quot;) %&gt;% metaMDS (nmdsBrayCurtis, k=6) # extract nmds scores nmdsScores &lt;- as.tibble(scores(nmdsBrayCurtis, display = &quot;site&quot;)) # Write the scores to a separate .csv write.csv(nmdsScores, &quot;data/nmdsBrayCurtis-bird-detections.csv&quot;, row.names = F) # With the above analysis, we note the stress is 0.001537865. However, if stress is high, we should reposition the points in 2 dimensions in the direction of decreasing stress, and repeat until stress is below some threshold.**A good rule of thumb: stress &lt; 0.05 provides an excellent representation in reduced dimensions, &lt; 0.1 is great, &lt; 0.2 is good/ok, and stress &lt; 0.3 provides a poor representation.** To reiterate: high stress is bad, low stress is good! 11.6 Plotting the NMDS scores # First let&#39;s add the treatment type back to the nmds scores nmdsScores$Restoration.type &lt;- nmdsDat$Restoration.type # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting nmdsScores$Restoration.type &lt;- factor(nmdsScores$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_nmds &lt;- ggplot(data=nmdsScores) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=5) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScores$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScores$NMDS1), y = max(nmdsScores$NMDS2), label = &quot;Stress = 0.001&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) ggsave(fig_nmds, filename = &quot;figs/fig_nmds_birdDetections.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() AR sites are transitioning toward BM sites 11.7 Main text Figure 5 # reload previous figures to create part A and B for the main text Fig. 4 library(patchwork) fig_prop_ordinations &lt;- wrap_plots(fig_propRF, fig_propOC, fig_nmds, design = &#39;AB CC&#39; ) + plot_annotation( tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot; ) # Expand the width to avoid compression ggsave(fig_prop_ordinations, filename = &quot;figs/fig05.png&quot;, width=15, height=12,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() Proportion of acoustic detections of rainforest and open-country bird species and NMDS ordination results of bird species detections (a) We observed significant differences in the proportion of rainforest bird species detections across each of the three treatment types (Tukey HSD test, P &lt; 0.05). BM sites had the highest proportion of rainforest bird species detections (mean ± SD: 0.97 ± 0.04), followed by AR sites (mean ± SD: 0.81 ± 0.12), and NR sites (mean ± SD: 0.71 ± 0.17). (b) We observed significant differences in the proportion of open-country bird species detections across each of the three treatment type (Tukey HSD test, P &lt; 0.05). NR sites had the highest proportion of open-country bird species detections (mean ± SD: 0.28 ± 0.17), followed by AR sites (mean ± SD: 0.18 ± 0.12), and BM sites (mean ± SD: 0.02 ± 0.04). (b) The ordination analysis of bird detections data (stress = 0.001) revealed distinct clusters of BM sites, but relatively loose clusters for AR and NR sites. However, AR sites occupied an intermediate position between BM and NR sites, indicating a direction of change in bird community composition toward BM sites. In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites and NR = Naturally regenerating forest sites. 11.8 Testing multivariate homogeneity of group dispersions One measure of multivariate dispersion (variance) for a group of samples is to calculate the average distance of group members to the group centroid or spatial median in multivariate space. To test if the dispersions (variances) of one or more groups are different, the distances of group members to the group centroid are subject to ANOVA. Betadisper tests whether two or more groups (for example, restored and unrestored sites) are homogeneously dispersed in relation to their species in studied samples. This test can be done to see if one group has more compositional variance than another. Moreover, homogeneity of dispersion among groups is very advisable to have if you want to test if two or more groups have different compositions, which is tested by adonis. nmdsVariance &lt;- betadisper(disBrayCurtis, group = nmdsDat$Restoration.type) nmdsVariance anova(nmdsVariance) permutest(nmdsVariance, pairwise = TRUE, permutations = 999) TukeyHSD(nmdsVariance) # These results suggest that there is no difference in within-group variance between one group and another 11.9 Visualizing the multivariate homogeneity of group dispersions The below lines of code have been adapted from: https://chrischizinski.github.io/rstats/adonis/ # extract the centroids and the site points in multivariate space. centroids &lt;-data.frame(grps=rownames(nmdsVariance$centroids), data.frame(nmdsVariance$centroids)) vectors &lt;- data.frame(group=nmdsVariance$group, data.frame(nmdsVariance$vectors)) # to create the lines from the centroids to each point we will put it in a format that ggplot can handle seg.data&lt;-cbind(vectors[,1:3],centroids[rep(1:nrow(centroids),as.data.frame(table(vectors$group))$Freq),2:3]) names(seg.data)&lt;-c(&quot;group&quot;,&quot;v.PCoA1&quot;,&quot;v.PCoA2&quot;,&quot;PCoA1&quot;,&quot;PCoA2&quot;) # create the convex hulls of the outermost points grp1.hull &lt;- seg.data[seg.data$group==&quot;Active&quot;,1:3][chull(seg.data[seg.data$group==&quot;Active&quot;,2:3]),] grp2.hull &lt;- seg.data[seg.data$group==&quot;Benchmark&quot;,1:3][chull(seg.data[seg.data$group==&quot;Benchmark&quot;,2:3]),] grp3.hull &lt;- seg.data[seg.data$group==&quot;Passive&quot;,1:3][chull(seg.data[seg.data$group==&quot;Passive&quot;,2:3]),] all.hull &lt;- rbind(grp1.hull,grp2.hull,grp3.hull) # plot the panel and convex hulls fig_hull &lt;- ggplot() + geom_polygon(data= all.hull,aes(x=v.PCoA1,y=v.PCoA2),colour=&quot;black&quot;,alpha=0,linetype=&quot;dashed&quot;) + geom_segment(data=seg.data,aes(x=v.PCoA1,xend=PCoA1,y=v.PCoA2,yend=PCoA2),alpha=0.30) + geom_point(data=centroids[,1:3], aes(x=PCoA1,y=PCoA2,shape=grps),size=4,colour=&quot;red&quot;) + geom_point(data=seg.data, aes(x=v.PCoA1,y=v.PCoA2,shape=group),size=2) + labs(title=&quot;All&quot;,x=&quot;&quot;,y=&quot;&quot;) + #coord_cartesian(xlim = c(-0.2,0.2), ylim = c(-0.25,0.2)) + theme_bw() + theme(legend.position=&quot;none&quot;) 11.10 Testing compositional dissimilarity between groups We will do this by using the vegan::adonis() function which allows you to run permutational multivariate analysis of variance using distance matrices. In the above figure, the NMDS confidence ellipses suggest that there is a significant difference between benchmark and passive-active sites, but no difference between active and passive sites. Adonis works by first finding the centroids for each group and then calculates the squared deviations of each of site to that centroid. Then significance tests are performed using F-tests based on sequential sums of squares from permutations of the raw data. Please note that adonis analyzes and partitions sums of squares using distance matrices. It can be thought of as an ANOVA using distance matrices (analogous to MANOVA - multivariate analysis of variance). Therefore, it is used to test if two or more groups have similar compositions. # We will use the NMDS scores for axis 1 and axis 2 to test for compositional dissimilarity groups &lt;- nmdsScores$Restoration.type adonisNMDS &lt;- adonis2(nmdsDatMatrix ~ groups, method=&quot;bray&quot;,perm=999) adonisNMDS # The results suggest that there are significant compositional differences between groups. 11.11 Separate analysis: Instead of using bird detections, using richness to run nmds ordinations nmdsDatRichness &lt;- datSummary %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) # Convert to matrix form nmdsDatMatrixRichness &lt;- as.matrix(nmdsDatRichness[, 3:ncol(nmdsDatRichness)]) # Run a bray-curtis dissimilarity index and use metaMDS function from vegan to run ordinations disBrayCurtisRichness &lt;- vegdist(nmdsDatMatrixRichness, method = &quot;bray&quot;) nmdsBrayCurtisRichness &lt;- vegdist(nmdsDatMatrixRichness, method = &quot;bray&quot;) %&gt;% metaMDS (nmdsBrayCurtisRichness, k=6) # extract nmds scores nmdsScoresRichness &lt;- as.tibble(scores(nmdsBrayCurtisRichness)) # plot the data # First let&#39;s add the treatment type back to the nmds scores nmdsScoresRichness$Restoration.type &lt;- nmdsDatRichness$Restoration.type # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting nmdsScoresRichness$Restoration.type &lt;- factor(nmdsScoresRichness$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_nmds_richness &lt;- ggplot(data=nmdsScoresRichness) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=4) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScoresRichness$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScoresRichness$NMDS1), y = max(nmdsScoresRichness$NMDS2), label = &quot;Stress = 0.1&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) ggsave(fig_nmds_richness, filename = &quot;figs/fig_nmds_birdRichness.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() Similar results as that of bird species detections were observed. "],["jackknife-estimates.html", "Section 12 Jackknife estimates 12.1 Install required libraries 12.2 Load the necessary data to calculate Jackknife scores 12.3 Preparing dataframe to extract jacknife scores 12.4 Save scores locally 12.5 Looking at correlations between jacknife scores and bootstrap estimates 12.6 Testing for differences between treatment types", " Section 12 Jackknife estimates In this script, we will extract jackknife scores, which essentially extrapolates species richness for a given species pool. This calculation is based on the number of sites and the number of visits to each site and the number of singletons/doubletons (detecting a species only once/site and twice/site respectively). 12.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 12.2 Load the necessary data to calculate Jackknife scores # We load the subset data datSubset &lt;- read.csv(&quot;results/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # Site-summary (Number of detections across all sites) datSummary &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) 12.3 Preparing dataframe to extract jacknife scores # Calculate the overall number of detections for each site across 6 days of data (translates to ~96-min of data per site; each detection corresponding to a temporal unit of 10 seconds). Here, we include dates, since each visit can explain the extrapolation of species richness when jackknife estimates are extracted. nDetections_site_date &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Combine the nDetections and trait based data to obtain a dataframe for jackknife estimates nDetections_trait &lt;- nDetections_site_date %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) # Extract jackknife scores # To do the same, we first prepare the dataframe in a manner where we have a matrix of Site by Date by Species name jacknifeAll &lt;- nDetections_trait %&gt;% dplyr::select(Site, Date, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Site, Date, Restoration.type, Species_Code) %&gt;% summarise(totDetections = sum(nDetections)) %&gt;% pivot_wider(names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) # Prepare a dataframe of rainforest species for jacknifing jacknife_rainForest &lt;- nDetections_trait %&gt;% filter(habitat==&quot;RF&quot;) %&gt;% dplyr::select(Site, Date, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Site, Date, Restoration.type, Species_Code) %&gt;% summarise(totDetections = sum(nDetections)) %&gt;% pivot_wider(names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) # Prepare a dataframe of open-country species for jacknifing jacknife_openCountry &lt;- nDetections_trait %&gt;% filter(habitat==&quot;OC&quot;) %&gt;% dplyr::select(Site, Date, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Site, Date, Restoration.type, Species_Code) %&gt;% summarise(totDetections = sum(nDetections)) %&gt;% pivot_wider(names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) 12.4 Save scores locally jackAllScore &lt;- specpool(jacknifeAll[,4:ncol(jacknifeAll)], pool = jacknifeAll$Site) %&gt;% rownames_to_column(&quot;Site&quot;) %&gt;% add_column (Restoration.type = datSummary$Restoration.type) # write out results write.csv(jackAllScore, &quot;data/jackAll.csv&quot;, row.names=F) jack_rainForestScore &lt;- specpool(jacknife_rainForest[,4:ncol(jacknife_rainForest)], pool = jacknife_rainForest$Site) %&gt;% rownames_to_column(&quot;Site&quot;) %&gt;% add_column (Restoration.type = datSummary$Restoration.type) %&gt;% mutate(Habitat = &quot;RF&quot;) # write out results write.csv(jack_rainForestScore,&quot;data/jackRainforest.csv&quot;, row.names = F) jack_openCountryScore &lt;-specpool(jacknife_openCountry[,4:ncol(jacknife_openCountry)], pool = jacknife_openCountry$Site) %&gt;% rownames_to_column(&quot;Site&quot;) %&gt;% add_column (Restoration.type = datSummary$Restoration.type) %&gt;% mutate(Habitat = &quot;OC&quot;) # write out results write.csv(jack_openCountryScore, &quot;data/jackOpencountry.csv&quot;, row.names = F) 12.5 Looking at correlations between jacknife scores and bootstrap estimates # This plot suggests an almost 1:1 correlation between jacknife estimates and bootstrap scores plot(jackAllScore$jack1, jackAllScore$boot) 12.6 Testing for differences between treatment types Plotting jacknife estimates and testing for any significant differences between treatment types # Test if there are significant differences in jacknife estimates across treatment types anovaJackAll &lt;- aov(jack1~Restoration.type, data = jackAllScore) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukeyJackAll &lt;- TukeyHSD(anovaJackAll) # The above result suggests that there is no significant different in jacknife scores between treatment types # Create a boxplot of jacknife estimates by group (Here: group refers to Restoration Type) # reordering factors for plotting jackAllScore$Restoration.type &lt;- factor(jackAllScore$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) fig_jackAll &lt;- ggplot(jackAllScore, aes(x=Restoration.type, y=jack1, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Jackknife estimates\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_jackAll, filename = &quot;figs/fig_jackAll.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() ## Jacknife scores by species traits Let’s test for significant differences in jacknife estimates as a function of species trait anovaJack_rainForest &lt;- aov(jack1~Restoration.type, data = jack_rainForestScore) anovaJack_openCountry &lt;- aov(jack1~Restoration.type, data = jack_openCountryScore) # Tukey test to study each pair of treatment tukeyJack_rainForest &lt;- TukeyHSD(anovaJack_rainForest) tukeyJack_openCountry &lt;- TukeyHSD(anovaJack_openCountry) # For rainforest species - there is no significant difference in jacknife estimates between any treatment types, while for open-country birds; there is a significant difference in jacknife estimates across active-benchmark and passive-benchmark # Plot the above results jackTrait &lt;- bind_rows(jack_rainForestScore, jack_openCountryScore) # reordering factors for plotting jackTrait$Restoration.type &lt;- factor(jackTrait$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Rainforest species fig_jackTrait &lt;- ggplot(jackTrait, aes(x=Restoration.type, y=jack1, fill=Habitat)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;, labels=c(&quot;Open-country&quot;,&quot;Rainforest&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Jackknife estimates\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) ggsave(fig_jackTrait, filename = &quot;figs/fig_jackTrait.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() # Please note that this figure is required to create the Fig 4a in the next script. No significant differences were observed for rainforest bird species across treatment types but the jacknife estimates of open-country bird species varied between BM-NR and BM-AR site pairs. "],["generalized-linear-mixed-modeling-species-richness-vegetation-data-and-planting-year.html", "Section 13 Generalized linear mixed modeling (species richness, vegetation data and planting year) 13.1 Install required libraries 13.2 Load the necessary data for statistical modeling 13.3 Getting data ready in a format for generalized linear modeling 13.4 Getting data ready for generalized linear mixed modeling with richness data as well as visits. 13.5 Running the generalized linear mixed models 13.6 Testing the role of habitat 13.7 Effect of planting year", " Section 13 Generalized linear mixed modeling (species richness, vegetation data and planting year) In this script, we run generalized linear mixed models to test the association between first order jacknife scores and restoration type. In addition, we run generalized linear mixed models to test associations between species richness and habitat (vegetation structure) using site-pair name (actively restored and naturally regenerating were specified to be paired) and repeat visits as random effects. Lastly, we assess associations between year since restoration began and first order jackknife scores of birds. 13.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 13.2 Load the necessary data for statistical modeling # We load the subset data datSubset &lt;- read.csv(&quot;results/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # richness by Visit # this data basically gives you richness per visit and adds a visit number for each consecutive visit to that site richnessPerVisit &lt;- datSubset %&gt;% group_by(Site, Date, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% dplyr::select(Site, Restoration.type, Date, richness, visit, siteCode) # combine the Detections dataframe with the trait dataset nDetectionsTrait &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) # Load data from previous scripts for use in a GLM vegData &lt;- read.csv(&quot;results/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;results/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) jackAll &lt;- read.csv(&quot;results/jackAll.csv&quot;) jackRainforest &lt;- read.csv(&quot;results/jackRainforest.csv&quot;) jackOpencountry &lt;- read.csv(&quot;results/jackOpencountry.csv&quot;) 13.3 Getting data ready in a format for generalized linear modeling # All birds modelDataAll &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% add_column(jacknife = jackAll$jack1, year = vegData$plantingYear) %&gt;% mutate (&quot;roundjk&quot; = round(jacknife)) # rainforest birds modelData_rainForest &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% add_column(jacknife = jackRainforest$jack1, year = vegData$plantingYear) %&gt;% mutate (&quot;roundjk&quot; = round(jacknife)) # open country birds modelData_openCountry &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% add_column(jacknife = jackOpencountry$jack1, year = vegData$plantingYear) %&gt;% mutate (&quot;roundjk&quot; = round(jacknife)) 13.4 Getting data ready for generalized linear mixed modeling with richness data as well as visits. # data for the GLMM (overall richness) glmmAll &lt;- richnessPerVisit[,-2] %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;)) # rainforest birds richness for glmm glmmRainforest &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habitat == &quot;RF&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelData_rainForest, by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # open-country birds glmmOpencountry &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habitat == &quot;OC&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelData_openCountry, by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # Let&#39;s look at species by foraging habit # canopy birds glmmCanopy &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habit == &quot;CAN&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # ground-feeding birds glmmGround &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habit == &quot;GRD&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # mid-storey birds glmmMidStorey &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habit == &quot;MID&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # understorey birds glmmUnderStory &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habit == &quot;UND&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) 13.5 Running the generalized linear mixed models We now run generalized linear mixed models (GLMM) assuming Poisson errors and using log link functions to examine the effects of restoration type (benchmark, actively restored and passively restored) on the jackknife estimates of bird species richness (for all, rainforest, and open-country species), followed by TukeyHSD multiple comparisons tests of means. # all birds glmm_alljk &lt;- glmer(roundjk ~ Restoration.type +(1|siteCode), data = modelDataAll, family = poisson(link = log)) summary(glmm_alljk) tukey_glmmAllJack &lt;- summary(glht(glmm_alljk, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmAllJack) # The above result suggests that there is a significant difference in first order jacknife estimates for benchmark sites and passively restored site (but no difference between active-passive and active-benchmark). # rainforest birds glmm_rainForestJack &lt;- glmer(roundjk ~ Restoration.type + +(1|siteCode), data = modelData_rainForest, family = poisson(link = log)) summary(glmm_rainForestJack) tukey_glmmRainForestJack &lt;- summary(glht(glmm_rainForestJack, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmRainForestJack) # The above result suggests no significant difference in means between any treatment types # open country birds glmm_openCountryJack &lt;- glmer(roundjk ~ Restoration.type + +(1|siteCode), data = modelData_openCountry, family = poisson(link = log)) summary(glmm_openCountryJack) tukey_glmmOpenCountryJack &lt;- summary(glht(glmm_openCountryJack, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmOpenCountryJack) # For open country birds, there is a significant difference in first-order jacknife estimates between benchmark and passive sites and benchmark and active sites 13.6 Testing the role of habitat Testing the role of habitat (vegetation structure) and foraging habit on species richness within a generalized linear modeling framework # Testing the role of habitat first # all bird species glmm_allBirds &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmAll, family = poisson(link = log)) summary(glmm_allBirds) plot_model(glmm_allBirds, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_allBirds) # significant negative association with PC2 # rainforest birds glmm_Rainforest &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmRainforest, family = poisson(link = log)) summary(glmm_Rainforest) plot_model(glmm_Rainforest, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_Rainforest) # Results above suggest PC2 is significantly negatively associated with richness of rainforest birds. glmm_openCountry &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmOpencountry, family = poisson(link = log)) summary(glmm_openCountry) plot_model(glmm_openCountry, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_openCountry) # statistically significant positive association with PC1 and significant negative association with PC2 # Testing the role of foraging habit # canopy birds (no significant association) glmm_Canopy &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmCanopy, family = poisson(link = log)) summary(glmm_Canopy) # ground-feeding birds (Marginal association between richness of ground-feeding birds and PC2) glmm_Ground &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmGround, family = poisson(link = log)) summary(glmm_Ground) # mid-storey birds (Marginal association between PC1 and richness of mid-storey birds) glmm_MidStorey &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmMidStorey, family = poisson(link = log)) summary(glmm_MidStorey) # understory birds (no significant association) glmm_Understory &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmUnderStory, family = poisson(link = log)) summary(glmm_Understory) 13.7 Effect of planting year Lastly, running a generalized linear model to test the effect of year since restoration on jacknife estimates of species richness # Let&#39;s look at overall species richness first # filter only data for restored sites allRestored &lt;- glmmAll %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-year)) glmmAllRest &lt;- glmer(roundjk ~ yearSinceRestoration + (1|visit), data = allRestored, family = poisson(link = log)) summary(glmmAllRest) plot_model(glmmAllRest, type=&quot;pred&quot;) report::report(glmmAllRest) # no significant association for overall richness # rainforest bird richness rainRestored &lt;- glmmRainforest %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-year)) glmmRain &lt;- glmer(roundjk ~ yearSinceRestoration + (1|visit), data = rainRestored, family = poisson(link = log)) summary(glmmRain) plot_model(glmmRain, type=&quot;pred&quot;) report::report(glmmRain) # no significant association for rainforest bird species richness # open-country richness openRestored &lt;- glmmOpencountry %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-year)) glmmOpen &lt;- glmer(roundjk ~ yearSinceRestoration + (1|visit), data = openRestored, family = poisson(link = log)) summary(glmmOpen) plot_model(glmmOpen, type=&quot;pred&quot;) report::report(glmmOpen) # The effect of yearSinceRestoration is statistically significant and positive for open country birds (beta = 0.02, 95% CI [4.05e-03, 0.04], p = 0.016; Std. beta = 0.06, 95% CI [0.01, 0.11]) "],["generalized-linear-mixed-modeling-species-proportions-vegetation-data-and-planting-year.html", "Section 14 Generalized linear mixed modeling (species proportions, vegetation data and planting year) 14.1 Install required libraries 14.2 Load the necessary data for statistical modeling 14.3 Getting data ready in a format for generalized linear mixed modeling 14.4 Running the generalized linear mixed models 14.5 Examining effect of restoration type on proportion of rainforest and open-country species detections 14.6 Testing the role of habitat 14.7 Effect of planting year", " Section 14 Generalized linear mixed modeling (species proportions, vegetation data and planting year) In this script, we run generalized linear mixed models to test the association between bird species proportions (rainforest and open-country species) and restoration type. In addition, we run generalized linear mixed models to test associations between species proportions and habitat (vegetation structure) using site-pair name (actively restored and naturally regenerating were specified to be paired) and repeat visits as random effects. Lastly, we assess associations between year since restoration began and bird species proportions. 14.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 14.2 Load the necessary data for statistical modeling # Load data from previous scripts vegData &lt;- read.csv(&quot;results/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;results/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) propVisit &lt;- read.csv(&quot;results/acoustic-detections-across-visits.csv&quot;) 14.3 Getting data ready in a format for generalized linear mixed modeling # prep veg data prepVegData &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) # prep bird species proportions prepBirdData &lt;- propVisit %&gt;% group_by(Site, Date, Restoration.type) %&gt;% rowwise() %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # join the above two dataframes modelData &lt;- prepBirdData %&gt;% full_join(prepVegData, by=c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) 14.4 Running the generalized linear mixed models We now run generalized linear mixed models (GLMM) assuming gaussian errors to examine the effects of restoration type (benchmark, actively restored and passively restored) on the proportion of bird species detections (for rainforest and open-country species), followed by TukeyHSD multiple comparisons tests of means. 14.5 Examining effect of restoration type on proportion of rainforest and open-country species detections Rainforest bird species glmm_rainForestProp &lt;- glmer(propRF ~ Restoration.type + +(1|siteCode) + (1|visit), data = modelData, family = gaussian(link=&quot;identity&quot;)) summary(glmm_rainForestProp) tukey_glmmRainForestProp &lt;- summary(glht(glmm_rainForestProp, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmRainForestProp) report::report(glmm_rainForestProp) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict propRF with Restoration.type (formula: propRF ~ Restoration.type). The model included siteCode and visit as random effects (formula: list(~1 | siteCode, ~1 | visit)). The model&#39;s total explanatory power is substantial (conditional R2 = 0.52) and the part related to the fixed effects alone (marginal R2) is of 0.37. The model&#39;s intercept, corresponding to Restoration.type = Active, is at 0.77 (95% CI [0.73, 0.82], t(251) = 35.87, p &lt; .001). Within this model: # - The effect of Restoration type [Benchmark] is statistically significant and positive (beta = 0.14, 95% CI [0.10, 0.17], t(251) = 8.51, p &lt; .001; Std. beta = 1.06, 95% CI [0.82, 1.31]) # - The effect of Restoration type [Passive] is statistically significant and negative (beta = -0.05, 95% CI [-0.07, -0.02], t(251) = -3.19, p = 0.002; Std. beta = -0.36, 95% CI [-0.58, -0.14]) Open-country species glmm_openCountryProp &lt;- glmer(propOC ~ Restoration.type + +(1|siteCode) + (1|visit), data = modelData, family = gaussian(link=&quot;identity&quot;)) summary(glmm_openCountryProp) tukey_glmmOpenCountryProp &lt;- summary(glht(glmm_openCountryProp, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmOpenCountryProp) report::report(glmm_openCountryProp) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict propOC with Restoration.type (formula: propOC ~ Restoration.type). The model included siteCode and visit as random effects (formula: list(~1 | siteCode, ~1 | visit)). The model&#39;s total explanatory power is substantial (conditional R2 = 0.52) and the part related to the fixed effects alone (marginal R2) is of 0.37. The model&#39;s intercept, corresponding to Restoration.type = Active, is at 0.23 (95% CI [0.18, 0.27], t(251) = 10.44, p &lt; .001). Within this model: # - The effect of Restoration type [Benchmark] is statistically significant and negative (beta = -0.14, 95% CI [-0.17, -0.10], t(251) = -8.51, p &lt; .001; Std. beta = -1.06, 95% CI [-1.31, -0.82]) # - The effect of Restoration type [Passive] is statistically significant and positive (beta = 0.05, 95% CI [0.02, 0.07], t(251) = 3.19, p = 0.002; Std. beta = 0.36, 95% CI [0.14, 0.58]) 14.6 Testing the role of habitat Testing the role of habitat (vegetation structure) and on species proportions within a generalized linear modeling framework # rainforest birds glmm_Rainforest &lt;- glmer(propRF ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelData, family = gaussian(link=&quot;identity&quot;)) summary(glmm_Rainforest) plot_model(glmm_Rainforest, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_Rainforest) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict propRF with PC1 and PC2 (formula: propRF ~ PC1 + PC2). The model included siteCode and visit as random effects (formula: list(~1 | siteCode, ~1 | visit)). The model&#39;s total explanatory power is substantial (conditional R2 = 0.48) and the part related to the fixed effects alone (marginal R2) is of 0.20. The model&#39;s intercept, corresponding to PC1 = 0 and PC2 = 0, is at 0.82 (95% CI [0.78, 0.86], t(251) = 38.90, p &lt; .001). Within this model: # - The effect of PC1 is statistically significant and negative (beta = -0.03, 95% CI [-0.04, -0.02], t(251) = -6.51, p &lt; .001; Std. beta = -0.43, 95% CI [-0.55, -0.30]) # - The effect of PC2 is statistically non-significant and positive (beta = 0.01, 95% CI [-2.25e-03, 0.03], t(251) = 1.67, p = 0.096; Std. beta = 0.11, 95% CI [-0.02, 0.24]) # open-country birds glmm_openCountry &lt;- glmer(propOC ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelData, family = gaussian(link=&quot;identity&quot;)) summary(glmm_openCountry) plot_model(glmm_openCountry, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_openCountry) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict propOC with PC1 and PC2 (formula: propOC ~ PC1 + PC2). The model included siteCode and visit as random effects (formula: list(~1 | siteCode, ~1 | visit)). The model&#39;s total explanatory power is substantial (conditional R2 = 0.48) and the part related to the fixed effects alone (marginal R2) is of 0.20. The model&#39;s intercept, corresponding to PC1 = 0 and PC2 = 0, is at 0.18 (95% CI [0.14, 0.22], t(251) = 8.57, p &lt; .001). Within this model: # - The effect of PC1 is statistically significant and positive (beta = 0.03, 95% CI [0.02, 0.04], t(251) = 6.51, p &lt; .001; Std. beta = 0.43, 95% CI [0.30, 0.55]) # - The effect of PC2 is statistically non-significant and negative (beta = -0.01, 95% CI [-0.03, 2.25e-03], t(251) = -1.67, p = 0.096; Std. beta = -0.11, 95% CI [-0.24, 0.02]) 14.7 Effect of planting year Lastly, running a generalized linear model to test the effect of year since restoration on bird species proportions # add planting Year allRestored &lt;- modelData %&gt;% left_join(vegData[,-c(2:9)], by=c(&quot;Site&quot;=&quot;Site_ID&quot;)) # filter only data for restored sites allRestored &lt;- allRestored %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-plantingYear)) # rainforest birds glmmRain &lt;- glmer(propRF ~ yearSinceRestoration + (1|visit), data = allRestored, family = gaussian(link=&quot;identity&quot;)) summary(glmmRain) plot_model(glmmRain, type=&quot;pred&quot;) report::report(glmmRain) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict propRF with yearSinceRestoration (formula: propRF ~ yearSinceRestoration). The model included visit as random effect (formula: ~1 | visit). The model&#39;s total explanatory power is weak (conditional R2 = 0.09) and the part related to the fixed effects alone (marginal R2) is of 2.65e-03. The model&#39;s intercept, corresponding to yearSinceRestoration = 0, is at 0.74 (95% CI [0.60, 0.88], t(80) = 10.79, p &lt; .001). Within this model: # - The effect of yearSinceRestoration is statistically non-significant and positive (beta = 2.11e-03, 95% CI [-6.42e-03, 0.01], t(80) = 0.49, p = 0.624; Std. beta = 0.05, 95% CI [-0.16, 0.26]) # open-country birds glmmOpen &lt;- glmer(propOC ~ yearSinceRestoration + (1|visit), data = allRestored, family = gaussian(link=&quot;identity&quot;)) summary(glmmOpen) plot_model(glmmOpen, type=&quot;pred&quot;) report::report(glmmOpen) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict propOC with yearSinceRestoration (formula: propOC ~ yearSinceRestoration). The model included visit as random effect (formula: ~1 | visit). The model&#39;s total explanatory power is weak (conditional R2 = 0.09) and the part related to the fixed effects alone (marginal R2) is of 2.65e-03. The model&#39;s intercept, corresponding to yearSinceRestoration = 0, is at 0.26 (95% CI [0.12, 0.40], t(80) = 3.79, p &lt; .001). Within this model: # - The effect of yearSinceRestoration is statistically non-significant and negative (beta = -2.11e-03, 95% CI [-0.01, 6.42e-03], t(80) = -0.49, p = 0.624; Std. beta = -0.05, 95% CI [-0.26, 0.16]) "],["generalized-linear-modeling-acoustic-space-use-and-proportion-of-bird-species-detections-and-time-since-restoration.html", "Section 15 Generalized linear modeling (acoustic space use and proportion of bird species detections and time since restoration) 15.1 Install required libraries 15.2 Load necessary data for statistical modeling 15.3 Getting data ready in a format for linear modeling 15.4 Acoustic space use and proportion of bird species detections 15.5 Year since restoration", " Section 15 Generalized linear modeling (acoustic space use and proportion of bird species detections and time since restoration) In this script, we run generalized linear models to test the association between acoustic space use values and species richness, and time since restoration. 15.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(ggpubr) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 15.2 Load necessary data for statistical modeling # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% dplyr::select(&quot;Site.code&quot;,&quot;Restoration.type&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # loading jacknife scores jackAll &lt;- read.csv(&quot;results/jackAll.csv&quot;) jack_rainForest &lt;- read.csv(&quot;results/jackRainforest.csv&quot;) jack_openCountry &lt;- read.csv(&quot;results/jackOpencountry.csv&quot;) # Load vegetation data from previous scripts vegData &lt;- read.csv(&quot;results/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;results/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) # proportion of acoustic detections of rainforest and open-country birds across visits propVisit &lt;- read.csv(&quot;results/acoustic-detections-across-visits.csv&quot;) # convert date column to character for left_join propVisit$Date &lt;- as.character(propVisit$Date) # load the entire asu data across all sites and days computed sitebyDayAsu &lt;- read.csv(&quot;results/site-by-day-asu.csv&quot;) # separate by Site and Date sitebyDayAsu &lt;- separate(sitebyDayAsu, col = Site_Day, into = c(&quot;Site&quot;, &quot;Date&quot;), sep = &quot;_&quot;) # Add restoration type column to the space use data sitebyDayAsu &lt;- left_join(sitebyDayAsu, sites, by=c(&quot;Site&quot;=&quot;Site.code&quot;)) # scale values per site/date for comparison between sites and treatment types sitebyDayAsu &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% mutate(f.cont.scaled = range01(f.cont)) # Let&#39;s look at data by restoration type # This suggests that we have more data for benchmark sites relative to the other two treatment types nDays_siteType &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, Date, Restoration.type) %&gt;% distinct() %&gt;% group_by(Restoration.type) %&gt;% count() # Prepare data for statistical modeling # Calculating total space use across all frequency bins and times of day: 128*24 for each site-day combination totSpaceUse &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(vegData, by=c(&quot;Site&quot;=&quot;Site_ID&quot;)) %&gt;% ungroup() 15.3 Getting data ready in a format for linear modeling # overall space use and bird species detections modelDataAll &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUse, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) %&gt;% full_join(propVisit[,-2], by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% distinct(.) 15.4 Acoustic space use and proportion of bird species detections # rainforest bird species detections glmm_detectionsRF_space &lt;- glmer(roundSpaceuse ~ propRF + (1|siteCode) + (1|visit), data = modelDataAll, family = gaussian(link=&quot;identity&quot;)) summary(glmm_detectionsRF_space) plot_model(glmm_detectionsRF_space, type=&quot;pred&quot;) report::report(glmm_detectionsRF_space) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict roundSpaceuse with propRF (formula: roundSpaceuse ~ propRF). The model included siteCode and visit as random effects (formula: list(~1 | siteCode, ~1 | visit)). The model&#39;s total explanatory power is substantial (conditional R2 = 0.84) and the part related to the fixed effects alone (marginal R2) is of 1.99e-04. The model&#39;s intercept, corresponding to propRF = 0, is at 289.91 (95% CI [233.66, 346.16], t(1215) = 10.11, p &lt; .001). Within this model: # - The effect of propRF is statistically non-significant and positive (beta = 15.96, 95% CI [-18.66, 50.57], t(1215) = 0.90, p = 0.366; Std. beta = 0.01, 95% CI [-0.02, 0.05]) # Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation. # open-country bird species detections glmm_detectionsOC_space &lt;- glmer(roundSpaceuse ~ propOC + (1|siteCode) + (1|visit), data = modelDataAll, family = gaussian(link=&quot;identity&quot;)) summary(glmm_detectionsOC_space) plot_model(glmm_detectionsOC_space, type=&quot;pred&quot;) report::report(glmm_detectionsOC_space) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict roundSpaceuse with propOC (formula: roundSpaceuse ~ propOC). The model included siteCode and visit as random effects (formula: list(~1 | siteCode, ~1 | visit)). The model&#39;s total explanatory power is substantial (conditional R2 = 0.84) and the part related to the fixed effects alone (marginal R2) is of 1.99e-04. The model&#39;s intercept, corresponding to propOC = 0, is at 305.87 (95% CI [257.26, 354.48], t(1215) = 12.35, p &lt; .001). Within this model: # - The effect of propOC is statistically non-significant and negative (beta = -15.96, 95% CI [-50.57, 18.66], t(1215) = -0.90, p = 0.366; Std. beta = -0.01, 95% CI [-0.05, 0.02]) # Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation. 15.5 Year since restoration Let’s look at year since restoration and its effect on acoustic space use and the proportion of rainforest and open country bird species detections # prep dataframe allRestored &lt;- modelDataAll %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-plantingYear)) ## acoustic space use glmmYearSpace &lt;- glmer(totSpaceuse ~ yearSinceRestoration + (1|visit), data = allRestored, family = gaussian(link=&quot;identity&quot;)) summary(glmmYearSpace) plot_model(glmmYearSpace, type=&quot;pred&quot;) report::report(glmmYearSpace) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict totSpaceuse with yearSinceRestoration (formula: totSpaceuse ~ yearSinceRestoration). The model included visit as random effect (formula: ~1 | visit). The model&#39;s total explanatory power is moderate (conditional R2 = 0.16) and the part related to the fixed effects alone (marginal R2) is of 0.12. The model&#39;s intercept, corresponding to yearSinceRestoration = 0, is at 343.91 (95% CI [238.78, 449.04], t(61) = 6.54, p &lt; .001). Within this model: # - The effect of yearSinceRestoration is statistically significant and negative (beta = -10.07, 95% CI [-16.66, -3.47], t(61) = -3.05, p = 0.003; Std. beta = -0.35, 95% CI [-0.58, -0.12]) # Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation. ## rainforest bird species detections glmmYearRF &lt;- glmer(propRF ~ yearSinceRestoration + (1|visit), data = allRestored, family = gaussian(link=&quot;identity&quot;)) summary(glmmYearRF) plot_model(glmmYearRF, type=&quot;pred&quot;) report::report(glmmYearRF) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict propRF with yearSinceRestoration (formula: propRF ~ yearSinceRestoration). The model included visit as random effect (formula: ~1 | visit). The model&#39;s explanatory power related to the fixed effects alone (marginal R2) is 3.67e-09. The model&#39;s intercept, corresponding to yearSinceRestoration = 0, is at 0.77 (95% CI [0.71, 0.83], t(366) = 24.62, p &lt; .001). Within this model: # - The effect of yearSinceRestoration is statistically non-significant and negative (beta = -2.33e-06, 95% CI [-3.94e-03, 3.93e-03], t(366) = -1.16e-03, p &gt; .999; Std. beta = -6.07e-05, 95% CI [-0.10, 0.10]) # Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation. ## open country bird species detections glmmYearOC &lt;- glmer(propOC ~ yearSinceRestoration + (1|visit), data = allRestored, family = gaussian(link=&quot;identity&quot;)) summary(glmmYearOC) plot_model(glmmYearOC, type=&quot;pred&quot;) report::report(glmmYearOC) # We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict propOC with yearSinceRestoration (formula: propOC ~ yearSinceRestoration). The model included visit as random effect (formula: ~1 | visit). The model&#39;s explanatory power related to the fixed effects alone (marginal R2) is 3.67e-09. The model&#39;s intercept, corresponding to yearSinceRestoration = 0, is at 0.23 (95% CI [0.17, 0.29], t(366) = 7.32, p &lt; .001). Within this model: # - The effect of yearSinceRestoration is statistically non-significant and positive (beta = 2.33e-06, 95% CI [-3.93e-03, 3.94e-03], t(366) = 1.16e-03, p &gt; .999; Std. beta = 6.07e-05, 95% CI [-0.10, 0.10]) # Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
