[["index.html", "Source code for Using acoustics to assess the impacts of forest restoration on bird communities in the Western Ghats Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing 1.4 Main Text Figure 1", " Source code for Using acoustics to assess the impacts of forest restoration on bird communities in the Western Ghats Vijay Ramesh Priyanka HariHaran VA Akshay Pooja Choksi Sarika Khanwilkar VV Robin Ruth DeFries 2022-05-11 Section 1 Introduction This is the readable version that showcases analyses carried out to test the impacts of forest restoration on bird communities and other vocalizing biodiversity in the Anamalai hills of the Western Ghats biodiversity hotspot. 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University 1.2 Data access The data used in this work will be archived and made public upon publication. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. 1.4 Main Text Figure 1 This figure was prepared using ArcGIS Pro. Sites corresponding to acoustic recorder deployment locations across the Valparai plateau of the Anamalai hills. The above figure showcases a gradient of forest regeneration across the Valparai plateau. Sites shown in orange represent undisturbed benchmark rainforest sites, sites shown in blue represent actively restored forest sites and sites shown in white represent naturally regenerating forest sites. Ecological restoration is currently being carried out in cooperation with three plantation companies in the Valparai plateau. For more information with respect to the weeding and active restoration protocol, please see methods in Hariharan and Raman (2021) and Osuri et al. (2019). Over the last two decades, the ecological restoration efforts have resulted in restoration of over 100 ha of degraded forests. This map was prepared using 30m resolution SRTM data (Farr et al. 2007) and ESRI satellite imagery is used as a basemap. "],["site-selection.html", "Section 2 Site selection 2.1 Install required libraries 2.2 Load list of sites 2.3 Extract elevation 2.4 Distance to nearest road 2.5 Distance between AR-NR site pairs", " Section 2 Site selection In this script, we plot the elevation, distance to the nearest road, and distance between sites to show that the sites selected are comparable (we also followed site selection criteria outlined by Osuri et al., (2019) and Hariharan and Raman (2021) to ensure that sites are similar in terms of physiognomy and climate). 2.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(data.table) library(extrafont) library(sf) library(raster) # for plotting library(scales) library(ggplot2) library(ggspatial) library(colorspace) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 2.2 Load list of sites # read from local file sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # removed this site as it was sampled in only one season and not across multiple seasons # load a shapefile of the Western Ghats (here, I will use the boundary of the Nilgiris, Anamalais and Palanis) # this file can be used for plotting (if necessary) # hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) # hills &lt;- st_transform(hills, 32643) 2.3 Extract elevation Extract elevation at each site and examine differences in elevation across treatment types # convert to sf object and transform sites &lt;- st_as_sf(sites, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) # add elevation raster alt &lt;- raster(&quot;data/spatial/elevation/alt&quot;) # this layer is not added to github as a result of its large size and can be downloaded from SRTM (Farr et al. (2007)) # extract values from that raster (note: transformation of coordinate system) elev &lt;- extract(alt, sites) sites &lt;- cbind(sites, elev) # Test if there are significant differences in elevation across treatment types anovaElevAll &lt;- aov(elev~Restoration.type, data = sites) # Tukey test to study each pair of treatment - reveals no significant difference across treatment types tukeyElevAll &lt;- TukeyHSD(anovaElevAll) # Create a boxplot of elevation estimates by group (Here: group refers to Restoration Type) # reordering factors for plotting sites$Restoration.type &lt;- factor(sites$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_elevAll &lt;- ggplot(sites, aes(x=Restoration.type, y=elev, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Elevation (in meters)\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) # plot and save elevation ggsave(fig_elevAll, filename = &quot;figs/fig_elev.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() We did not observe any significant difference in elevation across treatment types (Tukey HSD P &gt; 0.05) 2.4 Distance to nearest road Calculate distance to nearest road for each site and examine differences across treatment types. # add roads data roads &lt;- st_read(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) %&gt;% st_transform(32643) # get index of the nearest road to a particular site index &lt;- st_nearest_feature(sites, roads) # calculate distance between each site and the nearest road distToRoad &lt;- as.numeric(st_distance(sites, roads[index,], by_element=TRUE)) sites &lt;- cbind(sites, distToRoad) # Test if there are significant differences in distance to roads across treatment types anovaDistAll &lt;- aov(dist~Restoration.type, data = sites) # Tukey test to study each pair of treatment - no significant difference between AR-NR sites but a significant difference in distance to road between AR-BM and NR-BM sites tukeyDistAll &lt;- TukeyHSD(anovaDistAll) # Create a boxplot of distance to road estimates by group (Here: group refers to Restoration Type) # reordering factors for plotting sites$Restoration.type &lt;- factor(sites$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_distRoad &lt;- ggplot(sites, aes(x=Restoration.type, y=dist, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Distance to nearest road (in meters)\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) # plot and save above figure ggsave(fig_distRoad, filename = &quot;figs/fig_distRoad.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() We did not observe any significant difference in distance to nearest road between AR and NR sites. However, the distance to nearest road for BM sites were significantly different from both AR and NR sites (Tukey HSD P &lt;0.05) 2.5 Distance between AR-NR site pairs Calculate distances between actively restored and naturally regenerating sites that are paired. # Please note that there are only 12 site pairs as two AR sites and one NR site did not have a site-pair because they were too close to another AR/NR site in terms of their acoustic radius. sitePairs &lt;- sites %&gt;% rename(Site_ID = Site.code) %&gt;% mutate(Site.code = str_extract(Site_ID, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% filter(!Restoration.type == &quot;Benchmark&quot;) %&gt;% filter(!Site.code == &quot;THEN10&quot;) %&gt;% filter(!Site.code ==&quot;VAR210&quot;) %&gt;% filter(!Site.code == &quot;INOA03&quot;) AR &lt;- sitePairs[sitePairs$Restoration.type==&quot;Active&quot;,] NR &lt;- sitePairs[sitePairs$Restoration.type==&quot;Passive&quot;,] sitePairDist &lt;- st_distance(AR$geometry, NR$geometry, by_element = T) # The 12 site-pairs were at a minimum distance of 162 m to a maximum distance of 1.1 km. "],["processing-vegetation-data.html", "Section 3 Processing vegetation data 3.1 Install required libraries 3.2 Load the vegetation data 3.3 Process habitat structure variables 3.4 Principal component analysis of vegetation data", " Section 3 Processing vegetation data In this script, we process vegetation data and examine differences in vegetation structure and composition across treatment types. Note: this analysis is provided as supporting information to showcase differences between AR, NR, and BM sites. In addition, we save results of principal component analyses for future statistical models. 3.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 3.2 Load the vegetation data # load the .csv containing different vegetation variables (data collected by the Nature Conservation Foundation) veg &lt;- read.csv(&quot;data/2020-vegetation-data.csv&quot;) veg$Site_ID &lt;- str_remove(veg$Site_ID,&quot;_&quot;) # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # join dataframes to obtain vegetation data for only the required list of sites veg &lt;- right_join(veg,sites, by=c(&quot;Site_ID&quot;=&quot;Site.code&quot;)) # renaming restoration type column veg$Site_type[veg$Site_type==&quot;Unrestored&quot;] &lt;- &quot;Passive&quot; veg$Site_type[veg$Site_type==&quot;Restored&quot;] &lt;- &quot;Active&quot; 3.3 Process habitat structure variables Carrying out exploratory analysis and preparing a dataframe for further steps # Counting number of tree species and unique species per plot treerich &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise (count = n(), richness = n_distinct(tree_species)) # Calculate average tree height across each unique site treeheight &lt;- veg %&gt;% drop_na(height) %&gt;% group_by(Site_ID) %&gt;% summarise(height = mean(height)) # Calculate basal area and left join with other data basal_area &lt;- veg %&gt;% mutate(basal_sum = rowSums(veg[,c(5:15)]^2)/(4*pi)) %&gt;% group_by(Site_ID, Site_type) %&gt;% summarise(basal_area = sum(basal_sum)) # Calculate average canopy height canopy_height &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(canopy_cover = mean(Canopy_cover)) # Calculate average leaf litter leaf_litter &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(leaf_litter = mean(Leaf_litter)) # Calculate average vertical stratification vert_strat &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(vert_strat = mean(Foliage_score)) # Year of planting plantingYear &lt;- veg %&gt;% group_by(Site_ID) %&gt;% summarise(plantingYear = unique(Year.of.planting)) # Creating a final dataframe for further analysis allVeg &lt;- basal_area %&gt;% left_join(treeheight) %&gt;% left_join(treerich) %&gt;% left_join(canopy_height) %&gt;% left_join(leaf_litter) %&gt;% left_join(vert_strat) %&gt;% left_join(plantingYear) write.csv(allVeg, &quot;data/summaryVeg.csv&quot;, row.names = F) 3.4 Principal component analysis of vegetation data # Check for correlations among vegetation predictors pairs.panels(allVeg[,3:9]) # The above panel suggests that richness is highly correlated with a number of predictors including canopy cover, count and basal area. We will calculate a PCA and keep the top two explanatory axes vegPca &lt;- prcomp(allVeg[, 3:9], scale=TRUE, center = TRUE, retx=TRUE) summary(vegPca) # The proportion of variance explained by the first two axes account for ~73.42% # Extract PCA values PCAvalues &lt;- data.frame(&#39;Site_ID&#39;=allVeg$Site_ID, &#39;Site_type&#39; = allVeg$Site_type, vegPca$x[,1:2]) # the first two components are selected # save the data for use in a GLM later write.csv(PCAvalues,&quot;data/pcaVeg.csv&quot;, row.names = F) # Extract loadings of the variables PCAloadings &lt;- data.frame(variables = rownames(vegPca$rotation), vegPca$rotation) # figure below # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting PCAvalues$Site_type &lt;- factor(PCAvalues$Site_type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_pca &lt;- ggplot(PCAvalues, aes(x = PC1, y = PC2, colour = Site_type)) + geom_segment(data = PCAloadings, aes(x = 0, y = 0, xend = (PC1*5), yend = (PC2*5)), arrow = arrow(length = unit(1/2, &quot;picas&quot;)), color = &quot;black&quot;) + geom_point(aes(x=PC1, y=PC2, shape= Site_type, colour = Site_type),size=5) + annotate(&quot;text&quot;, x = (PCAloadings$PC1*3), y = (PCAloadings$PC2*3), label = PCAloadings$variables, family = &quot;Century Gothic&quot;, size=5) + theme_bw() + scale_x_continuous(name=&quot;PC1 (54.67%)&quot;) + scale_y_continuous(name=&quot;PC2 (18.74%)&quot;) + scale_color_manual(&quot;Treatment type&quot;,values = mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;)) + scale_shape_manual(&quot;Treatment type&quot;, values= 1:length(unique(PCAvalues$Site_type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) ggsave(fig_pca, filename = &quot;figs/fig_pca.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["acoustic-space-use.html", "Section 4 Acoustic space use 4.1 Install necessary libraries 4.2 Load recordings across multiple days and sites 4.3 Short-time discrete fourier transform (STDFT) 4.4 Average acoustic space use 4.5 Visual examination of ASU 4.6 Representative figure for publication", " Section 4 Acoustic space use In this study, acoustic space use reflects the amount and pattern of vocalizations within each frequency bin for a given time period. Previous papers calculated acoustic space use by ascertaining the proportion of recordings in a particular time and frequency bin that is above a certain amplitude. However, this proportion was estimated by counting the number of frequency peaks within a given recording (seewave::fpeaks()) and then scaling it to go from 0 to 1. In our analysis, we want to obtain an understanding of the overall acoustic activity for a given time and frequency bin. In other words, we want the area under the curve for a particular frequency contour. Note From Sound Analysis and Synthesis with R by Jerome Sueur The basic premise in calculating ASU is that we compute a Short-time discrete fourier transform for a given frequency bin size (obtained by dividing the sampling frequency/window length over which the fourier transform is computed, for example 48000Hz/256 = 187.5 is the bin size). However, the size of the frequency bin is inversely proportional to time (Uncertainty principle; Pg.312) which would mean a bin size of 187.5 corresponds to a time duration of 0.005s (1/187.5=0.005s). Ultimately what matters is a compromise between frequency resolution and temporal resolution (Fig 11.3; Pg. 313). 4.1 Install necessary libraries library(seewave) library(warbleR) library(tuneR) library(stringi) library(tidyverse) library(ggplot2) library(RColorBrewer) library(foreach) library(doSNOW) library(rlist) library(tictoc) library(patchwork) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) Below, we have summarized approaches taken by previous studies: Aide et al. 2017 aggregated recordings at time scale of hour of day and used a frequency bin size of 86.13 Hz and an amplitude filtering threshold of 0.02. So if the sampling rate is 22000 Hz, that would mean - 22000/86.13 ~ 256 frequency bins to divide up the frequency space. In this paper, there would be 24hr*256 bins = 6144 time/frequency bins. Campos-Cerqueira et al. 2019 aggregated recordings at the time scale of hour of day (24 h), used a frequency bin size of 172 Hz, and an amplitude filtering threshold of 0.003. So if the sampling rate is 22000 Hz, that would mean - 22000/172 ~ 128 frequency bins. This resulted in a threedimensional (x = hour, y = acoustic frequency, z = proportion of all recordings in each time/frequency bin with a frequency peak value &gt; 0.003 amplitude) matrix of acoustic activity with a total of 3,072 time/frequency bins (24 h Ã— 128 frequency bins). Campos-Cerqueira and Aide 2017 used the meanspec (f = 44,100, wl = 256, wn = hanning) and fpeaks (threshold = 0.1, freq = 172) function from the seewave package in R (Sueur et al., 2008a). The value of each peak was normalized using the maximum amplitude value within all recordings in the soundscape (i.e., site), and thus values ranged from 0 to 1. The number of frequency peaks was determined by counting the number of recordings with a peak within each of the 128 frequency bins that were equal or greater than the amplitude threshold. To control for the different number of recordings in each site and each time interval (i.e., hour), we divided the number of recordings with a peak in each time/frequency class by the total number of recordings collected during each hourly interval. 4.2 Load recordings across multiple days and sites First, we examined all unique sites and dates for which we had a continuous 24 hours of recordings. # List the path that contains all folders, which contain the audiomoth data path &lt;- &quot;D:\\\\2020-summer\\\\&quot; # change the path depending on where the raw data is # Listing the folders within which .WAV files are stored folders &lt;- dir(path, recursive=F,full.names=T) ###### Please note ###### # The below set of lines need to be run only once for a given set of sites and days. # Let&#39;s first rename the files by name of each site (as prefix) for(i in 1:length(folders)){ setwd(folders[i]) # List the files within each folder and renaming the files with the prefix - SITE_ID a &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;), full.names = T) file.rename(from = a, to=paste0(basename(folders)[i],&quot;_&quot;,basename(a))) } ###### Ending note here ###### # Now get only those files for a full 24 hours across every unique site files &lt;- list() for(i in 1:length(folders)){ a &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;), full.names = T) site_date &lt;- str_extract(basename(a),&#39;\\\\w+_\\\\d+_&#39;) # Choosing all 24 hours of data across every unique site (288 corresponds to 12 files every 1 hour) for(j in 1:length(unique(site_date))){ dat &lt;- a[str_detect(a,unique(site_date)[j])] if((length(dat)&lt;288)==TRUE){ next } else { files &lt;- c(files, dat) } } } files &lt;- unlist(files) 4.3 Short-time discrete fourier transform (STDFT) To compute an STDFT, we shall begin with an audio recording that has a sampling rate of s = 48,000 Hz and a window length (for the STDFT) wl = 256 samples, which results in a frequency bin size of z = 187.5 Hz (where z = s/wl). Given the sampling rate s, our Nyquist frequency f = 24,000 Hz (where f = s/2), and therefore, the total number of frequency bins across which an STDFT was run is n = 128 frequency bins (n = f/z). The final output of an STDFT corresponds to a matrix of N * m fourier coefficients (N = length of the audio recording and m = number of frequency bins). In this study, we computed STDFT across 24 hours of audio recordings, which translates to a matrix of 3072 (24 hours * 128 bins) fourier coefficients (Campos-Cerqueira et al. 2019). This matrix of coefficients essentially corresponds to ASU (or a measure of space used, for every hour of a day across the 128 bins in this study). Prior to computing STDFTs and ASU values, we set a threshold amplitude of 0.003 dB for audio recordings (following Campos-Cerqueira et al. 2019 and examination of our data). To calculate ASU, we selected five consecutive days of acoustic data (24 continuous hours of data) across each site. Five days was the minimum number of days of consecutive sampling without disruptions across sites (eg. memory card switching/battery replacements/failure of equipment etc.). # Select all unique site_date combinations for each unique site site_date &lt;- str_extract(basename(files),&#39;\\\\w+_\\\\d+_&#39;) unique(site_date) # The only step that involves sub-selection (Selecting 5 days across each site; Note: IPTO06R has 3 days sampled and OLV110R has only 2 days sampled) site_date &lt;- unique(site_date)[c(1:5,10:14,19:23,28:47,52:61,71:75, 86:90,97:101,107:111,118:122,130:134, 141:143,150:154,161:165,173:177,188:189, 193:197,202:206,211:215,219:223,225:229, 231:235,237:241,249:253,261:265,269:273, 281:285,289:293,300:304,309:313,319:323, 329:333,340:344,351:355,361:365,370:374, 378:382,387:391,396:400)] # Create a sequence of numbers to combine files by 1 hour hour_seq &lt;- seq(from=0,to=288, by=12) # To name files with a suffix for each hour time_of_day &lt;- c(&quot;00:00-01:00&quot;,&quot;01:00-02:00&quot;,&quot;02:00-03:00&quot;,&quot;03:00-04:00&quot;, &quot;04:00-05:00&quot;,&quot;05:00-06:00&quot;,&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;, &quot;08:00-09:00&quot;,&quot;09:00-10:00&quot;,&quot;10:00-11:00&quot;,&quot;11:00-12:00&quot;, &quot;12:00-13:00&quot;,&quot;13:00-14:00&quot;,&quot;14:00-15:00&quot;,&quot;15:00-16:00&quot;, &quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;,&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;, &quot;20:00-21:00&quot;,&quot;21:00-22:00&quot;,&quot;22:00-23:00&quot;,&quot;23:00-24:00&quot;) # Loading parameters necessary for the Short-term fourier transform to be performed on hourly aggregates of data for each site_date combination f &lt;- 48000 wl &lt;- 256 # This window length should be changed as a function of the frequency resolution (ie. bin size) and temporal resolution (ie. time) ovlp &lt;- 50 wn &lt;- &quot;hanning&quot; # Store the 24 hour acoustic space use data in a list and name it by a unique site and date site_date_asu &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar( min = 0, max = length(unique(site_date)), style = 3 ) # Select only 24 hours of data (00:00:00 to 23:55:00) for every unique site-date for(i in 1:length(unique(site_date))){ tic(&quot;Total time for a single site-day combination&quot;) # Store the acoustic space use data in a data.frame for plotting and analysis space_use &lt;- data.frame() # Extract the strings first by site dat &lt;- files[stringr::str_detect(files,unique(site_date)[i])] # Parallelize the runs cl &lt;- makeCluster(6, type = &quot;SOCK&quot;) registerDoSNOW(cl) # Store the each hour of data for an entire day as a list here (raw audio files read by tuneR::readWave()) tic(&quot;Reading in .WAV files&quot;) dailydata &lt;- foreach(k=1:length(dat), .combine = &#39;c&#39;, .inorder = T, .packages = &#39;tuneR&#39;) %dopar% { r &lt;- readWave(dat[k]) } toc() gc() stopCluster(cl) # renaming the files to ensure that data is read in hour by hour and stored in a separate object called hourlydata # Below we read the first 12 files in and then save it after computing the STDFT and then repeating it for the next 12 files and so on for(t in 1:(length(hour_seq)-1)) { # Every 12 files correspond to one hour tic(&quot;Computing Short-term fourier transforms for each hour&quot;) if (t == 1) { hourlydata &lt;- dailydata[hour_seq[t]:hour_seq[t+1]] } else { hourlydata &lt;- dailydata[(hour_seq[t]+1):hour_seq[t+1]] } gc() # Parallelize the runs cl &lt;- makeCluster(6, type = &quot;SOCK&quot;) registerDoSNOW(cl) # Store every hour&#39;s ASU data here data_per_hour &lt;- foreach(z = 1:length(hourlydata), .combine = &#39;rbind&#39;, .inorder=T, .packages = &#39;seewave&#39;) %dopar% { wave &lt;- hourlydata[[z]] n &lt;- length(wave) ## Short-term Fourier transform (using a seewave internal function) m &lt;- sspectro(wave, f = f, wl = wl, ovlp = ovlp, wn = wn) # Frequency selection and frequency axis # Here, want only a sequence of numbers that correspond to the length of rows # of the short-time fourier transform and we divide it by 1000 to get values # in kHz freq &lt;- seq(0, (f/2) - (f/wl), length.out = nrow(m))/1000 # Calculate acoustic space use per frequency bin f.cont &lt;- apply(m, MARGIN = 1, FUN = sum) # Store the space use data in a dataframe for plotting later a &lt;- data.frame(freq, f.cont) } rm(hourlydata); gc() data_per_hour &lt;- data_per_hour %&gt;% group_by(freq) %&gt;% summarise(f.cont=sum(f.cont)) data_per_hour$f.cont &lt;- (data_per_hour$f.cont)/12 data_per_hour$time_of_day &lt;- time_of_day[t] space_use &lt;- rbind(data_per_hour, space_use) stopCluster(cl); gc() toc() } space_use &lt;- as.data.frame(space_use) site_date_asu &lt;- c(site_date_asu,list(space_use)) names(site_date_asu)[i] &lt;- unique(site_date)[i] rm(space_use, dailydata, data_per_hour); gc() setTxtProgressBar(pb, i) toc() } close(pb) # store the list object for later list.save(site_date_asu, &quot;data/site_date_asu.rdata&quot;) # Save the list of dataframes for future analysis siteByDayAsu &lt;- bind_rows(site_date_asu, .id = &quot;Site_Day&quot;) # Write to .csv write.csv(siteByDayAsu, &quot;data/site-by-day-asu.csv&quot;, row.names = F) 4.4 Average acoustic space use Now that we have calculated acoustic space use values for a given frequency bin size and time resolution for every unique site-date combination, we would like to obtain a single set of space use values for every unique site. In other words, we would like to average space use values across all frequency bins for five days. # All unique sites site &lt;- str_extract(basename(files),&#39;^([[:alnum:]])+&#39;) unique(site) # Store the site-wise ASU values site_asu &lt;- list() # Loop through site-wise data and average data for each site for(i in 1:length(unique(site))) { # Extract data needed for every unique site dat &lt;- site_date_asu[stringr::str_detect(names(site_date_asu),unique(site)[i])] # Sum up values of acoustic space use across X no. of days for every unique site asu_sum &lt;- dat %&gt;% bind_rows(.id=&quot;data&quot;) %&gt;% group_by(freq, time_of_day) %&gt;% summarize(f.cont.sum=sum(f.cont)) # Averaging data as a function of number of days for a given site over which data was summed asu_sum$f.cont.sum &lt;- (asu_sum$f.cont.sum)/length(dat) # Scaling the data between 0 to 1 for the sake of comparison across sites asu_sum$f.cont.sum &lt;- range01(asu_sum$f.cont.sum) asu_sum &lt;- as.data.frame(asu_sum) site_asu &lt;- c(site_asu, list(asu_sum)) names(site_asu)[i] &lt;- unique(site)[i] rm(dat,asu_sum) } # store the list object for later list.save(site_asu, &quot;data/site_asu.rdata&quot;) # Save the list of dataframes for future analysis siteWiseAsu &lt;- bind_rows(site_asu, .id = &quot;Site&quot;) # Write to .csv write.csv(siteWiseAsu, &quot;data/site-wise-asu.csv&quot;, row.names = F) 4.5 Visual examination of ASU We will use ggplot2 to plot and save individual ASU plots for each unique site-date combination. This will then be repeated for each unique site (calculated in the previous chunk of code) # Saving the ggplots to a folder for every unique site-date combination for(i in 1:length(site_date_asu)){ # Get the data out from a list to a dataframe (else ggplot won&#39;t accept it) dat &lt;- data.frame(site_date_asu[i]) names(dat) &lt;- c(&quot;freq&quot;,&quot;f.cont&quot;,&quot;time_of_day&quot;) # Plot the data g1 &lt;- ggplot(dat, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ # scale_fill_scico(palette = &quot;vikO&quot;) + theme_bw() + labs(x=&quot;Time of Day (in hours)&quot;, y=&quot;Frequency (in kHz) &quot;) + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # Save the data to a folder ggsave(filename=&quot;&quot;,width=12, height=7, units=&quot;in&quot;, dpi = 300, plot=g1, device=&quot;png&quot;, path = paste(&quot;figs/siteByDayAsu/&quot;, paste(names(site_date_asu)[i], &quot;.png&quot;, sep=&quot;&quot;), sep=&quot;&quot;)) } # Saving the ggplots to a folder for every unique site for(j in 1:length(site_asu)){ # Get the data out from a list to a .dataframe (else ggplot won&#39;t accept it) dat &lt;- data.frame(site_asu[j]) names(dat) &lt;- c(&quot;freq&quot;,&quot;time_of_day&quot;,&quot;f.cont.sum&quot;) # Plot the data g1 &lt;- ggplot(dat, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ # scale_fill_scico(palette = &quot;vikO&quot;) + theme_bw() + labs(x=&quot;Time of Day (in hours)&quot;, y=&quot;Frequency (in kHz) &quot;) + theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # Save the data to a folder ggsave(filename=&quot;&quot;,width=12, height=7, units=&quot;in&quot;, dpi = 300, plot=g1, device=&quot;png&quot;, path = paste(&quot;figs/siteWiseAsu/&quot;, paste(names(site_asu)[j], &quot;.png&quot;, sep=&quot;&quot;), sep=&quot;&quot;)) } 4.6 Representative figure for publication # load .rdata to plot select figures for publication site_asu &lt;- load(&quot;data/site_asu.rdata&quot;) site_asu &lt;- bind_rows(site_asu, .id=&quot;Site&quot;) # Selecting three representative sites (PANMP1B, INBS04R, INBS04U) bm &lt;- site_asu %&gt;% filter(Site==&quot;PANMP1B&quot;) ar &lt;- site_asu %&gt;% filter(Site==&quot;INBS04R&quot;) nr &lt;- site_asu %&gt;% filter(Site==&quot;INBS04U&quot;) # create separate figures for benchmark, active and passive sites and then patchwork them # benchmark site fig_bm &lt;- ggplot(bm, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ theme_bw() + labs(fill = &quot;Acoustic Space Use\\n&quot;) + theme(axis.title = element_blank(), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(bm$time_of_day), y = 23, label = &quot;Benchmark site&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5, fontface=&quot;bold&quot;) # actively restored site fig_ar &lt;- ggplot(ar, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ theme_bw() + labs(x=&quot;\\nTime of Day (in hours)&quot;, fill = &quot;Acoustic Space Use\\n&quot;) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), axis.title.y = element_blank(), legend.position = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = max(ar$time_of_day), y = 23, label = &quot;Actively restored site&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5, fontface=&quot;bold&quot;) # naturally regenerating site fig_nr &lt;- ggplot(nr, aes(x=time_of_day, y=freq)) + geom_tile(aes(fill = f.cont.sum)) + scale_fill_gradientn(colours = brewer.pal(9,&quot;Reds&quot;))+ theme_bw() + labs(y=&quot;Frequency (in kHz)\\n&quot;, fill = &quot;Acoustic Space Use\\n&quot;) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), axis.title.x = element_blank(), axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5), legend.position = &quot;none&quot;) + annotate(geom = &quot;text&quot;, x = max(nr$time_of_day), y = 23, label = &quot;Naturally regenerating site&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5, fontface=&quot;bold&quot;) # patchwork the above three for the Fig. 3 fig_asu &lt;- fig_nr + fig_ar + fig_bm ggsave(fig_asu, filename = &quot;figs/fig3.png&quot;, width=52, height=17,device = png(), units=&quot;cm&quot;, dpi = 300); dev.off() Visual examination of acoustic space use across a naturally regenerating, actively restored and benchmark site In this figure, we visually examined ASU for each of the 128 frequency bins and 24 hours across each site and treatment type. Shown here are representative figures for an NR, AR, and BM site. In this visualization, we estimated the proportion of frequency space (values between 0 to 1) that was occupied by vocalizations/sounds above 0.003 dB for every single hour of recording across 24 hours in a day. We observed largely empty frequency bins between 12 kHz to 24 kHz for the majority of AR and NR sites. For the sake of this representative figure, we show the average ASU calculated across five days for each site. However, the patterns described here are broadly consistent across days and sites (Supporting Information). In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites, and NR = Naturally regenerating forest sites. "],["non-metric-multidimensional-scaling-of-acoustic-space-use-data.html", "Section 5 Non-metric multidimensional scaling of acoustic space use data 5.1 Load necessary libraries for analysis 5.2 Load the necessary data for nmds calculations 5.3 Preparing a dataframe of space use to run ordinations 5.4 Plotting the NMDS scores 5.5 Testing multivariate homogeneity of group dispersions 5.6 Visualizing the multivariate homogeneity of group dispersions", " Section 5 Non-metric multidimensional scaling of acoustic space use data Here, we will use ordinations as a method of analyzing how acoustic space use (defined in terms of frequency and time, across 24 hours in a day) varies between treatment types. 5.1 Load necessary libraries for analysis library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(ecodist) library(RColorBrewer) library(ggforce) library(ggalt) library(extrafont) # loadfonts(device = &quot;win&quot;) # run this prior to creating publication figures # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 5.2 Load the necessary data for nmds calculations # load the site-wise space use data sitebyDayAsu &lt;- read.csv(&quot;data/site-by-day-asu.csv&quot;) # separate the columns sitebyDayAsu &lt;- separate(sitebyDayAsu, col = Site_Day, into = c(&quot;Site&quot;,&quot;Day&quot;), sep = &quot;_&quot;) # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% dplyr::select(&quot;Site.code&quot;,&quot;Restoration.type&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # Add restoration type column to the space use data sitebyDayAsu &lt;- left_join(sitebyDayAsu,sites, by=c(&quot;Site&quot;=&quot;Site.code&quot;)) # Separate analysis: space use during certain times of day alone # let&#39;s look only at dawn chorus to explore what the data looks like dawn &lt;- c(&quot;05:00-06:00&quot;,&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;,&quot;08:00-09:00&quot;, &quot;09:00-10:00&quot;) dawnChorusAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% dawn) # Separate analysis: space use during certain times of day alone # let&#39;s look only at dusk chorus to explore what the data looks like dusk &lt;- c(&quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;,&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;) duskChorusAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% dusk) 5.3 Preparing a dataframe of space use to run ordinations # Please note that while creating this dataframe for ordinations, the overall acoustic space use (sum of f.cont.sum column) was calculated by frequency bin, irrespective of the time of day for a given site. In other words, for 24 hours of data, a single value of space use was obtained for a given frequency bin. The frequency bin was pivoted from long to wide and each column essentially corresponded to a frequency bin. Within that frequency bin, grouped for each site, an overall value of space use was computed. nmdsDat &lt;- sitebyDayAsu %&gt;% # replace with dawnChorusAsu/duskChorusAsu dplyr::select(Site, freq, f.cont, Restoration.type) %&gt;% group_by(Site, Restoration.type, freq) %&gt;% summarise(totSpaceuse = sum(f.cont)) %&gt;% arrange(Restoration.type) %&gt;% pivot_wider (names_from = freq, values_from = totSpaceuse, values_fill = list(totSpaceuse=0)) # Convert to matrix form nmdsDatMatrix &lt;- as.matrix(nmdsDat[, 3:ncol(nmdsDat)]) # Run a euclidean dissimilarity distance and use metaMDS function from vegan to run ordinations disEuclidean &lt;- vegdist(nmdsDatMatrix, method = &quot;euclidean&quot;) nmdsEuclidean &lt;- vegdist (nmdsDatMatrix, method = &quot;euclidean&quot;) %&gt;% metaMDS (nmdsEuclidean, k=6) # extract nmds scores nmdsScores &lt;- as_tibble(scores(nmdsEuclidean)) # Write the scores to a separate .csv write.csv(nmdsScores, &quot;data/nmds-acousticSpaceUse.csv&quot;, row.names = F) # With the above analysis, we note the stress is 0.009345172. However, if stress is high, we should reposition the points in 2 dimensions in the direction of decreasing stress, and repeat until stress is below some threshold.**A good rule of thumb: stress &lt; 0.05 provides an excellent representation in reduced dimensions, &lt; 0.1 is great, &lt; 0.2 is good/ok, and stress &lt; 0.3 provides a poor representation.** To reiterate: high stress is bad, low stress is good! 5.4 Plotting the NMDS scores # First let&#39;s add the treatment type back to the nmds scores nmdsScores$Restoration.type &lt;- nmdsDat$Restoration.type # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting nmdsScores$Restoration.type &lt;- factor(nmdsScores$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_nmds &lt;- ggplot(data=nmdsScores) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=5) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScores$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScores$NMDS1), y = max(nmdsScores$NMDS2), label = &quot;Stress = 0.009&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) ggsave(fig_nmds, filename = &quot;figs/fig_nmds_overallSpaceUse.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() 5.5 Testing multivariate homogeneity of group dispersions One measure of multivariate dispersion (variance) for a group of samples is to calculate the average distance of group members to the group centroid or spatial median in multivariate space. To test if the dispersions (variances) of one or more groups are different, the distances of group members to the group centroid are subject to ANOVA. Betadisper tests whether two or more groups (for example, restored and unrestored sites) are homogeneously dispersed in relation to their species in studied samples. This test can be done to see if one group has more compositional variance than another. Moreover, homogeneity of dispersion among groups is very advisable to have if you want to test if two or more groups have different compositions, which is tested by adonis. nmdsVariance &lt;- betadisper(disEuclidean, group = nmdsDat$Restoration.type) nmdsVariance anova(nmdsVariance) permutest(nmdsVariance, pairwise = TRUE, permutations = 999) TukeyHSD(nmdsVariance) # For overall space use, there is a significant difference in within group variance between one group and another (For Benchmark-Active sites and Passive-Benchmark sites, but not between Active-Passive) # This result suggests that there is heterogeneous variation and does not meet assumptions to run adonis 5.6 Visualizing the multivariate homogeneity of group dispersions The below lines of code have been adapted from: https://chrischizinski.github.io/rstats/adonis/ # extract the centroids and the site points in multivariate space. centroids &lt;-data.frame(grps=rownames(nmdsVariance$centroids), data.frame(nmdsVariance$centroids)) vectors &lt;- data.frame(group=nmdsVariance$group, data.frame(nmdsVariance$vectors)) # to create the lines from the centroids to each point we will put it in a format that ggplot can handle seg.data&lt;-cbind(vectors[,1:3],centroids[rep(1:nrow(centroids),as.data.frame(table(vectors$group))$Freq),2:3]) names(seg.data)&lt;-c(&quot;group&quot;,&quot;v.PCoA1&quot;,&quot;v.PCoA2&quot;,&quot;PCoA1&quot;,&quot;PCoA2&quot;) # create the convex hulls of the outermost points grp1.hull &lt;- seg.data[seg.data$group==&quot;Active&quot;,1:3][chull(seg.data[seg.data$group==&quot;Active&quot;,2:3]),] grp2.hull &lt;- seg.data[seg.data$group==&quot;Benchmark&quot;,1:3][chull(seg.data[seg.data$group==&quot;Benchmark&quot;,2:3]),] grp3.hull &lt;- seg.data[seg.data$group==&quot;Passive&quot;,1:3][chull(seg.data[seg.data$group==&quot;Passive&quot;,2:3]),] all.hull &lt;- rbind(grp1.hull,grp2.hull,grp3.hull) # plot the panel and convex hulls fig_hull &lt;- ggplot() + geom_polygon(data= all.hull,aes(x=v.PCoA1,y=v.PCoA2),colour=&quot;black&quot;,alpha=0,linetype=&quot;dashed&quot;) + geom_segment(data=seg.data,aes(x=v.PCoA1,xend=PCoA1,y=v.PCoA2,yend=PCoA2),alpha=0.30) + geom_point(data=centroids[,1:3], aes(x=PCoA1,y=PCoA2,shape=grps),size=4,colour=&quot;red&quot;) + geom_point(data=seg.data, aes(x=v.PCoA1,y=v.PCoA2,shape=group),size=2) + labs(title=&quot;All&quot;,x=&quot;&quot;,y=&quot;&quot;) + #coord_cartesian(xlim = c(-0.2,0.2), ylim = c(-0.25,0.2)) + theme_bw() + theme(legend.position=&quot;none&quot;) "],["generalized-linear-mixed-modeling-acoustic-space-use-and-vegetation-data.html", "Section 6 Generalized linear mixed modeling (acoustic space use and vegetation data) 6.1 Install required libraries 6.2 Load necessary data for statistical modeling 6.3 Getting data ready in a format for linear modeling 6.4 Running the generalized linear mixed models 6.5 Creating figure for publication", " Section 6 Generalized linear mixed modeling (acoustic space use and vegetation data) In this script, we run generalized linear models to test the association between acoustic space use values and restoration type. In addition, we run generalized linear mixed models to test associations between acoustic space use and habitat (vegetation structure) using site-pair name (actively restored and naturally regenerating were specified to be paired) and repeat visits as random effects. 6.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(ggpubr) library(sjPlot) library(RColorBrewer) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 6.2 Load necessary data for statistical modeling # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% dplyr::select(&quot;Site.code&quot;,&quot;Restoration.type&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # load the entire asu data across all sites and days computed sitebyDayAsu &lt;- read.csv(&quot;data/site-by-day-asu.csv&quot;) # separate by Site and Date sitebyDayAsu &lt;- separate(sitebyDayAsu, col = Site_Day, into = c(&quot;Site&quot;, &quot;Date&quot;), sep = &quot;_&quot;) # Add restoration type column to the space use data sitebyDayAsu &lt;- left_join(sitebyDayAsu, sites, by=c(&quot;Site&quot;=&quot;Site.code&quot;)) # scale values per site/date for comparison between sites and treatment types sitebyDayAsu &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% mutate(f.cont.scaled = range01(f.cont)) # computing total number of days for which space use has been computed for each site (some sites have very few days. Eg. OLV110R, else rest have 5 days of audio data each) nSites_Days &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, Date, Restoration.type) %&gt;% distinct() %&gt;% group_by(Site) %&gt;% count() # Let&#39;s look at data by restoration type # This suggests that we have more data for benchmark sites relative to the other two treatment types nDays_siteType &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, Date, Restoration.type) %&gt;% distinct() %&gt;% group_by(Restoration.type) %&gt;% count() # Separate analysis: space use during certain times of day alone # let&#39;s look only at dawn chorus to explore what the data looks like dawn &lt;- c(&quot;05:00-06:00&quot;,&quot;06:00-07:00&quot;,&quot;07:00-08:00&quot;,&quot;08:00-09:00&quot;, &quot;09:00-10:00&quot;) dawnChorusAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% dawn) # Separate analysis: space use during certain times of day alone # let&#39;s look only at dusk chorus to explore what the data looks like dusk &lt;- c(&quot;16:00-17:00&quot;,&quot;17:00-18:00&quot;,&quot;18:00-19:00&quot;,&quot;19:00-20:00&quot;) duskChorusAsu &lt;- sitebyDayAsu %&gt;% filter(time_of_day %in% dusk) # Prepare data for statistical modeling # Calculating total space use across all frequency bins and times of day: 128*24 for each site-day combination totSpaceUse &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # Calculating total space use for dawn chorus alone for each site-day combination totSpaceUseDawn &lt;- dawnChorusAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # Calculating total space use for dusk chorus alone for each site-date combination totSpaceUseDusk &lt;- duskChorusAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) # Load data from previous scripts for use in a GLM vegData &lt;- read.csv(&quot;data/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;data/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) 6.3 Getting data ready in a format for linear modeling # overall space use modelDataAll &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUse, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) # dawn chorus space use modelDataDawn &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUseDawn, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) # dusk chorus space use modelDataDusk &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUseDusk, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) 6.4 Running the generalized linear mixed models We now run generalized linear mixed models (GLMM) assuming gaussian errors to examine the effects of restoration type (benchmark, actively restored and passively restored) on acoustic space use, followed by TukeyHSD multiple comparisons tests of means. # overall space use glmm_allRestoration &lt;- glmer(roundSpaceuse ~ Restoration.type + (1|siteCode) + (1|visit), data = modelDataAll, family = gaussian(link=&quot;identity&quot;)) summary(glmm_allRestoration) tukey_glmmAllRestoration &lt;- summary(glht(glmm_allRestoration, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmAllRestoration) # The above result suggests a significant difference in acoustic space use between BM-AR and BM-NR sites, but no difference between AR and NR sites. # dawn chorus glmm_dawnRestoration &lt;- glmer(roundSpaceuse ~ Restoration.type + (1|siteCode) + (1|visit), data = modelDataDawn, family = gaussian(link=&quot;identity&quot;)) summary(glmm_dawnRestoration) tukey_glmmdawnRestoration &lt;- summary(glht(glmm_dawnRestoration, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmdawnRestoration) # When we look at only the dawn chorus data, there is no significant difference in acoustic space use across all treatment types highlighting similar levels of vocalizing biodiversity. # dusk chorus glmm_duskRestoration &lt;- glmer(roundSpaceuse ~ Restoration.type + (1|siteCode) + (1|visit), data = modelDataDusk, family=gaussian(link=&quot;identity&quot;)) summary(glmm_duskRestoration) tukey_glmmduskRestoration &lt;- summary(glht(glmm_duskRestoration, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmduskRestoration) # For dusk chorus data, the above result suggests a significant difference between BM-AR and BM-NR sites, but no difference between AR and NR sites. # Plotting the above results # reordering factors for plotting modelDataAll$Restoration.type &lt;- factor(modelDataAll$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_glmm_allRest &lt;- ggplot(modelDataAll, aes(Restoration.type, totSpaceuse, fill = Restoration.type)) + geom_boxplot(alpha = 0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Acoustic Space Use\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_glmm_allRest, filename = &quot;figs/fig_glmm_overallAcousticSpace.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() # reordering factors for plotting modelDataDawn$Restoration.type &lt;- factor(modelDataDawn$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_glmm_dawnRest &lt;- ggplot(modelDataDawn, aes(Restoration.type, totSpaceuse, group = Restoration.type, fill = Restoration.type)) + geom_boxplot(alpha = 0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Acoustic Space Use\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_glmm_dawnRest, filename = &quot;figs/fig_glmm_dawnAcousticSpaceUse.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() # reordering factors for plotting modelDataDusk$Restoration.type &lt;- factor(modelDataDusk$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_glmm_duskRest &lt;- ggplot(modelDataDusk, aes(Restoration.type, totSpaceuse, group = Restoration.type, fill = Restoration.type)) + geom_boxplot(alpha = 0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Acoustic Space Use\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_glmm_duskRest, filename = &quot;figs/fig_glmm_duskAcousticSpaceUse.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() Overall acoustic space use across treatment types. Analysis of ~4,128 hours of acoustic data across treatment types (for the 43 sites) revealed significant differences in ASU between treatment types (Tukey HSD P &lt; 0.05). ASU was significantly different across BM-AR, BM-NR and AR-NR sites, with the highest estimate in BM sites (mean Â± SD: 413 Â± 103), followed by AR sites (mean Â± SD: 188 Â± 78) and NR sites (mean Â± SD: 182 Â± 66). In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites and NR = Naturally regenerating forest sites. We now run generalized linear mixed models (GLMM) assuming gaussian errors and using log link functions to examine the effects of habitat (vegetation structure) on acoustic space use. # overall space use glmm_allVeg&lt;- glmer(roundSpaceuse ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelDataAll, family = gaussian(link=&quot;identity&quot;)) summary(glmm_allVeg) plot_model(glmm_allVeg, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_allVeg) # There is a significant negative effect of PC1 on acoustic space use # dawn chorus glmm_dawnVeg&lt;- glmer(roundSpaceuse ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelDataDawn, family = gaussian(link=&quot;identity&quot;)) summary(glmm_dawnVeg) plot_model(glmm_dawnVeg, type=&quot;pred&quot;, terms=c(&quot;PC2&quot;,&quot;PC1&quot;)) report::report(glmm_dawnVeg) # There is a significant negative effect of PC1 on dawn acoustic space use # dawn chorus glmm_duskVeg&lt;- glmer(roundSpaceuse ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = modelDataDusk, family = gaussian(link=&quot;identity&quot;)) summary(glmm_duskVeg) plot_model(glmm_duskVeg, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_duskVeg) # There is a significant negative effect of PC1 on dusk acoustic space use 6.5 Creating figure for publication # Patchworking nmds and glmm for acoustic space use to create part a and b for Fig. 4 library(patchwork) fig_glmm_ordinations &lt;- wrap_plots(fig_glmm_allRest, fig_nmds, design = &quot;AABBBB&quot; ) + plot_annotation( tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot; ) # Expand the width to avoid compression ggsave(fig_glmm_ordinations, filename = &quot;figs/fig04.png&quot;, width=20, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["splitting-large-.wav-files.html", "Section 7 Splitting large .wav files 7.1 Load required libraries", " Section 7 Splitting large .wav files Here, we will first split the raw data which was collected for 24 hours at a site, for 7 days at a stretch. This is being done for the sake of manual annotation of bird species. The deployment schedule of the AudioMoths was set to record for 4-minutes and was switched off for 1-min. For the sake of analysis, data was split into 10s chunks and annotated manually using Raven Pro. 7.1 Load required libraries library(warbleR) library(seewave) library(dplyr) library(stringr) library(tools) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) "],["selecting-the-dawn-chorus.html", "Section 8 Selecting the dawn chorus 8.1 Split the files", " Section 8 Selecting the dawn chorus We will use warbleR::split.wavs() to split a large file. To do so, we will first load a list of .wav files from folders (will have to be done site by site). Next, we we select only files between 6 am and 10 am (this can be varied depending on the exercise or the question at hand). For each day selected, we randomly extracted a continuous 16-min of recording. # List the path that contains all folders, which contain the audiomoth data path &lt;- &quot;C:\\\\data\\\\2020-winter\\\\&quot; # Listing the folders within which .WAV files are stored folders &lt;- dir(path, recursive=F,full.names=T) # Now get only those files that begin at 6am and end at 10am files &lt;- list() for(i in 1:length(folders)){ setwd(folders[i]) # Below code needs to be run only if we have to rename files # List the files within each folder and renaming the files with the prefix - SITE_ID a &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;), full.names = T) file.rename(from = a, to=paste0(basename(folders)[i],&quot;_&quot;,basename(a))) # Extract the strings for .wav files between 6am and 10am time_str &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;),full.names = T) %&gt;% tools::file_path_sans_ext() %&gt;% str_extract(&#39;\\\\d+$&#39;) time_str &lt;- time_str[time_str&gt;=&quot;060000&quot; &amp; time_str &lt;=&quot;100000&quot;] # vary times here depending on the question at hand for(j in 1:length(unique(time_str))){ b &lt;- list.files(paste0(path,basename(folders)[i],&quot;\\\\&quot;),full.names = T, pattern = time_str[j]) files &lt;- c(files,b) } } # These are the list of files we need files &lt;- unlist(files) # Now we choose a random consecutive 16 min of data between 6am and 10am # Get a list of unique dates (since we will be generating a random 16min for every date across every site) site_date &lt;- str_extract(basename(files),&#39;\\\\w+_\\\\d+_&#39;) unique(site_date) # Give you unique date and sites for which we need to generate 16 min of data subset_files &lt;- list() for(i in 1:length(unique(site_date))){ a &lt;- files[str_detect(files,unique(site_date)[i])] if(length(a)&lt;4){ # essentially specifies that the min number you need next } else { subset_dat &lt;- extractRandWindow(a,4) subset_dat &lt;- na.exclude(subset_dat) # If there are less than 4 files subset_files &lt;- c(subset_files, subset_dat) } } final_subset &lt;- unlist(subset_files) # Subset those files and copy it to a separate folder # Please note that these folders &amp; files are locally stored (they are extremely large and cannot be added to GitHub) dir.create(paste0(&quot;C:\\\\data\\\\&quot;,&quot;subset&quot;)) file.copy(from = final_subset, to=&quot;C:\\\\data\\\\subset\\\\&quot;) 8.1 Split the files Split the files and provide unique names to each file # Note: the path you choose to store data is upto the user. subset_path &lt;- &quot;C:\\\\data\\\\subset\\\\&quot; # Split the files into n-second chunks split_wavs(path=subset_path, sgmt.dur = 10, parallel=4) # Get files that need to be renamed split_files &lt;- list.files(subset_path, full.names = T, pattern = &quot;-&quot;) # Note the number of chunks will vary as a function of segment duration # 240 seconds = 24 chunks each of 10s setwd(subset_path) chunks &lt;- c(&quot;01-10&quot;,&quot;10-20&quot;,&quot;20-30&quot;, &quot;30-40&quot;,&quot;40-50&quot;,&quot;50-60&quot;, &quot;60-70&quot;,&quot;70-80&quot;,&quot;80-90&quot;, &quot;90-100&quot;,&quot;100-110&quot;,&quot;110-120&quot;, &quot;120-130&quot;,&quot;130-140&quot;,&quot;140-150&quot;, &quot;150-160&quot;,&quot;160-170&quot;,&quot;170-180&quot;, &quot;180-190&quot;,&quot;190-200&quot;,&quot;200-210&quot;, &quot;210-220&quot;,&quot;220-230&quot;,&quot;230-240&quot;) for(i in 1:length(chunks)){ c &lt;- split_files[endsWith(split_files,paste0(&quot;-&quot;,i,&quot;.wav&quot;))] d &lt;- str_replace(c,paste0(&quot;-&quot;,i),paste0(&quot;_&quot;,chunks[i])) file.rename(from=c, to=d) } # Remove the original files orig_files &lt;- list.files(subset_path, full.names = T, pattern = &quot;.WAV$&quot;) file.remove(orig_files) Now, go ahead and begin the process of manual annotation! "],["bird-species-richness.html", "Section 9 Bird Species Richness 9.1 Install required libraries 9.2 Load manual annotations data 9.3 Subset data 9.4 Total number of detections 9.5 Calculate species richness", " Section 9 Bird Species Richness In this script, we will calculate: Site-wise species richness to understand if species composition across treatment types are distinctly different. Repeat the above calculations, but using species traits - If a species is a rainforest specialist or an open-country generalist. 9.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(data.table) library(extrafont) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 9.2 Load manual annotations data To start with, we will compute species richness from 3 non-consecutive visits for each site for each season. Across two seasons, a total of 6 non-consecutive visits are chosen for each site. We will use an excel sheet of manual annotations, which contains 10s-clips from each site (A random 16-min was chosen between 6am and 10am and divided into 10s chunks for the ease of annotations). # Attach the annotation data for summer and winter summer_data &lt;- read.csv(&quot;data/2020-summer-annotation-working-document.csv&quot;) winter_data &lt;- read.csv(&quot;data/2020-winter-annotation-working-document.csv&quot;) # combine the datasets for a single dataframe for the present analysis data &lt;- bind_rows(summer_data,winter_data) names(data) # reorder columns data &lt;- data %&gt;% relocate(c(&quot;BFO&quot;,&quot;SBEO&quot;, &quot;JN&quot;, &quot;AK&quot;, &quot;HSWP&quot;), .after = &quot;CR&quot;) # Site-wise sorting of the 16-min of data # Split the file names into 4 columns : Site, Date, Time and Splits data &lt;- separate(data, col = Filename, into = c(&quot;Site&quot;, &quot;Date&quot;, &quot;Time&quot;, &quot;Splits&quot;), sep = &quot;_&quot;) data # Load the species-trait-data trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) 9.3 Subset data How many visits (counted as number of unique days) to a site has been annotated? Please note that some sites have more than 6 visits to a site, and hence we ensure that we choose only 6 non-consecutive visits to a given site. # First we remove OLCAP5B - a site for which only 3 visits were made in summer and not sampled as a result of logistic reasons in winter data &lt;- data %&gt;% filter(!str_detect(Site, &#39;OLCAP5B&#39;)) # Number of visits to a particular site # Only INBS04U has 5 visits because another visit was not possible due to rain nSites_Days &lt;- data %&gt;% dplyr::select(Site, Date)%&gt;% distinct() %&gt;% arrange(Site) %&gt;% count(Site) # Unique date site combination to give you a sense of sampling uniqueSiteDate &lt;- data %&gt;% group_by(Site) %&gt;% distinct(Date) # Convert date column to YMD format using lubridate::ymd() uniqueSiteDate$Date &lt;- lubridate::ymd(uniqueSiteDate$Date) # The below lines of code were written following a query on stackOverflow to select six non-consecutive visits to any site # Link: https://stackoverflow.com/questions/67212152/select-non-consecutive-dates-for-every-grouped-element-in-r nonConVisits &lt;- uniqueSiteDate%&gt;% ungroup() %&gt;% group_split(Site) %&gt;% map_df(., ~ .x %&gt;% ungroup() %&gt;% arrange(Date) %&gt;% mutate(n = 1) %&gt;% complete(Date = seq.Date(first(Date), last(Date), by = &#39;days&#39;))%&gt;% group_by(n = cumsum(is.na(n))) %&gt;% filter(!is.na(Site)) %&gt;% filter(row_number() %% 2 == 1) %&gt;% ungroup() %&gt;% sample_n(min(n(), 6)) ) %&gt;% dplyr::select(-n) # Change the structure of the date column back to character for using one of the join functions nonConVisits$Date &lt;- str_remove_all(as.character(nonConVisits$Date),&quot;-&quot;) # Left-join with the original dataframe to subset the data for analysis datSubset &lt;- left_join(nonConVisits,data) names(datSubset) # renaming columns datSubset &lt;- rename(datSubset, Restoration.type = Restoration.Type..Benchmark.Active.Passive.) # Save this data as a .csv for future analysis (later scripts) write.csv(datSubset,&quot;data/datSubset.csv&quot;, row.names = F) 9.4 Total number of detections Now group the data by site and restoration type and sum the number of detections across sites. We will calculate the overall number of detections for each 10s clip, which will be used to estimate species richness below. # Calculate the overall number of detections for each site across 6 days of data (translates to ~ 96 min of data per site) nDetections_Site &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) 9.5 Calculate species richness Convert the detections to 1, since we are interested in calculating richness per site by converting values &gt;1 to 1 for multiple visits to a site. In other words, we want to establish overall species richness for a 16-min to 48-min window. richness &lt;- nDetections_Site %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, richness) # Test if there are significant differences in richness across treatment types anovaAll &lt;- aov(richness~Restoration.type, data = richness) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukeyAll &lt;- TukeyHSD(anovaAll) # The above result suggests that there are no differences in overall species richness across treatment types # Create a boxplot of species richness by Restoration Type # reordering factors for plotting richness$Restoration.type &lt;- factor(richness$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_richness &lt;- ggplot(richness, aes(x=Restoration.type, y=richness, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Species Richness\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_richness, filename = &quot;figs/fig_richness.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300);dev.off() # We observe that the overall species richness is relatively higher in passively restored forest patches, followed by benchmark (protected area) forest patches and actively restored forest patches (But these differences are not significant) ## Species richness by trait Using species trait data to check if species richness varies by treatment type as a function of whether a species is a rainforest specialist vs. open-country specialist. To do so: Add an additional column of species-trait data and group data based on the same. # First, we pivot the species-codes and then match the codes with trait_data and reformat the data to keep all detections&gt;0 as 1 else they are 0 richness_trait &lt;- nDetections_Site %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1, count==0 ~ 0)) # Calculate overall richness for each site as a function of rainforest species and open-country species richness_trait &lt;- richness_trait %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, forRichness)%&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(richness = sum(forRichness)) %&gt;% drop_na() # Let&#39;s subset data for richness as a function of rainforest specialists and open-country generalists and test for significant differences (if any) richness_rainforest &lt;- richness_trait %&gt;% filter(Habitat==&quot;RF&quot;) richness_opencountry &lt;- richness_trait %&gt;% filter(Habitat==&quot;OC&quot;) # Test if there are significant differences in richness across treatment types as a function of species trait anova_rainforest &lt;- aov(richness~Restoration.type, data = richness_rainforest) anova_opencountry &lt;- aov(richness~Restoration.type, data = richness_opencountry) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types for rainforest birds, but a significant difference between benchmark-active and benchmark-passive sites for open country birds tukey_rainforest &lt;- TukeyHSD(anova_rainforest) tukey_opencountry &lt;- TukeyHSD(anova_opencountry) # reordering factors for plotting richness_trait$Restoration.type &lt;- factor(richness_trait$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Plot the above result fig_richness_trait &lt;- ggplot(richness_trait, aes(x=Restoration.type, y=richness, fill=Habitat)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;, labels=c(&quot;Open-country&quot;,&quot;Rainforest&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Species Richness\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) ggsave(fig_richness_trait, filename = &quot;figs/fig_richness_trait.png&quot;, width=12, height=7, device =png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["acoustic-detections.html", "Section 10 Acoustic Detections 10.1 Install required libraries 10.2 Load data 10.3 Overall number of detections 10.4 Detections by treatment type 10.5 Detections by species traits", " Section 10 Acoustic Detections In this script, we will calculate: Total Number of detections across sites (reported for varying time intervals 10s, 30s, 1-min, 2-min, 4-min). Repeat the above calculations, but using species traits - If a species is a rainforest specialist or an open-country generalist. 10.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(extrafont) 10.2 Load data We will use the data that was subset previously for further analysis. # Attach the annotated data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Load the species-trait-data trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) 10.3 Overall number of detections Given the data annotated so far, we will calculate the overall number of detections across different temporal periods, starting from 10s to 30-s, 1-min, 2-min and 4-min. We will first calculate the overall number of detections for the shortest possible temporal duration which could be annotated confidently - 10s. Other durations are chosen to confirm if the number of detections vary as a function of the temporal duration. Please note that: the data at the moment is imbalanced in terms of nVisits # Calculate the overall number of detections for each site where each temporal duration chosen is a 10s clip nDetections_10s &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 30s clip (In this case, every third row is chosen after grouping by Site, Date and Time) nDetections_30s &lt;- datSubset %&gt;% mutate(Splits = case_when((Splits == &quot;01-10&quot; | Splits==&quot;10-20&quot; | Splits ==&quot;20-30&quot;) ~ &quot;1&quot;,(Splits == &quot;30-40&quot; | Splits==&quot;40-50&quot; | Splits ==&quot;50-60&quot;) ~ &quot;2&quot;,(Splits == &quot;60-70&quot; | Splits==&quot;70-80&quot; | Splits ==&quot;80-90&quot;) ~ &quot;3&quot;, (Splits == &quot;90-100&quot; | Splits==&quot;100-110&quot; | Splits ==&quot;110-120&quot;) ~ &quot;4&quot;,(Splits == &quot;120-130&quot; | Splits==&quot;130-140&quot; | Splits ==&quot;140-150&quot;) ~ &quot;5&quot;,(Splits == &quot;150-160&quot; | Splits==&quot;160-170&quot; | Splits ==&quot;170-180&quot;) ~ &quot;6&quot;,(Splits == &quot;180-190&quot; | Splits==&quot;190-200&quot; | Splits ==&quot;200-210&quot;) ~ &quot;7&quot;, (Splits == &quot;210-220&quot; | Splits==&quot;220-230&quot; | Splits ==&quot;230-240&quot;) ~&quot;8&quot;)) %&gt;% group_by(Site, Date, Time, Splits, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections &gt;1 within a 30s period to 1 (since your temporal unit here is 30s) nDetections_30s &lt;- nDetections_30s %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 60s clip (In this case, every sixth row is chosen after grouping by Site, Date and Time) nDetections_1min &lt;- datSubset %&gt;% mutate(Splits = case_when((Splits == &quot;01-10&quot; | Splits==&quot;10-20&quot; | Splits ==&quot;20-30&quot; | Splits == &quot;30-40&quot; | Splits==&quot;40-50&quot; | Splits ==&quot;50-60&quot;) ~ &quot;1&quot;, (Splits == &quot;60-70&quot; | Splits==&quot;70-80&quot; | Splits ==&quot;80-90&quot; | Splits == &quot;90-100&quot; | Splits==&quot;100-110&quot; | Splits ==&quot;110-120&quot;) ~ &quot;2&quot;, (Splits == &quot;120-130&quot; | Splits==&quot;130-140&quot; | Splits ==&quot;140-150&quot; | Splits == &quot;150-160&quot; | Splits==&quot;160-170&quot; | Splits ==&quot;170-180&quot;) ~ &quot;3&quot;, (Splits == &quot;180-190&quot; | Splits==&quot;190-200&quot; | Splits ==&quot;200-210&quot; | Splits == &quot;210-220&quot; | Splits==&quot;220-230&quot; | Splits ==&quot;230-240&quot;) ~&quot;4&quot;)) %&gt;% group_by(Site, Date, Time, Splits, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections&gt;1 within a 1-min period to 1 (since your temporal unit here is 1-min) nDetections_1min &lt;- nDetections_1min %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 120s clip (In this case, every twelfth row is chosen after grouping by Site, Date and Time) nDetections_2min &lt;- datSubset %&gt;% mutate(Splits = case_when((Splits == &quot;01-10&quot; | Splits==&quot;10-20&quot; | Splits ==&quot;20-30&quot; | Splits == &quot;30-40&quot; | Splits==&quot;40-50&quot; | Splits ==&quot;50-60&quot; | Splits == &quot;60-70&quot; | Splits==&quot;70-80&quot; | Splits ==&quot;80-90&quot; | Splits == &quot;90-100&quot; | Splits==&quot;100-110&quot; | Splits ==&quot;110-120&quot;) ~ &quot;1&quot;, (Splits == &quot;120-130&quot; | Splits==&quot;130-140&quot; | Splits ==&quot;140-150&quot; | Splits == &quot;150-160&quot; | Splits==&quot;160-170&quot; | Splits ==&quot;170-180&quot; | Splits == &quot;180-190&quot; | Splits==&quot;190-200&quot; | Splits ==&quot;200-210&quot; | Splits == &quot;210-220&quot; | Splits==&quot;220-230&quot; | Splits ==&quot;230-240&quot;) ~&quot;2&quot;)) %&gt;% group_by(Site, Date, Time, Splits, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections&gt;1 within a 2-min period to 1 (since your temporal unit here is 2-min) nDetections_2min &lt;- nDetections_2min %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Calculate the overall number of detections for each site where each temporal duration chosen is a 240s clip (In this case, every twentyfourth row is chosen after grouping by Site, Date and Time) nDetections_4min &lt;- datSubset %&gt;% group_by(Site, Date, Time, Restoration.type) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Convert nDetections&gt;1 within a 4-min period to 1 (since your temporal unit here is 4-min) nDetections_4min &lt;- nDetections_4min %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) 10.4 Detections by treatment type How does the number of detections vary as a function of restoration type? (tested across different temporal durations)? # Testing if there is significant differences in overall number of detections across treatment types (this has been done only for the smallest temporal duration ~10s, while total number of detections have been estimated for different temporal durations) sum_Detections10s &lt;- nDetections_10s %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) # Test if there are significant differences in detections across treatment types anovaAllDetect &lt;- aov(sumDetections~Restoration.type, data = sum_Detections10s) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukeyAllDetect &lt;- TukeyHSD(anovaAllDetect) # Estimating total number of detections for different temporal durations sum_Detections30s &lt;- nDetections_30s %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) sum_Detections1min &lt;- nDetections_1min %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) sum_Detections2min &lt;- nDetections_2min %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) sum_Detections4min &lt;- nDetections_4min %&gt;% rowwise() %&gt;% mutate(sumDetections = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, sumDetections) # Plotting the above (multiple plots for each temporal duration) # Note: the cumulative number of detections aHSWPoss all species was obtained by summing every 16-min to 48-min set of detections aHSWPoss each site, including all species. # reordering factors for plotting sum_Detections10s$Restoration.type &lt;- factor(sum_Detections10s$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_sumDetections10s &lt;- ggplot(sum_Detections10s, aes(x=Restoration.type, y=sumDetections, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Cumulative number of detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_sumDetections10s, filename = &quot;figs/fig_sumDetections10s.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() 10.5 Detections by species traits How does the cumulative number of detections vary by treatment type, as a function of whether a species is a rainforest specialist or an open country generalist? (These calculations are repeated for different temporal durations to assess differences, if any) # First we merge the species trait dataset with the nDetections dataframe (across different temporal durations) detections_trait10s &lt;- nDetections_10s %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait30s &lt;- nDetections_30s %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait1min &lt;- nDetections_1min %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait2min &lt;- nDetections_2min %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) detections_trait4min &lt;- nDetections_4min %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) # Calculate overall number of detections for each site as a function of rainforest species and open-country species and test for differences across treatment types (Calculated only for the smallest temporal duration - 10s) detections_trait10s &lt;- detections_trait10s %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) %&gt;% drop_na() # Split the above data into rainforest species and open country detections_10s_rainforest &lt;- detections_trait10s %&gt;% filter(Habitat==&quot;RF&quot;) detections_10s_openCountry &lt;- detections_trait10s %&gt;% filter(Habitat==&quot;OC&quot;) # Test if there are significant differences in detections across treatment types as a function of species trait anova_rainforestDet &lt;- aov(sumDetections~Restoration.type, data = detections_10s_rainforest) anova_opencountryDet &lt;- aov(sumDetections~Restoration.type, data = detections_10s_openCountry) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukey_rainforestDet &lt;- TukeyHSD(anova_rainforestDet) tukey_opencountryDet &lt;- TukeyHSD(anova_opencountryDet) # The above results for rainforest birds reveal a significant difference in rainforest bird detections between benchmark sites and active sites and benchmark sites and passive sites. # For open-country birds, there is a significant difference between every pair of treatment type. # Calculating overall number of detections for other temporal durations as a function of species trait detections_trait30s &lt;- detections_trait30s %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) detections_trait1min &lt;- detections_trait1min %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) detections_trait2min &lt;- detections_trait2min %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) detections_trait4min &lt;- detections_trait4min %&gt;% dplyr::select(Site, Restoration.type, Species_Code, Habitat, count) %&gt;% group_by(Site, Restoration.type, Habitat) %&gt;% summarise(sumDetections = sum(count)) # Plot the figures # reordering factors for plotting detections_trait10s$Restoration.type &lt;- factor(detections_trait10s$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_detections_trait10s &lt;- ggplot(detections_trait10s, aes(x=Restoration.type, y=sumDetections, fill=Habitat)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;, labels=c(&quot;Open-country&quot;,&quot;Rainforest&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Cumulative number of detections\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) # Save the above plot ggsave(fig_detections_trait10s, filename = &quot;figs/fig_detections_trait10s.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["jackknife-estimates.html", "Section 11 Jackknife estimates 11.1 Install required libraries 11.2 Load the necessary data to calculate Jackknife scores 11.3 Preparing dataframe to extract jacknife scores 11.4 Save scores locally 11.5 Looking at correlations between jacknife scores and bootstrap estimates 11.6 Testing for differences between treatment types", " Section 11 Jackknife estimates In this script, we will extract jackknife scores, which essentially extrapolates species richness for a given species pool. This calculation is based on the number of sites and the number of visits to each site and the number of singletons/doubletons (detecting a species only once/site and twice/site respectively). 11.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 11.2 Load the necessary data to calculate Jackknife scores # We load the subset data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # Site-summary (Number of detections across all sites) datSummary &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) 11.3 Preparing dataframe to extract jacknife scores # Calculate the overall number of detections for each site across 6 days of data (translates to ~96-min of data per site; each detection corresponding to a temporal unit of 10 seconds). Here, we include dates, since each visit can explain the extrapolation of species richness when jackknife estimates are extracted. nDetections_site_date &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Combine the nDetections and trait based data to obtain a dataframe for jackknife estimates nDetections_trait &lt;- nDetections_site_date %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) # Extract jackknife scores # To do the same, we first prepare the dataframe in a manner where we have a matrix of Site by Date by Species name jacknifeAll &lt;- nDetections_trait %&gt;% dplyr::select(Site, Date, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Site, Date, Restoration.type, Species_Code) %&gt;% summarise(totDetections = sum(nDetections)) %&gt;% pivot_wider(names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) # Prepare a dataframe of rainforest species for jacknifing jacknife_rainForest &lt;- nDetections_trait %&gt;% filter(Habitat==&quot;RF&quot;) %&gt;% dplyr::select(Site, Date, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Site, Date, Restoration.type, Species_Code) %&gt;% summarise(totDetections = sum(nDetections)) %&gt;% pivot_wider(names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) # Prepare a dataframe of open-country species for jacknifing jacknife_openCountry &lt;- nDetections_trait %&gt;% filter(Habitat==&quot;OC&quot;) %&gt;% dplyr::select(Site, Date, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Site, Date, Restoration.type, Species_Code) %&gt;% summarise(totDetections = sum(nDetections)) %&gt;% pivot_wider(names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) 11.4 Save scores locally jackAllScore &lt;- specpool(jacknifeAll[,4:ncol(jacknifeAll)], pool = jacknifeAll$Site) %&gt;% rownames_to_column(&quot;Site&quot;) %&gt;% add_column (Restoration.type = datSummary$Restoration.type) # write out results write.csv(jackAllScore, &quot;data/jackAll.csv&quot;, row.names=F) jack_rainForestScore &lt;- specpool(jacknife_rainForest[,4:ncol(jacknife_rainForest)], pool = jacknife_rainForest$Site) %&gt;% rownames_to_column(&quot;Site&quot;) %&gt;% add_column (Restoration.type = datSummary$Restoration.type) %&gt;% mutate(Habitat = &quot;RF&quot;) # write out results write.csv(jack_rainForestScore,&quot;data/jackRainforest.csv&quot;, row.names = F) jack_openCountryScore &lt;-specpool(jacknife_openCountry[,4:ncol(jacknife_openCountry)], pool = jacknife_openCountry$Site) %&gt;% rownames_to_column(&quot;Site&quot;) %&gt;% add_column (Restoration.type = datSummary$Restoration.type) %&gt;% mutate(Habitat = &quot;OC&quot;) # write out results write.csv(jack_openCountryScore, &quot;data/jackOpencountry.csv&quot;, row.names = F) 11.5 Looking at correlations between jacknife scores and bootstrap estimates # This plot suggests an almost 1:1 correlation between jacknife estimates and bootstrap scores plot(jackAllScore$jack1, jackAllScore$boot) 11.6 Testing for differences between treatment types Plotting jacknife estimates and testing for any significant differences between treatment types # Test if there are significant differences in jacknife estimates across treatment types anovaJackAll &lt;- aov(jack1~Restoration.type, data = jackAllScore) # Tukey test to study each pair of treatment - reveals no signficant difference across treatment types tukeyJackAll &lt;- TukeyHSD(anovaJackAll) # The above result suggests that there is no significant different in jacknife scores between treatment types # Create a boxplot of jacknife estimates by group (Here: group refers to Restoration Type) # reordering factors for plotting jackAllScore$Restoration.type &lt;- factor(jackAllScore$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_jackAll &lt;- ggplot(jackAllScore, aes(x=Restoration.type, y=jack1, fill=Restoration.type)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Jackknife estimates\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family = &quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.position = &quot;none&quot;) ggsave(fig_jackAll, filename = &quot;figs/fig_jackAll.png&quot;, width=12, height=7, device = png(), units=&quot;in&quot;, dpi = 300); dev.off() ## Jacknife scores by species traits Lets test for significant differences in jacknife estimates as a function of species trait anovaJack_rainForest &lt;- aov(jack1~Restoration.type, data = jack_rainForestScore) anovaJack_openCountry &lt;- aov(jack1~Restoration.type, data = jack_openCountryScore) # Tukey test to study each pair of treatment tukeyJack_rainForest &lt;- TukeyHSD(anovaJack_rainForest) tukeyJack_openCountry &lt;- TukeyHSD(anovaJack_openCountry) # For rainforest species - there is no significant difference in jacknife estimates between any treatment types, while for open-country birds; there is a significant difference in jacknife estimates across active-benchmark and passive-benchmark # Plot the above results jackTrait &lt;- bind_rows(jack_rainForestScore, jack_openCountryScore) # reordering factors for plotting jackTrait$Restoration.type &lt;- factor(jackTrait$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) # Rainforest species fig_jackTrait &lt;- ggplot(jackTrait, aes(x=Restoration.type, y=jack1, fill=Habitat)) + geom_boxplot(alpha=0.7) + scale_fill_scico_d(palette = &quot;roma&quot;, labels=c(&quot;Open-country&quot;,&quot;Rainforest&quot;)) + theme_bw() + labs(x=&quot;\\nTreatment Type&quot;, y=&quot;Jackknife estimates\\n&quot;) + scale_x_discrete(labels = c(&#39;BM&#39;,&#39;AR&#39;,&#39;NR&#39;)) + theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;,size = 14), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) ggsave(fig_jackTrait, filename = &quot;figs/fig_jackTrait.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() # Please note that this figure is required to create the Fig 4a in the next script. "],["non-metric-multidimensional-scaling.html", "Section 12 Non-metric multidimensional scaling 12.1 Install required libraries 12.2 Load the necessary data for NMDS calculations 12.3 Preparing dataframe of traits and species to be used for NMDS later on 12.4 Preparing a dataframe of detections to run ordinations 12.5 Bray-Curtis dissimilarity index 12.6 Plotting the NMDS scores 12.7 Main text Figure 2 12.8 Testing multivariate homogeneity of group dispersions 12.9 Testing compositional dissimilarity between groups", " Section 12 Non-metric multidimensional scaling Here, we are interested not only in comparing univariate descriptors of communities, like diversity, but also in how the constituent species  or the composition  changes from one community to the next. One tool to do this is non-metric multidimensional scaling, or NMDS. The goal of NMDS is to collapse information from multiple dimensions (e.g, from multiple communities, sites, etc.) into just a few, so that they can be visualized and interpreted. Unlike other ordination techniques that rely on (primarily Euclidean) distances, such as Principal Component Analysis, NMDS uses rank orders, and thus is an extremely flexible technique that can accommodate a variety of different kinds of data (The text above was copied from the link below). NMDS does not use the absolute abundances of species in communities, but rather their rank orders. The use of ranks omits some of the issues associated with using absolute distance (e.g., sensitivity to transformation), and as a result is much more flexible technique that accepts a variety of types of data. (Its also where the non-metric part of the name comes from). A wonderful tutorial is presented in this link: https://jonlefcheck.net/2012/10/24/nmds-tutorial-in-r/ 12.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(ecodist) library(RColorBrewer) library(ggforce) library(ggalt) library(indicspecies) library(patchwork) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 12.2 Load the necessary data for NMDS calculations # We load the subset data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # Site-summary (Number of detections across all sites) datSummary &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # richness across sites (converting detections to 1) richness &lt;- datSummary %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, richness) 12.3 Preparing dataframe of traits and species to be used for NMDS later on # Calculate the overall number of detections for each site. Here, we include dates, since each visit can explain the extrapolation of species richness when jackknife estimates are extracted. nDetections_site_date &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # Combine the nDetections and trait based data to obtain a dataframe for further analysis nDetections_trait &lt;- nDetections_site_date %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) 12.4 Preparing a dataframe of detections to run ordinations # We will prepare a community matrix data that can be used to run dissimilarity indices nmdsDat &lt;- nDetections_trait %&gt;% dplyr::select(Site, Species_Code, nDetections, Restoration.type) %&gt;% group_by(Species_Code, Site, Restoration.type) %&gt;% summarise (totDetections = sum(nDetections)) %&gt;% arrange(Restoration.type) %&gt;% pivot_wider (names_from = Species_Code, values_from = totDetections, values_fill = list(totDetections=0)) # Convert to matrix form nmdsDatMatrix &lt;- as.matrix(nmdsDat[, 3:ncol(nmdsDat)]) 12.5 Bray-Curtis dissimilarity index Run a bray-curtis dissimilarity index and identify least stressed configuration for the ordinations. Bray-curtis is a statistic used to quantify the compositional dissimilarity between two different sites, based on counts at each site. Ecologists use the Bray-Curtis dissimilarity calculation, which has a number of ideal properties: 1. It is invariant to changes in units. 2. It is unaffected by additions/removals of species that are not present in two communities. 3. It is unaffected by the addition of a new community. 4. It can recognize differences in total abundances when relative abundances are the same. Please note that this link provides more information on NMDS: http://strata.uga.edu/8370/lecturenotes/multidimensionalScaling.html # Run a bray-curtis dissimilarity index and use metaMDS function from vegan to run ordinations disBrayCurtis &lt;- vegdist(nmdsDatMatrix, method = &quot;bray&quot;) nmdsBrayCurtis &lt;- vegdist (nmdsDatMatrix, method = &quot;bray&quot;) %&gt;% metaMDS (nmdsBrayCurtis, k=6) # extract nmds scores nmdsScores &lt;- as.tibble(scores(nmdsBrayCurtis)) # Write the scores to a separate .csv write.csv(nmdsScores, &quot;data/nmdsBrayCurtis-bird-detections.csv&quot;, row.names = F) # With the above analysis, we note the stress is 0.06107898. However, if stress is high, we should reposition the points in 2 dimensions in the direction of decreasing stress, and repeat until stress is below some threshold.**A good rule of thumb: stress &lt; 0.05 provides an excellent representation in reduced dimensions, &lt; 0.1 is great, &lt; 0.2 is good/ok, and stress &lt; 0.3 provides a poor representation.** To reiterate: high stress is bad, low stress is good! 12.6 Plotting the NMDS scores # First let&#39;s add the treatment type back to the nmds scores nmdsScores$Restoration.type &lt;- nmdsDat$Restoration.type # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting nmdsScores$Restoration.type &lt;- factor(nmdsScores$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_nmds &lt;- ggplot(data=nmdsScores) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=5) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScores$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScores$NMDS1), y = max(nmdsScores$NMDS2), label = &quot;Stress = 0.05&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) ggsave(fig_nmds, filename = &quot;figs/fig_nmds_birdDetections.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() 12.7 Main text Figure 2 # reload previous figures to create part A and B for the main text Fig. 2 fig_jack_ordinations &lt;- wrap_plots(fig_jackTrait, fig_nmds, design = &quot;AABBBB&quot; ) + plot_annotation( tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot; ) # Expand the width to avoid compression ggsave(fig_jack_ordinations, filename = &quot;figs/fig2.png&quot;, width=20, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() Jackknife estimates of bird species richness and NMDS ordination results of bird species detections (a) We observed no significant difference in the first-order jackknife scores across the three treatment types when we considered rainforest bird species. The jackknife estimate of open-country bird species varied significantly between BM-AR sites and BM-NR sites (Tukey HSD test, P &lt; 0.05), with NR sites having the highest estimate (mean Â± SD: 24 Â± 5), followed by AR sites (mean Â± SD: 20 Â± 6) and BM sites (mean Â± SD: 15 Â± 4). (b) The ordination analysis of bird detections data (stress = 0.05) revealed distinct clusters of BM sites, but relatively loose clusters for AR and NR sites. However, AR sites occupied an intermediate position between BM and NR sites, indicating a direction of change in bird community composition toward BM sites. In the above figure, BM = undisturbed benchmark rainforest sites, AR = Actively restored forest sites and NR = Naturally regenerating forest sites. 12.8 Testing multivariate homogeneity of group dispersions One measure of multivariate dispersion (variance) for a group of samples is to calculate the average distance of group members to the group centroid or spatial median in multivariate space. To test if the dispersions (variances) of one or more groups are different, the distances of group members to the group centroid are subject to ANOVA. Betadisper tests whether two or more groups (for example, restored and unrestored sites) are homogeneously dispersed in relation to their species in studied samples. This test can be done to see if one group has more compositional variance than another. Moreover, homogeneity of dispersion among groups is very advisable to have if you want to test if two or more groups have different compositions, which is tested by adonis. nmdsVariance &lt;- betadisper(disBrayCurtis, group = nmdsDat$Restoration.type) nmdsVariance anova(nmdsVariance) permutest(nmdsVariance, pairwise = TRUE, permutations = 999) TukeyHSD(nmdsVariance) # These results suggest that there is no difference in within-group variance between one group and another Visualizing the multivariate homogeneity of group dispersions The below lines of code have been adapted from: https://chrischizinski.github.io/rstats/adonis/ # extract the centroids and the site points in multivariate space. centroids &lt;-data.frame(grps=rownames(nmdsVariance$centroids), data.frame(nmdsVariance$centroids)) vectors &lt;- data.frame(group=nmdsVariance$group, data.frame(nmdsVariance$vectors)) # to create the lines from the centroids to each point we will put it in a format that ggplot can handle seg.data&lt;-cbind(vectors[,1:3],centroids[rep(1:nrow(centroids),as.data.frame(table(vectors$group))$Freq),2:3]) names(seg.data)&lt;-c(&quot;group&quot;,&quot;v.PCoA1&quot;,&quot;v.PCoA2&quot;,&quot;PCoA1&quot;,&quot;PCoA2&quot;) # create the convex hulls of the outermost points grp1.hull &lt;- seg.data[seg.data$group==&quot;Active&quot;,1:3][chull(seg.data[seg.data$group==&quot;Active&quot;,2:3]),] grp2.hull &lt;- seg.data[seg.data$group==&quot;Benchmark&quot;,1:3][chull(seg.data[seg.data$group==&quot;Benchmark&quot;,2:3]),] grp3.hull &lt;- seg.data[seg.data$group==&quot;Passive&quot;,1:3][chull(seg.data[seg.data$group==&quot;Passive&quot;,2:3]),] all.hull &lt;- rbind(grp1.hull,grp2.hull,grp3.hull) # plot the panel and convex hulls fig_hull &lt;- ggplot() + geom_polygon(data= all.hull,aes(x=v.PCoA1,y=v.PCoA2),colour=&quot;black&quot;,alpha=0,linetype=&quot;dashed&quot;) + geom_segment(data=seg.data,aes(x=v.PCoA1,xend=PCoA1,y=v.PCoA2,yend=PCoA2),alpha=0.30) + geom_point(data=centroids[,1:3], aes(x=PCoA1,y=PCoA2,shape=grps),size=4,colour=&quot;red&quot;) + geom_point(data=seg.data, aes(x=v.PCoA1,y=v.PCoA2,shape=group),size=2) + labs(title=&quot;All&quot;,x=&quot;&quot;,y=&quot;&quot;) + #coord_cartesian(xlim = c(-0.2,0.2), ylim = c(-0.25,0.2)) + theme_bw() + theme(legend.position=&quot;none&quot;) 12.9 Testing compositional dissimilarity between groups We will do this by using the vegan::adonis() function which allows you to run permutational multivariate analysis of variance using distance matrices. In the above figure, the NMDS confidence ellipses suggest that there is a significant difference between benchmark and passive-active sites, but no difference between active and passive sites. Adonis works by first finding the centroids for each group and then calculates the squared deviations of each of site to that centroid. Then significance tests are performed using F-tests based on sequential sums of squares from permutations of the raw data. Please note that adonis analyzes and partitions sums of squares using distance matrices. It can be thought of as an ANOVA using distance matrices (analogous to MANOVA - multivariate analysis of variance). Therefore, it is used to test if two or more groups have similar compositions. # We will use the NMDS scores for axis 1 and axis 2 to test for compositional dissimilarity groups &lt;- nmdsScores$Restoration.type adonisNMDS &lt;- adonis(nmdsDatMatrix ~ groups, method=&quot;bray&quot;,perm=999) adonisNMDS # The results suggest that there are significant compositional differences between groups. Separate analysis: Instead of using bird detections, using richness to run nmds ordinations nmdsDatRichness &lt;- datSummary %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) # Convert to matrix form nmdsDatMatrixRichness &lt;- as.matrix(nmdsDatRichness[, 3:ncol(nmdsDatRichness)]) # Run a bray-curtis dissimilarity index and use metaMDS function from vegan to run ordinations disBrayCurtisRichness &lt;- vegdist(nmdsDatMatrixRichness, method = &quot;bray&quot;) nmdsBrayCurtisRichness &lt;- vegdist(nmdsDatMatrixRichness, method = &quot;bray&quot;) %&gt;% metaMDS (nmdsBrayCurtisRichness, k=6) # extract nmds scores nmdsScoresRichness &lt;- as.tibble(scores(nmdsBrayCurtisRichness)) # plot the data # First let&#39;s add the treatment type back to the nmds scores nmdsScoresRichness$Restoration.type &lt;- nmdsDatRichness$Restoration.type # Add a custom set of colors mycolors &lt;- c(brewer.pal(name=&quot;Dark2&quot;, n = 3), brewer.pal(name=&quot;Paired&quot;, n = 3)) # reordering factors for plotting nmdsScoresRichness$Restoration.type &lt;- factor(nmdsScoresRichness$Restoration.type, levels = c(&quot;Benchmark&quot;, &quot;Active&quot;, &quot;Passive&quot;)) fig_nmds_richness &lt;- ggplot(data=nmdsScoresRichness) + stat_ellipse(aes(x=NMDS1,y=NMDS2,colour=Restoration.type),level = 0.50) + geom_point(aes(x=NMDS1,y=NMDS2,shape=Restoration.type,colour=Restoration.type),size=4) + theme_bw() + scale_x_continuous(name=&quot;NMDS 1&quot;) + scale_y_continuous(name=&quot;NMDS 2&quot;) + scale_shape_manual(&quot;Treatment type&quot;,values= 1:length(unique(nmdsScoresRichness$Restoration.type)), labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ scale_color_manual(&quot;Treatment type&quot;,values=mycolors, labels=c(&quot;BM&quot;,&quot;AR&quot;,&quot;NR&quot;))+ theme(axis.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), axis.text = element_text(family=&quot;Century Gothic&quot;, size = 12), legend.title = element_text(family=&quot;Century Gothic&quot;, size = 14, face = &quot;bold&quot;), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(family=&quot;Century Gothic&quot;,size = 14)) + annotate(geom = &quot;text&quot;, x = max(nmdsScoresRichness$NMDS1), y = max(nmdsScoresRichness$NMDS2), label = &quot;Stress = 0.1&quot;, hjust = 1, family = &quot;Century Gothic&quot;, size=5) ggsave(fig_nmds_richness, filename = &quot;figs/fig_nmds_birdRichness.png&quot;, width=12, height=7,device = png(), units=&quot;in&quot;, dpi = 300); dev.off() "],["generalized-linear-mixed-modeling-species-richness-vegetation-data-and-planting-year.html", "Section 13 Generalized linear mixed modeling (species richness, vegetation data and planting year) 13.1 Install required libraries 13.2 Load the necessary data for statistical modeling 13.3 Getting data ready in a format for generalized linear modeling 13.4 Getting data ready for generalized linear mixed modeling with richness data as well as visits. 13.5 Running the generalized linear mixed models 13.6 Testing the role of habitat 13.7 Effect of planting year", " Section 13 Generalized linear mixed modeling (species richness, vegetation data and planting year) In this script, we run generalized linear mixed models to test the association between first order jacknife scores and restoration type. In addition, we run generalized linear mixed models to test associations between species richness and habitat (vegetation structure) using site-pair name (actively restored and naturally regenerating were specified to be paired) and repeat visits as random effects. Lastly, we assess associations between year since restoration began and first order jackknife scores of birds. 13.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 13.2 Load the necessary data for statistical modeling # We load the subset data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Load species-trait data to essentially check for associations by habitat type trait_dat &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) # Total number of detections totalDetections &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% summarise(totalDetections = sum(c_across(&quot;IP&quot;:&quot;HSWP&quot;))) # richness richness &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% dplyr::select(Site, Restoration.type, richness) # richness by Visit # this data basically gives you richness per visit and adds a visit number for each consecutive visit to that site richnessPerVisit &lt;- datSubset %&gt;% group_by(Site, Date, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% mutate_at(vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),~ replace(., . &gt; 0, 1)) %&gt;% rowwise() %&gt;% mutate(richness = sum(c_across(IP:HSWP))) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% dplyr::select(Site, Restoration.type, Date, richness, visit, siteCode) # We further estimate the number of detections per point count datSummary &lt;- totalDetections %&gt;% left_join(richness) %&gt;% mutate(pointDetections = totalDetections/3) # combine the Detections dataframe with the trait dataset nDetectionsTrait &lt;- datSubset %&gt;% group_by(Site, Restoration.type, Date) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) %&gt;% pivot_longer(cols=IP:HSWP, names_to=&quot;Species_Code&quot;, values_to=&quot;count&quot;) %&gt;% left_join(.,trait_dat, by=c(&quot;Species_Code&quot;=&quot;species_annotation_codes&quot;)) %&gt;% mutate(forRichness = case_when(count&gt;0 ~ 1,count==0 ~ 0)) %&gt;% rename(., nDetections = count) # Load data from previous scripts for use in a GLM vegData &lt;- read.csv(&quot;data/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;data/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) jackAll &lt;- read.csv(&quot;data/jackAll.csv&quot;) jackRainforest &lt;- read.csv(&quot;data/jackRainforest.csv&quot;) jackOpencountry &lt;- read.csv(&quot;data/jackOpencountry.csv&quot;) 13.3 Getting data ready in a format for generalized linear modeling # All birds modelDataAll &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% add_column(jacknife = jackAll$jack1, year = vegData$plantingYear, richness = richness$richness) %&gt;% mutate (&quot;roundjk&quot; = round(jacknife)) # rainforest birds modelData_rainForest &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% add_column(jacknife = jackRainforest$jack1, year = vegData$plantingYear) %&gt;% mutate (&quot;roundjk&quot; = round(jacknife)) # open country birds modelData_openCountry &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% add_column(jacknife = jackOpencountry$jack1, year = vegData$plantingYear) %&gt;% mutate (&quot;roundjk&quot; = round(jacknife)) 13.4 Getting data ready for generalized linear mixed modeling with richness data as well as visits. # data for the GLMM (overall richness) glmmAll &lt;- richnessPerVisit[,-2] %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;)) # rainforest birds richness for glmm glmmRainforest &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habitat == &quot;RF&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelData_rainForest, by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # open-country birds glmmOpencountry &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habitat == &quot;OC&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelData_openCountry, by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # Let&#39;s look at species by foraging habit # canopy birds glmmCanopy &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habit == &quot;CAN&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # ground-feeding birds glmmGround &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habit == &quot;GRD&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # mid-storey birds glmmMidStorey &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habit == &quot;MID&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) # understorey birds glmmUnderStory &lt;- nDetectionsTrait %&gt;% group_by(Site, Restoration.type, Date) %&gt;% filter (habit == &quot;UND&quot;) %&gt;% summarise(richness = sum(forRichness)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(modelDataAll[,-8], by = c(&quot;Site&quot;,&quot;siteCode&quot;,&quot;Restoration.type&quot;)) 13.5 Running the generalized linear mixed models We now run generalized linear models (GLM) assuming Poisson errors and using log link functions to examine the effects of restoration type (benchmark, actively restored and passively restored) on the jackknife estimates of bird species richness (for all, rainforest, and open-country species), followed by TukeyHSD multiple comparisons tests of means. # all birds glmm_alljk &lt;- glmer(roundjk ~ Restoration.type +(1|siteCode), data = modelDataAll, family = poisson(link = log)) summary(glmm_alljk) tukey_glmmAllJack &lt;- summary(glht(glmm_alljk, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmAllJack) # The above result suggests that there is a significant difference in first order jacknife estimates for benchmark sites and passively restored site (but no difference between active-passive and active-benchmark). # rainforest birds glmm_rainForestJack &lt;- glmer(roundjk ~ Restoration.type + +(1|siteCode), data = modelData_rainForest, family = poisson(link = log)) summary(glmm_rainForestJack) tukey_glmmRainForestJack &lt;- summary(glht(glmm_rainForestJack, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmRainForestJack) # The above result suggests no significant difference in means between any treatment types # open country birds glmm_openCountryJack &lt;- glmer(roundjk ~ Restoration.type + +(1|siteCode), data = modelData_openCountry, family = poisson(link = log)) summary(glmm_openCountryJack) tukey_glmmOpenCountryJack &lt;- summary(glht(glmm_openCountryJack, linfct=mcp(Restoration.type =&quot;Tukey&quot;))) cld(tukey_glmmOpenCountryJack) # For open country birds, there is a significant difference in first-order jacknife estimates between benchmark and passive sites and benchmark and active sites 13.6 Testing the role of habitat Testing the role of habitat (vegetation structure) and foraging habit on species richness within a generalized linear modeling framework # Testing the role of habitat first # all bird species glmm_allBirds &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmAll, family = poisson(link = log)) summary(glmm_allBirds) plot_model(glmm_allBirds, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_allBirds) # significant negative association with PC2 # rainforest birds glmm_Rainforest &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmRainforest, family = poisson(link = log)) summary(glmm_Rainforest) plot_model(glmm_Rainforest, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_Rainforest) # Results above suggest PC2 is significantly negatively associated with richness of rainforest birds. glmm_openCountry &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmOpencountry, family = poisson(link = log)) summary(glmm_openCountry) plot_model(glmm_openCountry, type=&quot;pred&quot;, terms=c(&quot;PC1&quot;,&quot;PC2&quot;)) report::report(glmm_openCountry) # statistically significant positive association with PC1 and significant negative association with PC2 # Testing the role of foraging habit # canopy birds (no significant association) glmm_Canopy &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmCanopy, family = poisson(link = log)) summary(glmm_Canopy) # ground-feeding birds (Marginal association between richness of ground-feeding birds and PC2) glmm_Ground &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmGround, family = poisson(link = log)) summary(glmm_Ground) # mid-storey birds (Marginal association between PC1 and richness of mid-storey birds) glmm_MidStorey &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmMidStorey, family = poisson(link = log)) summary(glmm_MidStorey) # understory birds (no significant association) glmm_Understory &lt;- glmer(richness ~ PC1 + PC2 + (1|siteCode) + (1|visit), data = glmmUnderStory, family = poisson(link = log)) summary(glmm_Understory) 13.7 Effect of planting year Lastly, running a generalized linear model to test the effect of year since restoration on jacknife estimates of species richness # Let&#39;s look at overall species richness first # filter only data for restored sites allRestored &lt;- glmmAll %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-year)) glmmAllRest &lt;- glmer(roundjk ~ yearSinceRestoration + (1|visit), data = allRestored, family = poisson(link = log)) summary(glmmAllRest) plot_model(glmmAllRest, type=&quot;pred&quot;) report::report(glmmAllRest) # no significant association for overall richness # rainforest bird richness rainRestored &lt;- glmmRainforest %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-year)) glmmRain &lt;- glmer(roundjk ~ yearSinceRestoration + (1|visit), data = rainRestored, family = poisson(link = log)) summary(glmmRain) plot_model(glmmRain, type=&quot;pred&quot;) report::report(glmmRain) # no significant association for rainforest bird species richness # open-country richness openRestored &lt;- glmmOpencountry %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-year)) glmmOpen &lt;- glmer(roundjk ~ yearSinceRestoration + (1|visit), data = openRestored, family = poisson(link = log)) summary(glmmOpen) plot_model(glmmOpen, type=&quot;pred&quot;) report::report(glmmOpen) # The effect of yearSinceRestoration is statistically significant and positive for open country birds (beta = 0.02, 95% CI [4.05e-03, 0.04], p = 0.016; Std. beta = 0.06, 95% CI [0.01, 0.11]) "],["generalized-linear-modeling-acoustic-space-use-and-bird-species-richness-and-time-since-restoration.html", "Section 14 Generalized linear modeling (acoustic space use and bird species richness, and time since restoration) 14.1 Install required libraries 14.2 Load necessary data for statistical modeling 14.3 Getting data ready in a format for linear modeling 14.4 Bird species richness 14.5 Year since restoration", " Section 14 Generalized linear modeling (acoustic space use and bird species richness, and time since restoration) In this script, we run generalized linear models to test the association between acoustic space use values and species richness, and time since restoration. 14.1 Install required libraries library(tidyverse) library(dplyr) library(stringr) library(vegan) library(ggplot2) library(scico) library(psych) library(rcompanion) library(multcomp) library(lme4) library(ggpubr) library(sjPlot) # Source any custom/other internal functions necessary for analysis source(&quot;code/01_internal-functions.R&quot;) 14.2 Load necessary data for statistical modeling # load list of sites sites &lt;- read.csv(&quot;data/list-of-sites.csv&quot;) %&gt;% dplyr::select(&quot;Site.code&quot;,&quot;Restoration.type&quot;) %&gt;% filter(Site.code != &quot;OLCAP5B&quot;) # loading jacknife scores jackAll &lt;- read.csv(&quot;data/jackAll.csv&quot;) jack_rainForest &lt;- read.csv(&quot;data/jackRainforest.csv&quot;) jack_openCountry &lt;- read.csv(&quot;data/jackOpencountry.csv&quot;) # Load vegetation data from previous scripts vegData &lt;- read.csv(&quot;data/summaryVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) vegPcaScores &lt;- read.csv(&quot;data/pcaVeg.csv&quot;) %&gt;% filter(!str_detect(Site_ID, &#39;OLCAP5B&#39;)) # Attach the annotation data datSubset &lt;- read.csv(&quot;data/datSubset.csv&quot;) # Calculate the overall number of detections for each site where each temporal duration chosen is a 10s clip nDetections &lt;- datSubset %&gt;% group_by(Site, Restoration.type) %&gt;% transform() %&gt;% replace(is.na(.), 0) %&gt;% summarise_at(.vars = vars(c(&quot;IP&quot;:&quot;HSWP&quot;)),.funs = sum) # total number of detections at each site totDetections &lt;- nDetections %&gt;% rowwise() %&gt;% summarise(totDetections = sum(c_across(IP:HSWP))) # load the entire asu data across all sites and days computed sitebyDayAsu &lt;- read.csv(&quot;data/site-by-day-asu.csv&quot;) # separate by Site and Date sitebyDayAsu &lt;- separate(sitebyDayAsu, col = Site_Day, into = c(&quot;Site&quot;, &quot;Date&quot;), sep = &quot;_&quot;) # Add restoration type column to the space use data sitebyDayAsu &lt;- left_join(sitebyDayAsu, sites, by=c(&quot;Site&quot;=&quot;Site.code&quot;)) # scale values per site/date for comparison between sites and treatment types sitebyDayAsu &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% mutate(f.cont.scaled = range01(f.cont)) # computing total number of days for which space use has been computed for each site (some sites have very few days. Eg. OLV110R, else rest have 5 days of audio data each) nSites_Days &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, Date, Restoration.type) %&gt;% distinct() %&gt;% group_by(Site) %&gt;% count() # Let&#39;s look at data by restoration type # This suggests that we have more data for benchmark sites relative to the other two treatment types nDays_siteType &lt;- sitebyDayAsu %&gt;% dplyr::select(Site, Date, Restoration.type) %&gt;% distinct() %&gt;% group_by(Restoration.type) %&gt;% count() # Prepare data for statistical modeling # Calculating total space use across all frequency bins and times of day: 128*24 for each site-day combination totSpaceUse &lt;- sitebyDayAsu %&gt;% group_by(Site, Date, Restoration.type) %&gt;% summarise(totSpaceuse = sum(f.cont.scaled)) %&gt;% group_by(Site) %&gt;% mutate(visit = row_number()) %&gt;% mutate(siteCode = str_extract(Site, pattern = &quot;\\\\w+\\\\d+&quot;)) %&gt;% mutate(siteCode = factor(siteCode)) %&gt;% full_join(vegData, by=c(&quot;Site&quot;=&quot;Site_ID&quot;)) 14.3 Getting data ready in a format for linear modeling # overall space use modelDataAll &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUse, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) %&gt;% full_join(jackAll, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate (&quot;roundjk&quot; = round(jack1)) %&gt;% full_join(vegData, by=c(&quot;Site&quot;=&quot;Site_ID&quot;)) %&gt;% full_join(totDetections, by=c(&quot;Site&quot;=&quot;Site&quot;)) # overall space use and rainforest birds modelDataRain &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUse, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) %&gt;% full_join(jack_rainForest, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate (&quot;roundjk&quot; = round(jack1)) %&gt;% full_join(vegData, by=c(&quot;Site&quot;=&quot;Site_ID&quot;)) # overall space use and open country birds modelDataOpen &lt;- vegPcaScores %&gt;% rename(Site = Site_ID) %&gt;% rename(Restoration.type = Site_type) %&gt;% mutate(across(Restoration.type, factor)) %&gt;% full_join(totSpaceUse, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate(&quot;roundSpaceuse&quot; = round(totSpaceuse)) %&gt;% full_join(jack_openCountry, by=c(&quot;Site&quot;,&quot;Restoration.type&quot;)) %&gt;% mutate (&quot;roundjk&quot; = round(jack1)) %&gt;% full_join(vegData, by=c(&quot;Site&quot;=&quot;Site_ID&quot;)) 14.4 Bird species richness Lets first explore richness (as measured through first order jacknife scores and test for associations with space use) # running generalized linear models glmm_overallRichnessSpace &lt;- glmer(roundSpaceuse ~ roundjk + (1|siteCode) + (1|visit), data = modelDataAll, family = gaussian(link=&quot;identity&quot;)) summary(glmm_overallRichnessSpace) plot_model(glmm_overallRichnessSpace, type=&quot;pred&quot;) report::report(glmm_overallRichnessSpace) # The results suggest a statistically non-significant and postive association between overall space use and first-order jacknife scores # Rainforest species glmm_RainRichnessSpace &lt;- glmer(roundSpaceuse ~ roundjk + (1|siteCode) + (1|visit), data = modelDataRain, family = gaussian(link=&quot;identity&quot;)) summary(glmm_RainRichnessSpace) plot_model(glmm_RainRichnessSpace, type=&quot;pred&quot;) report::report(glmm_RainRichnessSpace) # for rainforest species, The effect of roundjk is statistically significant and positive (beta = 5.30, 95% CI [1.98, 8.62], t(205) = 3.15, p = 0.002; Std. beta = 0.24, 95% CI [0.09, 0.40]) # open country species glmm_OpenRichnessSpace &lt;- glmer(roundSpaceuse ~ roundjk + (1|siteCode) + (1|visit), data = modelDataOpen, family = gaussian(link=&quot;identity&quot;)) summary(glmm_OpenRichnessSpace) plot_model(glmm_OpenRichnessSpace, type=&quot;pred&quot;) report::report(glmm_OpenRichnessSpace) # For open country birds, The effect of roundjk is statistically non-significant and negative (beta = -1.56, 95% CI [-4.29, 1.18], t(205) = -1.12, p = 0.263; Std. beta = -0.07, 95% CI [-0.19, 0.05]) 14.5 Year since restoration Lets look at year since restoration and ask if space use has a significant association with time allRestored &lt;- totSpaceUse %&gt;% filter(Restoration.type==&quot;Active&quot;) %&gt;% mutate(yearSinceRestoration = (2022-plantingYear)) glmmAllYear &lt;- glmer(totSpaceuse ~ yearSinceRestoration + (1|visit), data = allRestored, family = gaussian(link=&quot;identity&quot;)) summary(glmmAllYear) plot_model(glmmAllYear, type=&quot;pred&quot;) report::report(glmmAllYear) # The effect of yearSinceRestoration is statistically significant and negative (beta = -10.07, 95% CI [-16.66, -3.47], t(61) = -3.05, p = 0.003; Std. beta = -0.35, 95% CI [-0.58, -0.12]) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
