---
editor_options: 
  chunk_output_type: console
---

Install necessary libraries
```{r}
library(seewave)
library(warbleR)
library(tuneR)
library(stringi)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
library(foreach)
library(doParallel)
library(doSNOW)

# Source any custom/other internal functions necessary for analysis
source("code/01_internal-functions.R")
```

Acoustic Space Use (ASU)

We are interested in creating a measure (3-dimensional matrix) of acoustic activity (x=hour, y=acoustic frequencies, z=proportion of all recordings in each time/frequency bin)

Previous papers broadly calculated space use by ascertaining the proportion of recordings in a particular time and frequency bin that is above a certain amplitude. However, this proportion was estimated by counting the number of frequency peaks within a given recording (seewave::fpeaks()) and then scaling it to go from 0 to 1. In our analysis, we want to obtain an understanding of the overall acoustic activity for a given time and frequency bin. In other words, we want the area under the curve for a particular frequency contour. 

*Note* From Sound Analysis and Synthesis with R by Jerome Sueur

The basic premise in calculating ASU is that we compute a Short-time discrete fourier transform for a given frequency bin size (obtained by dividing the sampling frequency/window length over which the fourier transform is computed, for example 48000Hz/512 = 93.75 is the bin size). However, the size of the frequency bin is inversely proportional to time (Uncertainty principle; Pg.312) which would mean a bin size of 93.75 corresponds to a time duration of 0.01s (1/93.75=0.01s). Ultimately what matters is a compromise between frequency resolution and temporal resolution (Fig 11.3; Pg. 313) 

Below, we have summarized approaches taken by previous studies: 

Aide et al. 2017: We aggregated recordings at time scale of hour of day and used a frequency bin size of 86.13 Hz and an amplitude filtering threshold of 0.02. So if the sampling rate is 22000 Hz, that would mean - 22000/86.13 ~ 256 frequency bins to divide up the frequency space. In this paper, there would be 24hr*256 bins = 6144 time/frequency bins

Campos-Cerqueira et al. 2019: We aggregated recordings at the time scale of hour of day (24 h), used a frequency bin size of 172 Hz, and an amplitude filtering threshold of 0.003. So if the sampling rate is 22000 Hz, that would mean - 22000/172 ~ 128 frequency bins. This resulted in a three‐dimensional (x = hour, y = acoustic frequency, z = proportion of all recordings in each time/frequency bin with a frequency peak value > 0.003 amplitude) matrix of acoustic activity with a total of 3,072 time/frequency bins (24 h × 128 frequency bins).

Campos-Cerqueira and Aide 2017: To calculate the amplitude, we used the meanspec (f = 44,100, wl = 256, wn = “hanning”) and fpeaks (threshold = 0.1, freq = 172) function from the seewave package in R (Sueur et al., 2008a). The value of each peak was normalized using the maximum amplitude value within all recordings in the soundscape (i.e., site), and thus values ranged from 0 to 1. The number of frequency peaks was determined by counting the number of recordings with a peak within each of the 128 frequency bins that were equal or greater than the amplitude threshold. To control for the different number of recordings in each site and each time interval (i.e., hour), we divided the number of recordings with a peak in each time/frequency class by the total number of recordings collected during each hourly interval.

To calculate ASU in our study:

- A. We aggregate recordings for a multiple days across multiple sites (Keep only data that has been recorded continuously for a 24hr period)
```{r}
# List the path that contains all folders, which contain the audiomoth data
path <- "C:\\data\\"

# Listing the folders within which .WAV files are stored
folders <- dir(path, recursive=F,full.names=T)

# Let's first rename the files by name of each site (as prefix)
# PLEASE NOTE: The renaming needs to be done only a single time (else there will be errors)

for(i in 1:length(folders)){

setwd(folders[i])
  
# List the files within each folder and renaming the files with the prefix - SITE_ID
a <- list.files(paste0(path,basename(folders)[i],"\\"), full.names = T)
file.rename(from = a, to=paste0(basename(folders)[i],"_",basename(a)))
}

# Now get only those files for a full 24 hours across every unique site
files <- list()

for(i in 1:length(folders)){

a <- list.files(paste0(path,basename(folders)[i],"\\"), full.names = T)
site_date <- str_extract(basename(a),'\\w+_\\d+_')

# Choosing all 24 hours of data across every unique site (288 corresponds to 12 files every 1 hour)
  for(j in 1:length(unique(site_date))){
    dat <- a[str_detect(a,unique(site_date)[j])]
    if((length(dat)<288)==TRUE){
      next
    } else {
      files <- c(files, dat) 
    }
  }
}

files <- unlist(files)
```


- B. Aggregate recordings for any single day for every unique site and sort it in order (between 00:00:00 to 23:55:00 hrs) and then loop it across multiple days

ASU is being calculated for every one hour period and showcased for a 24 hour period of data. ASU here is defined as the overall area occupied by the frequency contour.In our case, the sampling rate of our signal is 48,000 Hz (in future iterations, this value can be changed depending on the range of frequency values for which we would want to estimate acoustic space use)
```{r}
# Select all unique site_date combinations for each unique site
site_date <- str_extract(basename(files),'\\w+_\\d+_')
unique(site_date)

# Create a sequence of numbers to combine files by 1 hour
hour_seq <- seq(from=0,to=288, by=12)

# To name files with a suffix for each hour
time_of_day <- c("00:00-01:00","01:00-02:00","02:00-03:00","03:00-04:00",
                 "04:00-05:00","05:00-06:00","06:00-07:00","07:00-08:00",
                 "08:00-09:00","09:00-10:00","10:00-11:00","11:00-12:00",
                 "12:00-13:00","13:00-14:00","14:00-15:00","15:00-16:00",
                 "16:00-17:00","17:00-18:00","18:00-19:00","19:00-20:00",
                 "20:00-21:00","21:00-22:00","22:00-23:00","23:00-24:00")

# Loading parameters necessary for the Short-term fourier transform to be performed on hourly aggregates of data for each site_date combination
f <- 48000
wl <- 512 # This window length should be changed as a function of the frequency resolution (ie. bin size) and temporal resolution (ie. time)
ovlp <- 50
wn <- "hanning"

# Store the 24 hour acoustic space use data in a list and name it by a unique site and date
site_date_asu <-  list()

# Add a progress bar for the loop
pb <- txtProgressBar(
  min = 0,
  max = length(unique(site_date)),
  style = 3
)

# Select only 24 hours of data (00:00:00 to 23:55:00) for every unique site-date

length(unique(site_date))
for(i in 1:1){ 
  
  # Store the acoustic space use data in a data.frame for plotting and analysis
  space_use <- data.frame()

  # Extract the strings first by site 
  dat <- files[stringr::str_detect(files,unique(site_date)[i])]
  
  # Parallelize the runs
  cl <- makeCluster(8, type = "SOCK")
  registerDoSNOW(cl)
  
  # Store the each hour of data for an entire day as a list here (raw audio files read by tuneR::readWave())
  dailydata <- foreach(k=1:length(dat), .combine = 'c', .inorder = T, .packages = 'tuneR') %dopar% {
    r <- readWave(dat[k])
  }
  gc()
  stopCluster(cl)
  
  # renaming the files to ensure that data is read in hour by hour and stored in a separate object called hourlydata
  # Below we read the first 12 files in and then save it after computing the STDFT and then repeating it for the next 12 files and so on
  
  for(t in 1:(length(hour_seq)-1)) { # Every 12 files correspond to one hour here
    if (t == 1) {
    hourlydata <- dailydata[hour_seq[t]:hour_seq[t+1]]
    } else {
    hourlydata <- dailydata[(hour_seq[t]+1):hour_seq[t+1]]
    }
  
  gc()
  
  # Parallelize the runs
  cl <- makeCluster(8, type = "SOCK")
  registerDoSNOW(cl)
  
  # Store every hour's ASU data here
  data_per_hour <- foreach(z = 1:length(hourlydata), .combine = 'rbind',
                           .inorder=T, .packages = 'seewave') %dopar% {
      wave <- hourlydata[[z]] 
      n <- length(wave)
        
      ## Short-term Fourier transform (using a seewave internal function)
      m <- sspectro(wave, f = f, wl = wl, ovlp = ovlp, wn = wn)

      # Frequency selection and frequency axis
      # Here, want only a sequence of numbers that correspond to the length of rows of the
      # short-time fourier transform and we divide it by 1000 to get values in kHz
      freq <- seq(0, (f/2) - (f/wl), length.out = nrow(m))/1000
        
      # Calculate acoustic space use per frequency bin 
      f.cont <- apply(m, MARGIN = 1, FUN = sum)
      #f.cont <- f.cont/sum(f.cont)
        
      # Store the space use data in a dataframe for plotting later
      a <- data.frame(freq, f.cont)
      #data_per_hour <- rbind(a, data_per_hour)
    }
  rm(hourlydata); gc()
  
  #data_needed <- do.call(c, data_per_hour)
    
  data_per_hour  <- data_per_hour %>%
  group_by(freq) %>%
  summarise(f.cont=sum(f.cont)) 
  
  data_per_hour$f.cont <- (data_per_hour$f.cont)/12
  data_per_hour$time_of_day <- time_of_day[t]
  
  space_use <- rbind(data_per_hour, space_use)
  stopCluster(cl); gc()
}}
  space_use <- as.data.frame(space_use)
  site_date_asu <- c(site_date_asu,list(space_use))
  names(site_date_asu)[i] <- unique(site_date)[i] 
  rm(hourlydata, space_use, data_needed); gc()
  setTxtProgressBar(pb, i)
}
close(pb)    
 
```


Now that we have calculated acoustic space use values for a given frequency bin size and time resolution for every unique site-date combination, we would like to obtain a single set of space use values for every unique site. In other words, we would like to average space use values across all frequency bins for seven days (a week approximately)

```{r}
# All unique sites
site <- str_extract(basename(files),'^([[:alnum:]])+')
unique(site)

# Store the site-wise ASU values
site_asu <- list()

# Loop through site-wise data and average data for each site
for(i in 1:length(unique(site))) {
  
  # Extract data needed for every unique site
  dat <- site_date_asu[stringr::str_detect(names(site_date_asu),unique(site)[i])]
  
  # Sum up values of acoustic space use across X no. of days for every unique site
  asu_sum <-  dat %>%
    bind_rows(.id="data") %>% 
    group_by(freq, time_of_day) %>% 
    summarize(f.cont.sum=sum(f.cont))
  
  # Averaging data as a function of number of days for a given site over which data was summed
  asu_sum$f.cont.sum <- (asu_sum$f.cont.sum)/length(dat)
  
  # Scaling the data between 0 to 1 for the sake of comparison across sites
  asu_sum$f.cont.sum <- range01(asu_sum$f.cont.sum)
  
  site_asu <- c(site_asu, list(asu_sum))
  names(site_asu)[i] <- unique(site)[i] 
  rm(dat,asu_sum)
}

```


Visualization of data

We will use ggplot2 to plot and save individual ASU plots for each unique site-date combination. This will then be repeated for each unique site (calculated in the previous chunk of code) 
```{r}
# Saving the ggplots to a folder for every unique site-date combination
for(i in 1:length(site_date_asu)){
  
  # Get the data out from a list to a dataframe (else ggplot won't accept it)
  dat <- data.frame(site_date_asu[i])
  names(dat) <- c("freq","f.cont","time_of_day")
  
  # Plot the data
  g1 <- ggplot(dat, aes(x=time_of_day, y=freq)) +  
  geom_tile(aes(fill = f.cont)) +
  scale_fill_gradientn(colours = brewer.pal(9,"Reds"))+
    # scale_fill_scico(palette = "vikO") +
    theme_bw() +
    labs(x="Time of Day (in hours)",
       y="Frequency (in kHz) ") +
    theme(axis.title = element_text(size = 16, face = "bold"), 
        axis.ticks.length.x = unit(.5, "cm"),
        axis.text = element_text(size = 14),
        axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5),
        legend.title = element_blank(),
        legend.key.size = unit(1,"cm"),
        legend.text = element_text(size = 12))
  
  # Save the data to a folder
  ggsave(filename="",width=12, height=7, units="in", dpi = 300, 
         plot=g1, device="png", path = paste("figs/site_date_asu/",  paste(names(site_date_asu)[i], ".png", sep=""), sep=""))
}


# Saving the ggplots to a folder for every unique site
for(j in 1:length(site_asu)){
  
  # Get the data out from a list to a .dataframe (else ggplot won't accept it)
  dat <- data.frame(site_asu[j])
  names(dat) <- c("freq","time_of_day","f.cont.sum")
  
  # Plot the data
  g1 <- ggplot(dat, aes(x=time_of_day, y=freq)) +  
  geom_tile(aes(fill = f.cont.sum)) +
  scale_fill_gradientn(colours = brewer.pal(9,"Reds"))+
    # scale_fill_scico(palette = "vikO") +
    theme_bw() +
    labs(x="Time of Day (in hours)",
       y="Frequency (in kHz) ") +
    theme(axis.title = element_text(size = 16, face = "bold"), 
        axis.ticks.length.x = unit(.5, "cm"),
        axis.text = element_text(size = 14),
        axis.text.x = element_text(angle=90, vjust=0.5, hjust=0.5),
        legend.title = element_blank(),
        legend.key.size = unit(1,"cm"),
        legend.text = element_text(size = 12))
  
  # Save the data to a folder
  ggsave(filename="",width=12, height=7, units="in", dpi = 300, 
         plot=g1, device="png", path = paste("figs/site_asu/",  paste(names(site_asu)[j], ".png", sep=""), sep=""))
}


ggsave(g1, filename = "figs/fig_asu_HP37P4B.svg", width=12, height=7,
       device = svg(), units="in", dpi = 300); dev.off()



```

